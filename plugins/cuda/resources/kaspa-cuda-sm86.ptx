//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31833905
// Cuda compilation tools, release 11.8, V11.8.89
// Based on NVVM 7.0.1
//

.version 7.8
.target sm_86
.address_size 64

	// .globl	heavy_hash
.global .align 4 .b8 IV[32] = {103, 230, 9, 106, 133, 174, 103, 187, 114, 243, 110, 60, 58, 245, 79, 165, 127, 82, 14, 81, 140, 104, 5, 155, 171, 217, 131, 31, 25, 205, 224, 91};
.global .align 1 .b8 MSG_SCHEDULE[112] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 6, 3, 10, 7, 0, 4, 13, 1, 11, 12, 5, 9, 14, 15, 8, 3, 4, 10, 12, 13, 2, 7, 14, 6, 5, 9, 0, 11, 15, 8, 1, 10, 7, 12, 9, 14, 3, 13, 15, 4, 0, 11, 2, 5, 8, 1, 6, 12, 13, 9, 11, 15, 10, 14, 8, 7, 2, 5, 3, 0, 1, 6, 4, 9, 14, 11, 5, 8, 12, 15, 1, 13, 3, 0, 10, 2, 6, 4, 7, 11, 15, 5, 0, 1, 9, 8, 6, 14, 10, 2, 12, 3, 4, 7, 13};
.global .align 1 .b8 rho[24] = {1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 2, 14, 27, 41, 56, 8, 25, 43, 62, 18, 39, 61, 20, 44};
.global .align 1 .b8 pi[24] = {10, 7, 11, 17, 18, 3, 5, 16, 8, 21, 24, 4, 15, 23, 19, 13, 12, 2, 20, 14, 22, 9, 6, 1};
.global .align 8 .b8 RC[192] = {1, 0, 0, 0, 0, 0, 0, 0, 130, 128, 0, 0, 0, 0, 0, 0, 138, 128, 0, 0, 0, 0, 0, 128, 0, 128, 0, 128, 0, 0, 0, 128, 139, 128, 0, 0, 0, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 0, 129, 128, 0, 128, 0, 0, 0, 128, 9, 128, 0, 0, 0, 0, 0, 128, 138, 0, 0, 0, 0, 0, 0, 0, 136, 0, 0, 0, 0, 0, 0, 0, 9, 128, 0, 128, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 0, 139, 128, 0, 128, 0, 0, 0, 0, 139, 0, 0, 0, 0, 0, 0, 128, 137, 128, 0, 0, 0, 0, 0, 128, 3, 128, 0, 0, 0, 0, 0, 128, 2, 128, 0, 0, 0, 0, 0, 128, 128, 0, 0, 0, 0, 0, 0, 128, 10, 128, 0, 0, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 128, 129, 128, 0, 128, 0, 0, 0, 128, 128, 128, 0, 0, 0, 0, 0, 128, 1, 0, 0, 128, 0, 0, 0, 0, 8, 128, 0, 128, 0, 0, 0, 128};
.global .align 8 .b8 _ZZ15xoshiro256_jumpP10ulonglong4E4JUMP[32] = {186, 10, 253, 60, 211, 198, 14, 24, 44, 57, 201, 240, 102, 18, 166, 213, 170, 201, 63, 224, 24, 38, 88, 169, 28, 102, 177, 41, 69, 220, 171, 57};
.global .align 8 .b8 _ZZ20xoshiro256_long_jumpP10ulonglong4E9LONG_JUMP[32] = {191, 203, 253, 254, 62, 93, 225, 118, 179, 47, 82, 28, 68, 78, 0, 197, 65, 226, 78, 133, 105, 0, 113, 119, 53, 230, 203, 42, 176, 155, 16, 57};
.const .align 4 .b8 matrix[4096];
.const .align 8 .b8 hash_header[72];
.const .align 8 .b8 target[32];
.const .align 1 .b8 powP[200] = {61, 216, 246, 161, 13, 255, 60, 17, 60, 126, 2, 183, 85, 136, 191, 41, 210, 68, 251, 14, 114, 46, 95, 30, 160, 105, 152, 245, 163, 164, 165, 27, 101, 45, 94, 135, 202, 175, 47, 123, 70, 226, 220, 41, 214, 97, 239, 74, 16, 91, 65, 173, 30, 152, 58, 24, 156, 194, 155, 120, 12, 246, 107, 119, 64, 49, 102, 136, 51, 241, 235, 248, 240, 95, 40, 67, 60, 28, 101, 46, 10, 74, 241, 64, 5, 7, 150, 15, 82, 145, 41, 91, 135, 103, 227, 68, 21, 55, 177, 37, 164, 241, 112, 236, 137, 218, 233, 130, 143, 93, 200, 230, 35, 178, 180, 133, 31, 96, 26, 178, 70, 106, 163, 100, 144, 84, 133, 52, 26, 133, 47, 122, 28, 221, 6, 15, 66, 177, 59, 86, 29, 2, 162, 193, 228, 104, 22, 69, 228, 229, 29, 186, 141, 95, 9, 5, 65, 87, 2, 209, 74, 207, 206, 155, 132, 78, 202, 137, 219, 46, 116, 168, 39, 148, 176, 72, 114, 82, 139, 231, 156, 206, 252, 177, 188, 165, 175, 130, 207, 41, 17, 93, 131, 67, 130, 111, 120, 124, 185, 2};
.const .align 1 .b8 heavyP[200] = {9, 133, 36, 178, 82, 76, 215, 58, 22, 66, 159, 47, 14, 155, 98, 121, 238, 248, 199, 22, 72, 255, 20, 122, 152, 100, 5, 128, 76, 95, 167, 17, 218, 206, 238, 68, 223, 224, 32, 231, 105, 64, 243, 20, 46, 216, 199, 114, 186, 53, 137, 147, 42, 255, 0, 193, 98, 196, 15, 37, 64, 144, 33, 94, 72, 106, 207, 13, 166, 249, 57, 128, 12, 61, 42, 121, 159, 170, 188, 160, 38, 162, 169, 208, 93, 192, 49, 244, 63, 140, 193, 84, 195, 76, 31, 211, 61, 204, 105, 167, 1, 125, 107, 108, 228, 147, 36, 86, 211, 91, 198, 46, 68, 176, 205, 153, 58, 75, 247, 78, 176, 242, 52, 84, 131, 134, 76, 119, 22, 148, 188, 54, 176, 97, 233, 7, 7, 204, 101, 119, 177, 29, 143, 126, 57, 109, 196, 186, 128, 219, 143, 234, 88, 202, 52, 123, 211, 242, 146, 185, 87, 185, 129, 132, 4, 197, 118, 199, 46, 194, 18, 81, 103, 159, 195, 71, 10, 12, 41, 181, 157, 57, 187, 146, 21, 198, 159, 47, 49, 224, 154, 84, 53, 218, 185, 16, 125, 50, 25, 22};

.visible .entry heavy_hash(
	.param .u64 heavy_hash_param_0,
	.param .u64 heavy_hash_param_1,
	.param .u64 heavy_hash_param_2,
	.param .u8 heavy_hash_param_3,
	.param .u64 heavy_hash_param_4,
	.param .u64 heavy_hash_param_5
)
{
	.local .align 8 .b8 	__local_depot0[1912];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<113>;
	.reg .b32 	%r<6245>;
	.reg .b64 	%rd<490>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u8 	%rs11, [heavy_hash_param_3];
	ld.param.u64 	%rd78, [heavy_hash_param_0];
	ld.param.u64 	%rd79, [heavy_hash_param_1];
	ld.param.u64 	%rd80, [heavy_hash_param_2];
	ld.param.u64 	%rd81, [heavy_hash_param_4];
	ld.param.u64 	%rd82, [heavy_hash_param_5];
	cvta.to.global.u64 	%rd1, %rd81;
	cvta.to.global.u64 	%rd2, %rd82;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r20, %r18, %r17, %r19;
	cvt.s64.s32 	%rd4, %r20;
	setp.ge.u64 	%p6, %rd4, %rd80;
	@%p6 bra 	$L__BB0_19;

	cvt.u32.u64 	%r21, %rd4;
	setp.ne.s32 	%p7, %r21, 0;
	@%p7 bra 	$L__BB0_3;

	mov.u64 	%rd84, 0;
	st.global.u64 	[%rd2], %rd84;

$L__BB0_3:
	setp.eq.s16 	%p8, %rs11, 0;
	@%p8 bra 	$L__BB0_5;

	shl.b64 	%rd85, %rd4, 5;
	add.s64 	%rd86, %rd1, %rd85;
	ld.global.v2.u64 	{%rd87, %rd88}, [%rd86];
	mul.lo.s64 	%rd91, %rd88, 5;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd91, 7;
	shr.b64 	%rhs, %rd91, 57;
	add.u64 	%rd92, %lhs, %rhs;
	}
	mul.lo.s64 	%rd463, %rd92, 9;
	shl.b64 	%rd93, %rd88, 17;
	ld.global.v2.u64 	{%rd94, %rd95}, [%rd86+16];
	xor.b64  	%rd98, %rd94, %rd87;
	xor.b64  	%rd99, %rd95, %rd88;
	xor.b64  	%rd100, %rd88, %rd98;
	xor.b64  	%rd101, %rd87, %rd99;
	st.global.v2.u64 	[%rd86], {%rd101, %rd100};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r22,%dummy}, %rd99;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r23}, %rd99;
	}
	shf.r.wrap.b32 	%r24, %r23, %r22, 19;
	shf.r.wrap.b32 	%r25, %r22, %r23, 19;
	mov.b64 	%rd102, {%r25, %r24};
	xor.b64  	%rd103, %rd98, %rd93;
	st.global.v2.u64 	[%rd86+16], {%rd103, %rd102};
	bra.uni 	$L__BB0_6;

$L__BB0_5:
	ld.global.u64 	%rd104, [%rd1];
	xor.b64  	%rd463, %rd104, %rd4;

$L__BB0_6:
	and.b64  	%rd105, %rd463, %rd78;
	or.b64  	%rd8, %rd105, %rd79;
	mov.b64 	{%r26, %r27}, %rd8;
	mov.u64 	%rd106, 0;
	ld.const.u64 	%rd107, [hash_header];
	cvt.u32.u64 	%r28, %rd107;
	shr.u64 	%rd108, %rd107, 8;
	cvt.u32.u64 	%r29, %rd108;
	shr.u64 	%rd109, %rd107, 16;
	cvt.u32.u64 	%r30, %rd109;
	shr.u64 	%rd110, %rd107, 32;
	cvt.u32.u64 	%r31, %rd110;
	shr.u64 	%rd111, %rd107, 40;
	cvt.u32.u64 	%r32, %rd111;
	shr.u64 	%rd112, %rd107, 48;
	cvt.u32.u64 	%r33, %rd112;
	ld.const.u64 	%rd113, [hash_header+8];
	cvt.u32.u64 	%r34, %rd113;
	shr.u64 	%rd114, %rd113, 8;
	cvt.u32.u64 	%r35, %rd114;
	shr.u64 	%rd115, %rd113, 16;
	cvt.u32.u64 	%r36, %rd115;
	shr.u64 	%rd116, %rd113, 32;
	cvt.u32.u64 	%r37, %rd116;
	shr.u64 	%rd117, %rd113, 40;
	cvt.u32.u64 	%r38, %rd117;
	shr.u64 	%rd118, %rd113, 48;
	cvt.u32.u64 	%r39, %rd118;
	ld.const.u64 	%rd119, [hash_header+16];
	cvt.u32.u64 	%r40, %rd119;
	shr.u64 	%rd120, %rd119, 8;
	cvt.u32.u64 	%r41, %rd120;
	shr.u64 	%rd121, %rd119, 16;
	cvt.u32.u64 	%r42, %rd121;
	shr.u64 	%rd122, %rd119, 32;
	cvt.u32.u64 	%r43, %rd122;
	shr.u64 	%rd123, %rd119, 40;
	cvt.u32.u64 	%r44, %rd123;
	shr.u64 	%rd124, %rd119, 48;
	cvt.u32.u64 	%r45, %rd124;
	ld.const.u64 	%rd125, [hash_header+24];
	cvt.u32.u64 	%r46, %rd125;
	shr.u64 	%rd126, %rd125, 8;
	cvt.u32.u64 	%r47, %rd126;
	shr.u64 	%rd127, %rd125, 16;
	cvt.u32.u64 	%r48, %rd127;
	shr.u64 	%rd128, %rd125, 32;
	cvt.u32.u64 	%r49, %rd128;
	shr.u64 	%rd129, %rd125, 40;
	cvt.u32.u64 	%r50, %rd129;
	shr.u64 	%rd130, %rd125, 48;
	cvt.u32.u64 	%r51, %rd130;
	ld.const.v4.u16 	{%rs12, %rs13, %rs14, %rs15}, [hash_header+32];
	shr.u16 	%rs17, %rs12, 8;
	shr.u16 	%rs19, %rs13, 8;
	shr.u16 	%rs21, %rs14, 8;
	shr.u16 	%rs23, %rs15, 8;
	ld.const.v4.u16 	{%rs24, %rs25, %rs26, %rs27}, [hash_header+40];
	shr.u16 	%rs29, %rs24, 8;
	shr.u16 	%rs31, %rs25, 8;
	shr.u16 	%rs33, %rs26, 8;
	shr.u16 	%rs35, %rs27, 8;
	ld.const.v4.u16 	{%rs36, %rs37, %rs38, %rs39}, [hash_header+48];
	shr.u16 	%rs41, %rs36, 8;
	shr.u16 	%rs43, %rs37, 8;
	shr.u16 	%rs45, %rs38, 8;
	shr.u16 	%rs47, %rs39, 8;
	ld.const.v4.u16 	{%rs48, %rs49, %rs50, %rs51}, [hash_header+56];
	shr.u16 	%rs53, %rs48, 8;
	shr.u16 	%rs55, %rs49, 8;
	shr.u16 	%rs57, %rs50, 8;
	shr.u16 	%rs59, %rs51, 8;
	ld.const.u64 	%rd131, [hash_header+64];
	mov.b64 	{%r52, %r53}, %rd131;
	mov.u32 	%r54, -1150833019;
	mov.u32 	%r55, 1779033703;
	st.local.v2.u32 	[%rd3], {%r55, %r54};
	mov.u32 	%r56, -1521486534;
	mov.u32 	%r57, 1013904242;
	st.local.v2.u32 	[%rd3+8], {%r57, %r56};
	mov.u32 	%r58, -1694144372;
	mov.u32 	%r59, 1359893119;
	st.local.v2.u32 	[%rd3+16], {%r59, %r58};
	mov.u32 	%r60, 1541459225;
	mov.u32 	%r61, 528734635;
	st.local.v2.u32 	[%rd3+24], {%r61, %r60};
	st.local.u64 	[%rd3+64], %rd106;
	mov.u32 	%r62, 0;
	st.local.v2.u32 	[%rd3+88], {%r62, %r62};
	st.local.v2.u32 	[%rd3+96], {%r62, %r62};
	st.local.v2.u32 	[%rd3+104], {%r62, %r62};
	st.local.v2.u32 	[%rd3+112], {%r62, %r62};
	st.local.v2.u32 	[%rd3+120], {%r62, %r62};
	st.local.v2.u32 	[%rd3+128], {%r62, %r62};
	mov.u16 	%rs60, 0;
	st.local.v2.u8 	[%rd3+136], {%rs60, %rs60};
	st.local.u8 	[%rd3+138], %rs60;
	st.local.v2.u32 	[%rd3+32], {%r55, %r54};
	st.local.v2.u32 	[%rd3+40], {%r57, %r56};
	st.local.v2.u32 	[%rd3+48], {%r59, %r58};
	st.local.v2.u32 	[%rd3+56], {%r61, %r60};
	st.local.v2.u32 	[%rd3+72], {%r62, %r62};
	st.local.v2.u32 	[%rd3+80], {%r62, %r62};
	st.local.u8 	[%rd3+144], %rs60;
	ld.local.v4.u8 	{%rs61, %rs62, %rs63, %rs64}, [%rd3+136];
	setp.eq.s16 	%p9, %rs62, 0;
	selp.u16 	%rs68, 1, 0, %p9;
	or.b16  	%rs69, %rs63, %rs68;
	shr.u32 	%r63, %r28, 24;
	mov.u32 	%r64, 64;
	prmt.b32 	%r65, %r28, %r29, %r64;
	mov.u32 	%r66, 1040;
	prmt.b32 	%r67, %r65, %r30, %r66;
	mov.u32 	%r68, 16912;
	prmt.b32 	%r69, %r67, %r63, %r68;
	and.b32  	%r70, %r31, 255;
	and.b32  	%r71, %r32, 255;
	prmt.b32 	%r72, %r71, %r70, 30212;
	shl.b32 	%r73, %r33, 16;
	and.b32  	%r74, %r73, 16711680;
	or.b32  	%r75, %r72, %r74;
	and.b32  	%r76, %r31, -16777216;
	or.b32  	%r77, %r75, %r76;
	shr.u32 	%r78, %r34, 24;
	prmt.b32 	%r79, %r34, %r35, %r64;
	prmt.b32 	%r80, %r79, %r36, %r66;
	prmt.b32 	%r81, %r80, %r78, %r68;
	and.b32  	%r82, %r37, 255;
	and.b32  	%r83, %r38, 255;
	prmt.b32 	%r84, %r83, %r82, 30212;
	shl.b32 	%r85, %r39, 16;
	and.b32  	%r86, %r85, 16711680;
	or.b32  	%r87, %r84, %r86;
	and.b32  	%r88, %r37, -16777216;
	or.b32  	%r89, %r87, %r88;
	shr.u32 	%r90, %r40, 24;
	prmt.b32 	%r91, %r40, %r41, %r64;
	prmt.b32 	%r92, %r91, %r42, %r66;
	prmt.b32 	%r93, %r92, %r90, %r68;
	and.b32  	%r94, %r43, 255;
	and.b32  	%r95, %r44, 255;
	prmt.b32 	%r96, %r95, %r94, 30212;
	shl.b32 	%r97, %r45, 16;
	and.b32  	%r98, %r97, 16711680;
	or.b32  	%r99, %r96, %r98;
	and.b32  	%r100, %r43, -16777216;
	or.b32  	%r101, %r99, %r100;
	shr.u32 	%r102, %r46, 24;
	prmt.b32 	%r103, %r46, %r47, %r64;
	prmt.b32 	%r104, %r103, %r48, %r66;
	prmt.b32 	%r105, %r104, %r102, %r68;
	and.b32  	%r106, %r49, 255;
	and.b32  	%r107, %r50, 255;
	prmt.b32 	%r108, %r107, %r106, 30212;
	shl.b32 	%r109, %r51, 16;
	and.b32  	%r110, %r109, 16711680;
	or.b32  	%r111, %r108, %r110;
	and.b32  	%r112, %r49, -16777216;
	or.b32  	%r113, %r111, %r112;
	cvt.u32.u16 	%r114, %rs12;
	and.b32  	%r115, %r114, 255;
	cvt.u32.u16 	%r116, %rs17;
	prmt.b32 	%r117, %r116, %r115, 30212;
	cvt.u32.u16 	%r118, %rs13;
	prmt.b32 	%r119, %r118, %r117, 28756;
	cvt.u32.u16 	%r120, %rs19;
	prmt.b32 	%r121, %r120, %r119, 1620;
	cvt.u32.u16 	%r122, %rs14;
	and.b32  	%r123, %r122, 255;
	cvt.u32.u16 	%r124, %rs21;
	prmt.b32 	%r125, %r124, %r123, 30212;
	cvt.u32.u16 	%r126, %rs15;
	prmt.b32 	%r127, %r126, %r125, 28756;
	cvt.u32.u16 	%r128, %rs23;
	prmt.b32 	%r129, %r128, %r127, 1620;
	cvt.u32.u16 	%r130, %rs24;
	and.b32  	%r131, %r130, 255;
	cvt.u32.u16 	%r132, %rs29;
	prmt.b32 	%r133, %r132, %r131, 30212;
	cvt.u32.u16 	%r134, %rs25;
	prmt.b32 	%r135, %r134, %r133, 28756;
	cvt.u32.u16 	%r136, %rs31;
	prmt.b32 	%r137, %r136, %r135, 1620;
	cvt.u32.u16 	%r138, %rs26;
	and.b32  	%r139, %r138, 255;
	cvt.u32.u16 	%r140, %rs33;
	prmt.b32 	%r141, %r140, %r139, 30212;
	cvt.u32.u16 	%r142, %rs27;
	prmt.b32 	%r143, %r142, %r141, 28756;
	cvt.u32.u16 	%r144, %rs35;
	prmt.b32 	%r145, %r144, %r143, 1620;
	cvt.u32.u16 	%r146, %rs36;
	and.b32  	%r147, %r146, 255;
	cvt.u32.u16 	%r148, %rs41;
	prmt.b32 	%r149, %r148, %r147, 30212;
	cvt.u32.u16 	%r150, %rs37;
	prmt.b32 	%r151, %r150, %r149, 28756;
	cvt.u32.u16 	%r152, %rs43;
	prmt.b32 	%r153, %r152, %r151, 1620;
	cvt.u32.u16 	%r154, %rs38;
	and.b32  	%r155, %r154, 255;
	cvt.u32.u16 	%r156, %rs45;
	prmt.b32 	%r157, %r156, %r155, 30212;
	cvt.u32.u16 	%r158, %rs39;
	prmt.b32 	%r159, %r158, %r157, 28756;
	cvt.u32.u16 	%r160, %rs47;
	prmt.b32 	%r161, %r160, %r159, 1620;
	cvt.u32.u16 	%r162, %rs48;
	and.b32  	%r163, %r162, 255;
	cvt.u32.u16 	%r164, %rs53;
	prmt.b32 	%r165, %r164, %r163, 30212;
	cvt.u32.u16 	%r166, %rs49;
	prmt.b32 	%r167, %r166, %r165, 28756;
	cvt.u32.u16 	%r168, %rs55;
	prmt.b32 	%r169, %r168, %r167, 1620;
	cvt.u32.u16 	%r170, %rs50;
	and.b32  	%r171, %r170, 255;
	cvt.u32.u16 	%r172, %rs57;
	prmt.b32 	%r173, %r172, %r171, 30212;
	cvt.u32.u16 	%r174, %rs51;
	prmt.b32 	%r175, %r174, %r173, 28756;
	cvt.u32.u16 	%r176, %rs59;
	prmt.b32 	%r177, %r176, %r175, 1620;
	cvt.u32.u16 	%r178, %rs69;
	and.b32  	%r179, %r178, 255;
	add.s32 	%r180, %r69, -1156040474;
	shf.l.wrap.b32 	%r181, %r180, %r180, 16;
	add.s32 	%r182, %r181, 1779033703;
	xor.b32  	%r183, %r182, 1359893119;
	shf.l.wrap.b32 	%r184, %r183, %r183, 20;
	add.s32 	%r185, %r77, %r180;
	add.s32 	%r186, %r185, %r184;
	xor.b32  	%r187, %r186, %r181;
	shf.l.wrap.b32 	%r188, %r187, %r187, 24;
	add.s32 	%r189, %r188, %r182;
	xor.b32  	%r190, %r189, %r184;
	shf.l.wrap.b32 	%r191, %r190, %r190, 25;
	add.s32 	%r192, %r81, 1449989905;
	shf.l.wrap.b32 	%r193, %r192, %r192, 16;
	add.s32 	%r194, %r193, -1150833019;
	xor.b32  	%r195, %r194, -1694144372;
	shf.l.wrap.b32 	%r196, %r195, %r195, 20;
	add.s32 	%r197, %r89, %r192;
	add.s32 	%r198, %r197, %r196;
	xor.b32  	%r199, %r198, %r193;
	shf.l.wrap.b32 	%r200, %r199, %r199, 24;
	add.s32 	%r201, %r200, %r194;
	xor.b32  	%r202, %r201, %r196;
	shf.l.wrap.b32 	%r203, %r202, %r202, 25;
	add.s32 	%r204, %r93, 1542638877;
	shr.u32 	%r205, %r204, 16;
	shl.b32 	%r206, %r204, 16;
	xor.b32  	%r207, %r206, 4194304;
	or.b32  	%r208, %r207, %r205;
	add.s32 	%r209, %r208, 1013904242;
	xor.b32  	%r210, %r209, 528734635;
	shf.l.wrap.b32 	%r211, %r210, %r210, 20;
	add.s32 	%r212, %r101, %r204;
	add.s32 	%r213, %r212, %r211;
	xor.b32  	%r214, %r213, %r208;
	shf.l.wrap.b32 	%r215, %r214, %r214, 24;
	add.s32 	%r216, %r215, %r209;
	xor.b32  	%r217, %r216, %r211;
	shf.l.wrap.b32 	%r218, %r217, %r217, 25;
	add.s32 	%r219, %r105, 19972691;
	xor.b32  	%r220, %r219, %r179;
	shr.u32 	%r221, %r219, 16;
	shl.b32 	%r222, %r220, 16;
	or.b32  	%r223, %r222, %r221;
	add.s32 	%r224, %r223, -1521486534;
	xor.b32  	%r225, %r224, 1541459225;
	shf.l.wrap.b32 	%r226, %r225, %r225, 20;
	add.s32 	%r227, %r113, %r219;
	add.s32 	%r228, %r227, %r226;
	xor.b32  	%r229, %r228, %r223;
	shf.l.wrap.b32 	%r230, %r229, %r229, 24;
	add.s32 	%r231, %r230, %r224;
	xor.b32  	%r232, %r231, %r226;
	shf.l.wrap.b32 	%r233, %r232, %r232, 25;
	add.s32 	%r234, %r203, %r186;
	add.s32 	%r235, %r234, %r121;
	xor.b32  	%r236, %r230, %r235;
	shf.l.wrap.b32 	%r237, %r236, %r236, 16;
	add.s32 	%r238, %r237, %r216;
	xor.b32  	%r239, %r238, %r203;
	shf.l.wrap.b32 	%r240, %r239, %r239, 20;
	add.s32 	%r241, %r129, %r235;
	add.s32 	%r242, %r241, %r240;
	xor.b32  	%r243, %r242, %r237;
	shf.l.wrap.b32 	%r244, %r243, %r243, 24;
	add.s32 	%r245, %r244, %r238;
	xor.b32  	%r246, %r245, %r240;
	shf.l.wrap.b32 	%r247, %r246, %r246, 25;
	add.s32 	%r248, %r218, %r198;
	add.s32 	%r249, %r248, %r137;
	xor.b32  	%r250, %r249, %r188;
	shf.l.wrap.b32 	%r251, %r250, %r250, 16;
	add.s32 	%r252, %r251, %r231;
	xor.b32  	%r253, %r252, %r218;
	shf.l.wrap.b32 	%r254, %r253, %r253, 20;
	add.s32 	%r255, %r145, %r249;
	add.s32 	%r256, %r255, %r254;
	xor.b32  	%r257, %r256, %r251;
	shf.l.wrap.b32 	%r258, %r257, %r257, 24;
	add.s32 	%r259, %r258, %r252;
	xor.b32  	%r260, %r259, %r254;
	shf.l.wrap.b32 	%r261, %r260, %r260, 25;
	add.s32 	%r262, %r233, %r213;
	add.s32 	%r263, %r262, %r153;
	xor.b32  	%r264, %r263, %r200;
	shf.l.wrap.b32 	%r265, %r264, %r264, 16;
	add.s32 	%r266, %r265, %r189;
	xor.b32  	%r267, %r266, %r233;
	shf.l.wrap.b32 	%r268, %r267, %r267, 20;
	add.s32 	%r269, %r161, %r263;
	add.s32 	%r270, %r269, %r268;
	xor.b32  	%r271, %r270, %r265;
	shf.l.wrap.b32 	%r272, %r271, %r271, 24;
	add.s32 	%r273, %r272, %r266;
	xor.b32  	%r274, %r273, %r268;
	shf.l.wrap.b32 	%r275, %r274, %r274, 25;
	add.s32 	%r276, %r228, %r191;
	add.s32 	%r277, %r276, %r169;
	xor.b32  	%r278, %r277, %r215;
	shf.l.wrap.b32 	%r279, %r278, %r278, 16;
	add.s32 	%r280, %r279, %r201;
	xor.b32  	%r281, %r280, %r191;
	shf.l.wrap.b32 	%r282, %r281, %r281, 20;
	add.s32 	%r283, %r177, %r277;
	add.s32 	%r284, %r283, %r282;
	xor.b32  	%r285, %r284, %r279;
	shf.l.wrap.b32 	%r286, %r285, %r285, 24;
	add.s32 	%r287, %r286, %r280;
	xor.b32  	%r288, %r287, %r282;
	shf.l.wrap.b32 	%r289, %r288, %r288, 25;
	add.s32 	%r290, %r242, %r81;
	add.s32 	%r291, %r290, %r289;
	xor.b32  	%r292, %r291, %r258;
	shf.l.wrap.b32 	%r293, %r292, %r292, 16;
	add.s32 	%r294, %r293, %r273;
	xor.b32  	%r295, %r294, %r289;
	shf.l.wrap.b32 	%r296, %r295, %r295, 20;
	add.s32 	%r297, %r291, %r105;
	add.s32 	%r298, %r297, %r296;
	xor.b32  	%r299, %r298, %r293;
	shf.l.wrap.b32 	%r300, %r299, %r299, 24;
	add.s32 	%r301, %r300, %r294;
	xor.b32  	%r302, %r301, %r296;
	shf.l.wrap.b32 	%r303, %r302, %r302, 25;
	add.s32 	%r304, %r256, %r89;
	add.s32 	%r305, %r304, %r247;
	xor.b32  	%r306, %r272, %r305;
	shf.l.wrap.b32 	%r307, %r306, %r306, 16;
	add.s32 	%r308, %r287, %r307;
	xor.b32  	%r309, %r308, %r247;
	shf.l.wrap.b32 	%r310, %r309, %r309, 20;
	add.s32 	%r311, %r305, %r137;
	add.s32 	%r312, %r311, %r310;
	xor.b32  	%r313, %r312, %r307;
	shf.l.wrap.b32 	%r314, %r313, %r313, 24;
	add.s32 	%r315, %r314, %r308;
	xor.b32  	%r316, %r315, %r310;
	shf.l.wrap.b32 	%r317, %r316, %r316, 25;
	add.s32 	%r318, %r261, %r113;
	add.s32 	%r319, %r318, %r270;
	xor.b32  	%r320, %r286, %r319;
	shf.l.wrap.b32 	%r321, %r320, %r320, 16;
	add.s32 	%r322, %r321, %r245;
	xor.b32  	%r323, %r322, %r261;
	shf.l.wrap.b32 	%r324, %r323, %r323, 20;
	add.s32 	%r325, %r319, %r69;
	add.s32 	%r326, %r325, %r324;
	xor.b32  	%r327, %r326, %r321;
	shf.l.wrap.b32 	%r328, %r327, %r327, 24;
	add.s32 	%r329, %r328, %r322;
	xor.b32  	%r330, %r329, %r324;
	shf.l.wrap.b32 	%r331, %r330, %r330, 25;
	add.s32 	%r332, %r275, %r93;
	add.s32 	%r333, %r332, %r284;
	xor.b32  	%r334, %r333, %r244;
	shf.l.wrap.b32 	%r335, %r334, %r334, 16;
	add.s32 	%r336, %r335, %r259;
	xor.b32  	%r337, %r336, %r275;
	shf.l.wrap.b32 	%r338, %r337, %r337, 20;
	add.s32 	%r339, %r333, %r161;
	add.s32 	%r340, %r339, %r338;
	xor.b32  	%r341, %r340, %r335;
	shf.l.wrap.b32 	%r342, %r341, %r341, 24;
	add.s32 	%r343, %r342, %r336;
	xor.b32  	%r344, %r343, %r338;
	shf.l.wrap.b32 	%r345, %r344, %r344, 25;
	add.s32 	%r346, %r298, %r77;
	add.s32 	%r347, %r346, %r317;
	xor.b32  	%r348, %r347, %r342;
	shf.l.wrap.b32 	%r349, %r348, %r348, 16;
	add.s32 	%r350, %r349, %r329;
	xor.b32  	%r351, %r350, %r317;
	shf.l.wrap.b32 	%r352, %r351, %r351, 20;
	add.s32 	%r353, %r347, %r145;
	add.s32 	%r354, %r353, %r352;
	xor.b32  	%r355, %r354, %r349;
	shf.l.wrap.b32 	%r356, %r355, %r355, 24;
	add.s32 	%r357, %r356, %r350;
	xor.b32  	%r358, %r357, %r352;
	shf.l.wrap.b32 	%r359, %r358, %r358, 25;
	add.s32 	%r360, %r312, %r153;
	add.s32 	%r361, %r360, %r331;
	xor.b32  	%r362, %r361, %r300;
	shf.l.wrap.b32 	%r363, %r362, %r362, 16;
	add.s32 	%r364, %r363, %r343;
	xor.b32  	%r365, %r364, %r331;
	shf.l.wrap.b32 	%r366, %r365, %r365, 20;
	add.s32 	%r367, %r361, %r101;
	add.s32 	%r368, %r367, %r366;
	xor.b32  	%r369, %r368, %r363;
	shf.l.wrap.b32 	%r370, %r369, %r369, 24;
	add.s32 	%r371, %r370, %r364;
	xor.b32  	%r372, %r371, %r366;
	shf.l.wrap.b32 	%r373, %r372, %r372, 25;
	add.s32 	%r374, %r326, %r129;
	add.s32 	%r375, %r374, %r345;
	xor.b32  	%r376, %r375, %r314;
	shf.l.wrap.b32 	%r377, %r376, %r376, 16;
	add.s32 	%r378, %r377, %r301;
	xor.b32  	%r379, %r378, %r345;
	shf.l.wrap.b32 	%r380, %r379, %r379, 20;
	add.s32 	%r381, %r375, %r169;
	add.s32 	%r382, %r381, %r380;
	xor.b32  	%r383, %r382, %r377;
	shf.l.wrap.b32 	%r384, %r383, %r383, 24;
	add.s32 	%r385, %r384, %r378;
	xor.b32  	%r386, %r385, %r380;
	shf.l.wrap.b32 	%r387, %r386, %r386, 25;
	add.s32 	%r388, %r340, %r177;
	add.s32 	%r389, %r388, %r303;
	xor.b32  	%r390, %r389, %r328;
	shf.l.wrap.b32 	%r391, %r390, %r390, 16;
	add.s32 	%r392, %r391, %r315;
	xor.b32  	%r393, %r392, %r303;
	shf.l.wrap.b32 	%r394, %r393, %r393, 20;
	add.s32 	%r395, %r389, %r121;
	add.s32 	%r396, %r395, %r394;
	xor.b32  	%r397, %r396, %r391;
	shf.l.wrap.b32 	%r398, %r397, %r397, 24;
	add.s32 	%r399, %r398, %r392;
	xor.b32  	%r400, %r399, %r394;
	shf.l.wrap.b32 	%r401, %r400, %r400, 25;
	add.s32 	%r402, %r354, %r89;
	add.s32 	%r403, %r402, %r401;
	xor.b32  	%r404, %r403, %r370;
	shf.l.wrap.b32 	%r405, %r404, %r404, 16;
	add.s32 	%r406, %r405, %r385;
	xor.b32  	%r407, %r406, %r401;
	shf.l.wrap.b32 	%r408, %r407, %r407, 20;
	add.s32 	%r409, %r403, %r93;
	add.s32 	%r410, %r409, %r408;
	xor.b32  	%r411, %r410, %r405;
	shf.l.wrap.b32 	%r412, %r411, %r411, 24;
	add.s32 	%r413, %r412, %r406;
	xor.b32  	%r414, %r413, %r408;
	shf.l.wrap.b32 	%r415, %r414, %r414, 25;
	add.s32 	%r416, %r368, %r137;
	add.s32 	%r417, %r416, %r359;
	xor.b32  	%r418, %r417, %r384;
	shf.l.wrap.b32 	%r419, %r418, %r418, 16;
	add.s32 	%r420, %r419, %r399;
	xor.b32  	%r421, %r420, %r359;
	shf.l.wrap.b32 	%r422, %r421, %r421, 20;
	add.s32 	%r423, %r417, %r153;
	add.s32 	%r424, %r423, %r422;
	xor.b32  	%r425, %r424, %r419;
	shf.l.wrap.b32 	%r426, %r425, %r425, 24;
	add.s32 	%r427, %r426, %r420;
	xor.b32  	%r428, %r427, %r422;
	shf.l.wrap.b32 	%r429, %r428, %r428, 25;
	add.s32 	%r430, %r382, %r161;
	add.s32 	%r431, %r430, %r373;
	xor.b32  	%r432, %r431, %r398;
	shf.l.wrap.b32 	%r433, %r432, %r432, 16;
	add.s32 	%r434, %r433, %r357;
	xor.b32  	%r435, %r434, %r373;
	shf.l.wrap.b32 	%r436, %r435, %r435, 20;
	add.s32 	%r437, %r431, %r81;
	add.s32 	%r438, %r437, %r436;
	xor.b32  	%r439, %r438, %r433;
	shf.l.wrap.b32 	%r440, %r439, %r439, 24;
	add.s32 	%r441, %r440, %r434;
	xor.b32  	%r442, %r441, %r436;
	shf.l.wrap.b32 	%r443, %r442, %r442, 25;
	add.s32 	%r444, %r396, %r113;
	add.s32 	%r445, %r444, %r387;
	xor.b32  	%r446, %r445, %r356;
	shf.l.wrap.b32 	%r447, %r446, %r446, 16;
	add.s32 	%r448, %r447, %r371;
	xor.b32  	%r449, %r448, %r387;
	shf.l.wrap.b32 	%r450, %r449, %r449, 20;
	add.s32 	%r451, %r445, %r169;
	add.s32 	%r452, %r451, %r450;
	xor.b32  	%r453, %r452, %r447;
	shf.l.wrap.b32 	%r454, %r453, %r453, 24;
	add.s32 	%r455, %r454, %r448;
	xor.b32  	%r456, %r455, %r450;
	shf.l.wrap.b32 	%r457, %r456, %r456, 25;
	add.s32 	%r458, %r410, %r105;
	add.s32 	%r459, %r458, %r429;
	xor.b32  	%r460, %r459, %r454;
	shf.l.wrap.b32 	%r461, %r460, %r460, 16;
	add.s32 	%r462, %r461, %r441;
	xor.b32  	%r463, %r462, %r429;
	shf.l.wrap.b32 	%r464, %r463, %r463, 20;
	add.s32 	%r465, %r459, %r101;
	add.s32 	%r466, %r465, %r464;
	xor.b32  	%r467, %r466, %r461;
	shf.l.wrap.b32 	%r468, %r467, %r467, 24;
	add.s32 	%r469, %r468, %r462;
	xor.b32  	%r470, %r469, %r464;
	shf.l.wrap.b32 	%r471, %r470, %r470, 25;
	add.s32 	%r472, %r424, %r129;
	add.s32 	%r473, %r472, %r443;
	xor.b32  	%r474, %r473, %r412;
	shf.l.wrap.b32 	%r475, %r474, %r474, 16;
	add.s32 	%r476, %r475, %r455;
	xor.b32  	%r477, %r476, %r443;
	shf.l.wrap.b32 	%r478, %r477, %r477, 20;
	add.s32 	%r479, %r473, %r69;
	add.s32 	%r480, %r479, %r478;
	xor.b32  	%r481, %r480, %r475;
	shf.l.wrap.b32 	%r482, %r481, %r481, 24;
	add.s32 	%r483, %r482, %r476;
	xor.b32  	%r484, %r483, %r478;
	shf.l.wrap.b32 	%r485, %r484, %r484, 25;
	add.s32 	%r486, %r438, %r145;
	add.s32 	%r487, %r486, %r457;
	xor.b32  	%r488, %r487, %r426;
	shf.l.wrap.b32 	%r489, %r488, %r488, 16;
	add.s32 	%r490, %r489, %r413;
	xor.b32  	%r491, %r490, %r457;
	shf.l.wrap.b32 	%r492, %r491, %r491, 20;
	add.s32 	%r493, %r487, %r177;
	add.s32 	%r494, %r493, %r492;
	xor.b32  	%r495, %r494, %r489;
	shf.l.wrap.b32 	%r496, %r495, %r495, 24;
	add.s32 	%r497, %r496, %r490;
	xor.b32  	%r498, %r497, %r492;
	shf.l.wrap.b32 	%r499, %r498, %r498, 25;
	add.s32 	%r500, %r452, %r121;
	add.s32 	%r501, %r500, %r415;
	xor.b32  	%r502, %r501, %r440;
	shf.l.wrap.b32 	%r503, %r502, %r502, 16;
	add.s32 	%r504, %r503, %r427;
	xor.b32  	%r505, %r504, %r415;
	shf.l.wrap.b32 	%r506, %r505, %r505, 20;
	add.s32 	%r507, %r501, %r77;
	add.s32 	%r508, %r507, %r506;
	xor.b32  	%r509, %r508, %r503;
	shf.l.wrap.b32 	%r510, %r509, %r509, 24;
	add.s32 	%r511, %r510, %r504;
	xor.b32  	%r512, %r511, %r506;
	shf.l.wrap.b32 	%r513, %r512, %r512, 25;
	add.s32 	%r514, %r466, %r137;
	add.s32 	%r515, %r514, %r513;
	xor.b32  	%r516, %r515, %r482;
	shf.l.wrap.b32 	%r517, %r516, %r516, 16;
	add.s32 	%r518, %r517, %r497;
	xor.b32  	%r519, %r518, %r513;
	shf.l.wrap.b32 	%r520, %r519, %r519, 20;
	add.s32 	%r521, %r515, %r113;
	add.s32 	%r522, %r521, %r520;
	xor.b32  	%r523, %r522, %r517;
	shf.l.wrap.b32 	%r524, %r523, %r523, 24;
	add.s32 	%r525, %r524, %r518;
	xor.b32  	%r526, %r525, %r520;
	shf.l.wrap.b32 	%r527, %r526, %r526, 25;
	add.s32 	%r528, %r480, %r153;
	add.s32 	%r529, %r528, %r471;
	xor.b32  	%r530, %r529, %r496;
	shf.l.wrap.b32 	%r531, %r530, %r530, 16;
	add.s32 	%r532, %r531, %r511;
	xor.b32  	%r533, %r532, %r471;
	shf.l.wrap.b32 	%r534, %r533, %r533, 20;
	add.s32 	%r535, %r529, %r129;
	add.s32 	%r536, %r535, %r534;
	xor.b32  	%r537, %r536, %r531;
	shf.l.wrap.b32 	%r538, %r537, %r537, 24;
	add.s32 	%r539, %r538, %r532;
	xor.b32  	%r540, %r539, %r534;
	shf.l.wrap.b32 	%r541, %r540, %r540, 25;
	add.s32 	%r542, %r494, %r169;
	add.s32 	%r543, %r542, %r485;
	xor.b32  	%r544, %r543, %r510;
	shf.l.wrap.b32 	%r545, %r544, %r544, 16;
	add.s32 	%r546, %r545, %r469;
	xor.b32  	%r547, %r546, %r485;
	shf.l.wrap.b32 	%r548, %r547, %r547, 20;
	add.s32 	%r549, %r543, %r89;
	add.s32 	%r550, %r549, %r548;
	xor.b32  	%r551, %r550, %r545;
	shf.l.wrap.b32 	%r552, %r551, %r551, 24;
	add.s32 	%r553, %r552, %r546;
	xor.b32  	%r554, %r553, %r548;
	shf.l.wrap.b32 	%r555, %r554, %r554, 25;
	add.s32 	%r556, %r508, %r161;
	add.s32 	%r557, %r556, %r499;
	xor.b32  	%r558, %r557, %r468;
	shf.l.wrap.b32 	%r559, %r558, %r558, 16;
	add.s32 	%r560, %r559, %r483;
	xor.b32  	%r561, %r560, %r499;
	shf.l.wrap.b32 	%r562, %r561, %r561, 20;
	add.s32 	%r563, %r557, %r177;
	add.s32 	%r564, %r563, %r562;
	xor.b32  	%r565, %r564, %r559;
	shf.l.wrap.b32 	%r566, %r565, %r565, 24;
	add.s32 	%r567, %r566, %r560;
	xor.b32  	%r568, %r567, %r562;
	shf.l.wrap.b32 	%r569, %r568, %r568, 25;
	add.s32 	%r570, %r522, %r93;
	add.s32 	%r571, %r570, %r541;
	xor.b32  	%r572, %r571, %r566;
	shf.l.wrap.b32 	%r573, %r572, %r572, 16;
	add.s32 	%r574, %r573, %r553;
	xor.b32  	%r575, %r574, %r541;
	shf.l.wrap.b32 	%r576, %r575, %r575, 20;
	add.s32 	%r577, %r571, %r69;
	add.s32 	%r578, %r577, %r576;
	xor.b32  	%r579, %r578, %r573;
	shf.l.wrap.b32 	%r580, %r579, %r579, 24;
	add.s32 	%r581, %r580, %r574;
	xor.b32  	%r582, %r581, %r576;
	shf.l.wrap.b32 	%r583, %r582, %r582, 25;
	add.s32 	%r584, %r536, %r145;
	add.s32 	%r585, %r584, %r555;
	xor.b32  	%r586, %r585, %r524;
	shf.l.wrap.b32 	%r587, %r586, %r586, 16;
	add.s32 	%r588, %r587, %r567;
	xor.b32  	%r589, %r588, %r555;
	shf.l.wrap.b32 	%r590, %r589, %r589, 20;
	add.s32 	%r591, %r585, %r81;
	add.s32 	%r592, %r591, %r590;
	xor.b32  	%r593, %r592, %r587;
	shf.l.wrap.b32 	%r594, %r593, %r593, 24;
	add.s32 	%r595, %r594, %r588;
	xor.b32  	%r596, %r595, %r590;
	shf.l.wrap.b32 	%r597, %r596, %r596, 25;
	add.s32 	%r598, %r550, %r101;
	add.s32 	%r599, %r598, %r569;
	xor.b32  	%r600, %r599, %r538;
	shf.l.wrap.b32 	%r601, %r600, %r600, 16;
	add.s32 	%r602, %r601, %r525;
	xor.b32  	%r603, %r602, %r569;
	shf.l.wrap.b32 	%r604, %r603, %r603, 20;
	add.s32 	%r605, %r599, %r121;
	add.s32 	%r606, %r605, %r604;
	xor.b32  	%r607, %r606, %r601;
	shf.l.wrap.b32 	%r608, %r607, %r607, 24;
	add.s32 	%r609, %r608, %r602;
	xor.b32  	%r610, %r609, %r604;
	shf.l.wrap.b32 	%r611, %r610, %r610, 25;
	add.s32 	%r612, %r564, %r77;
	add.s32 	%r613, %r612, %r527;
	xor.b32  	%r614, %r613, %r552;
	shf.l.wrap.b32 	%r615, %r614, %r614, 16;
	add.s32 	%r616, %r615, %r539;
	xor.b32  	%r617, %r616, %r527;
	shf.l.wrap.b32 	%r618, %r617, %r617, 20;
	add.s32 	%r619, %r613, %r105;
	add.s32 	%r620, %r619, %r618;
	xor.b32  	%r621, %r620, %r615;
	shf.l.wrap.b32 	%r622, %r621, %r621, 24;
	add.s32 	%r623, %r622, %r616;
	xor.b32  	%r624, %r623, %r618;
	shf.l.wrap.b32 	%r625, %r624, %r624, 25;
	add.s32 	%r626, %r578, %r153;
	add.s32 	%r627, %r626, %r625;
	xor.b32  	%r628, %r627, %r594;
	shf.l.wrap.b32 	%r629, %r628, %r628, 16;
	add.s32 	%r630, %r629, %r609;
	xor.b32  	%r631, %r630, %r625;
	shf.l.wrap.b32 	%r632, %r631, %r631, 20;
	add.s32 	%r633, %r627, %r161;
	add.s32 	%r634, %r633, %r632;
	xor.b32  	%r635, %r634, %r629;
	shf.l.wrap.b32 	%r636, %r635, %r635, 24;
	add.s32 	%r637, %r636, %r630;
	xor.b32  	%r638, %r637, %r632;
	shf.l.wrap.b32 	%r639, %r638, %r638, 25;
	add.s32 	%r640, %r592, %r129;
	add.s32 	%r641, %r640, %r583;
	xor.b32  	%r642, %r641, %r608;
	shf.l.wrap.b32 	%r643, %r642, %r642, 16;
	add.s32 	%r644, %r643, %r623;
	xor.b32  	%r645, %r644, %r583;
	shf.l.wrap.b32 	%r646, %r645, %r645, 20;
	add.s32 	%r647, %r641, %r145;
	add.s32 	%r648, %r647, %r646;
	xor.b32  	%r649, %r648, %r643;
	shf.l.wrap.b32 	%r650, %r649, %r649, 24;
	add.s32 	%r651, %r650, %r644;
	xor.b32  	%r652, %r651, %r646;
	shf.l.wrap.b32 	%r653, %r652, %r652, 25;
	add.s32 	%r654, %r606, %r177;
	add.s32 	%r655, %r654, %r597;
	xor.b32  	%r656, %r655, %r622;
	shf.l.wrap.b32 	%r657, %r656, %r656, 16;
	add.s32 	%r658, %r657, %r581;
	xor.b32  	%r659, %r658, %r597;
	shf.l.wrap.b32 	%r660, %r659, %r659, 20;
	add.s32 	%r661, %r655, %r137;
	add.s32 	%r662, %r661, %r660;
	xor.b32  	%r663, %r662, %r657;
	shf.l.wrap.b32 	%r664, %r663, %r663, 24;
	add.s32 	%r665, %r664, %r658;
	xor.b32  	%r666, %r665, %r660;
	shf.l.wrap.b32 	%r667, %r666, %r666, 25;
	add.s32 	%r668, %r620, %r169;
	add.s32 	%r669, %r668, %r611;
	xor.b32  	%r670, %r669, %r580;
	shf.l.wrap.b32 	%r671, %r670, %r670, 16;
	add.s32 	%r672, %r671, %r595;
	xor.b32  	%r673, %r672, %r611;
	shf.l.wrap.b32 	%r674, %r673, %r673, 20;
	add.s32 	%r675, %r669, %r121;
	add.s32 	%r676, %r675, %r674;
	xor.b32  	%r677, %r676, %r671;
	shf.l.wrap.b32 	%r678, %r677, %r677, 24;
	add.s32 	%r679, %r678, %r672;
	xor.b32  	%r680, %r679, %r674;
	shf.l.wrap.b32 	%r681, %r680, %r680, 25;
	add.s32 	%r682, %r634, %r113;
	add.s32 	%r683, %r682, %r653;
	xor.b32  	%r684, %r683, %r678;
	shf.l.wrap.b32 	%r685, %r684, %r684, 16;
	add.s32 	%r686, %r685, %r665;
	xor.b32  	%r687, %r686, %r653;
	shf.l.wrap.b32 	%r688, %r687, %r687, 20;
	add.s32 	%r689, %r683, %r81;
	add.s32 	%r690, %r689, %r688;
	xor.b32  	%r691, %r690, %r685;
	shf.l.wrap.b32 	%r692, %r691, %r691, 24;
	add.s32 	%r693, %r692, %r686;
	xor.b32  	%r694, %r693, %r688;
	shf.l.wrap.b32 	%r695, %r694, %r694, 25;
	add.s32 	%r696, %r648, %r101;
	add.s32 	%r697, %r696, %r667;
	xor.b32  	%r698, %r697, %r636;
	shf.l.wrap.b32 	%r699, %r698, %r698, 16;
	add.s32 	%r700, %r699, %r679;
	xor.b32  	%r701, %r700, %r667;
	shf.l.wrap.b32 	%r702, %r701, %r701, 20;
	add.s32 	%r703, %r697, %r89;
	add.s32 	%r704, %r703, %r702;
	xor.b32  	%r705, %r704, %r699;
	shf.l.wrap.b32 	%r706, %r705, %r705, 24;
	add.s32 	%r707, %r706, %r700;
	xor.b32  	%r708, %r707, %r702;
	shf.l.wrap.b32 	%r709, %r708, %r708, 25;
	add.s32 	%r710, %r662, %r69;
	add.s32 	%r711, %r710, %r681;
	xor.b32  	%r712, %r711, %r650;
	shf.l.wrap.b32 	%r713, %r712, %r712, 16;
	add.s32 	%r714, %r713, %r637;
	xor.b32  	%r715, %r714, %r681;
	shf.l.wrap.b32 	%r716, %r715, %r715, 20;
	add.s32 	%r717, %r711, %r77;
	add.s32 	%r718, %r717, %r716;
	xor.b32  	%r719, %r718, %r713;
	shf.l.wrap.b32 	%r720, %r719, %r719, 24;
	add.s32 	%r721, %r720, %r714;
	xor.b32  	%r722, %r721, %r716;
	shf.l.wrap.b32 	%r723, %r722, %r722, 25;
	add.s32 	%r724, %r676, %r105;
	add.s32 	%r725, %r724, %r639;
	xor.b32  	%r726, %r725, %r664;
	shf.l.wrap.b32 	%r727, %r726, %r726, 16;
	add.s32 	%r728, %r727, %r651;
	xor.b32  	%r729, %r728, %r639;
	shf.l.wrap.b32 	%r730, %r729, %r729, 20;
	add.s32 	%r731, %r725, %r93;
	add.s32 	%r732, %r731, %r730;
	xor.b32  	%r733, %r732, %r727;
	shf.l.wrap.b32 	%r734, %r733, %r733, 24;
	add.s32 	%r735, %r734, %r728;
	xor.b32  	%r736, %r735, %r730;
	shf.l.wrap.b32 	%r737, %r736, %r736, 25;
	add.s32 	%r738, %r690, %r129;
	add.s32 	%r739, %r738, %r737;
	xor.b32  	%r740, %r739, %r706;
	shf.l.wrap.b32 	%r741, %r740, %r740, 16;
	add.s32 	%r742, %r741, %r721;
	xor.b32  	%r743, %r742, %r737;
	shf.l.wrap.b32 	%r744, %r743, %r743, 20;
	add.s32 	%r745, %r739, %r169;
	add.s32 	%r746, %r745, %r744;
	xor.b32  	%r747, %r746, %r741;
	shf.l.wrap.b32 	%r748, %r747, %r747, 24;
	add.s32 	%r749, %r748, %r742;
	xor.b32  	%r750, %r749, %r744;
	shf.l.wrap.b32 	%r751, %r750, %r750, 25;
	add.s32 	%r752, %r704, %r145;
	add.s32 	%r753, %r752, %r695;
	xor.b32  	%r754, %r753, %r720;
	shf.l.wrap.b32 	%r755, %r754, %r754, 16;
	add.s32 	%r756, %r755, %r735;
	xor.b32  	%r757, %r756, %r695;
	shf.l.wrap.b32 	%r758, %r757, %r757, 20;
	add.s32 	%r759, %r753, %r101;
	add.s32 	%r760, %r759, %r758;
	xor.b32  	%r761, %r760, %r755;
	shf.l.wrap.b32 	%r762, %r761, %r761, 24;
	add.s32 	%r763, %r762, %r756;
	xor.b32  	%r764, %r763, %r758;
	shf.l.wrap.b32 	%r765, %r764, %r764, 25;
	add.s32 	%r766, %r718, %r121;
	add.s32 	%r767, %r766, %r709;
	xor.b32  	%r768, %r767, %r734;
	shf.l.wrap.b32 	%r769, %r768, %r768, 16;
	add.s32 	%r770, %r769, %r693;
	xor.b32  	%r771, %r770, %r709;
	shf.l.wrap.b32 	%r772, %r771, %r771, 20;
	add.s32 	%r773, %r767, %r153;
	add.s32 	%r774, %r773, %r772;
	xor.b32  	%r775, %r774, %r769;
	shf.l.wrap.b32 	%r776, %r775, %r775, 24;
	add.s32 	%r777, %r776, %r770;
	xor.b32  	%r778, %r777, %r772;
	shf.l.wrap.b32 	%r779, %r778, %r778, 25;
	add.s32 	%r780, %r732, %r177;
	add.s32 	%r781, %r780, %r723;
	xor.b32  	%r782, %r781, %r692;
	shf.l.wrap.b32 	%r783, %r782, %r782, 16;
	add.s32 	%r784, %r783, %r707;
	xor.b32  	%r785, %r784, %r723;
	shf.l.wrap.b32 	%r786, %r785, %r785, 20;
	add.s32 	%r787, %r781, %r77;
	add.s32 	%r788, %r787, %r786;
	xor.b32  	%r789, %r788, %r783;
	shf.l.wrap.b32 	%r790, %r789, %r789, 24;
	add.s32 	%r791, %r790, %r784;
	xor.b32  	%r792, %r791, %r786;
	shf.l.wrap.b32 	%r793, %r792, %r792, 25;
	add.s32 	%r794, %r746, %r161;
	add.s32 	%r795, %r794, %r765;
	xor.b32  	%r796, %r795, %r790;
	shf.l.wrap.b32 	%r797, %r796, %r796, 16;
	add.s32 	%r798, %r797, %r777;
	xor.b32  	%r799, %r798, %r765;
	shf.l.wrap.b32 	%r800, %r799, %r799, 20;
	add.s32 	%r801, %r795, %r89;
	add.s32 	%r802, %r801, %r800;
	xor.b32  	%r803, %r802, %r797;
	shf.l.wrap.b32 	%r804, %r803, %r803, 24;
	add.s32 	%r805, %r804, %r798;
	xor.b32  	%r806, %r805, %r800;
	shf.l.wrap.b32 	%r807, %r806, %r806, 25;
	add.s32 	%r808, %r760, %r69;
	add.s32 	%r809, %r808, %r779;
	xor.b32  	%r810, %r809, %r748;
	shf.l.wrap.b32 	%r811, %r810, %r810, 16;
	add.s32 	%r812, %r811, %r791;
	xor.b32  	%r813, %r812, %r779;
	shf.l.wrap.b32 	%r814, %r813, %r813, 20;
	add.s32 	%r815, %r809, %r137;
	add.s32 	%r816, %r815, %r814;
	xor.b32  	%r817, %r816, %r811;
	shf.l.wrap.b32 	%r818, %r817, %r817, 24;
	add.s32 	%r819, %r818, %r812;
	xor.b32  	%r820, %r819, %r814;
	shf.l.wrap.b32 	%r821, %r820, %r820, 25;
	add.s32 	%r822, %r774, %r81;
	add.s32 	%r823, %r822, %r793;
	xor.b32  	%r824, %r823, %r762;
	shf.l.wrap.b32 	%r825, %r824, %r824, 16;
	add.s32 	%r826, %r825, %r749;
	xor.b32  	%r827, %r826, %r793;
	shf.l.wrap.b32 	%r828, %r827, %r827, 20;
	add.s32 	%r829, %r823, %r105;
	add.s32 	%r830, %r829, %r828;
	xor.b32  	%r831, %r830, %r825;
	shf.l.wrap.b32 	%r832, %r831, %r831, 24;
	add.s32 	%r833, %r832, %r826;
	xor.b32  	%r834, %r833, %r828;
	shf.l.wrap.b32 	%r835, %r834, %r834, 25;
	add.s32 	%r836, %r788, %r93;
	add.s32 	%r837, %r836, %r751;
	xor.b32  	%r838, %r837, %r776;
	shf.l.wrap.b32 	%r839, %r838, %r838, 16;
	add.s32 	%r840, %r839, %r763;
	xor.b32  	%r841, %r840, %r751;
	shf.l.wrap.b32 	%r842, %r841, %r841, 20;
	add.s32 	%r843, %r837, %r113;
	add.s32 	%r844, %r843, %r842;
	xor.b32  	%r845, %r844, %r839;
	shf.l.wrap.b32 	%r846, %r845, %r845, 24;
	add.s32 	%r847, %r846, %r840;
	xor.b32  	%r848, %r847, %r842;
	shf.l.wrap.b32 	%r849, %r848, %r848, 25;
	add.s32 	%r850, %r802, %r145;
	add.s32 	%r851, %r850, %r849;
	xor.b32  	%r852, %r851, %r818;
	shf.l.wrap.b32 	%r853, %r852, %r852, 16;
	add.s32 	%r854, %r853, %r833;
	xor.b32  	%r855, %r854, %r849;
	shf.l.wrap.b32 	%r856, %r855, %r855, 20;
	add.s32 	%r857, %r851, %r177;
	add.s32 	%r858, %r857, %r856;
	xor.b32  	%r859, %r858, %r853;
	shf.l.wrap.b32 	%r860, %r859, %r859, 24;
	add.s32 	%r861, %r860, %r854;
	xor.b32  	%r862, %r861, %r856;
	shf.l.wrap.b32 	%r863, %r862, %r862, 25;
	add.s32 	%r864, %r816, %r101;
	add.s32 	%r865, %r864, %r807;
	xor.b32  	%r866, %r865, %r832;
	shf.l.wrap.b32 	%r867, %r866, %r866, 16;
	add.s32 	%r868, %r867, %r847;
	xor.b32  	%r869, %r868, %r807;
	shf.l.wrap.b32 	%r870, %r869, %r869, 20;
	add.s32 	%r871, %r865, %r69;
	add.s32 	%r872, %r871, %r870;
	xor.b32  	%r873, %r872, %r867;
	shf.l.wrap.b32 	%r874, %r873, %r873, 24;
	add.s32 	%r875, %r874, %r868;
	xor.b32  	%r876, %r875, %r870;
	shf.l.wrap.b32 	%r877, %r876, %r876, 25;
	add.s32 	%r878, %r830, %r77;
	add.s32 	%r879, %r878, %r821;
	xor.b32  	%r880, %r879, %r846;
	shf.l.wrap.b32 	%r881, %r880, %r880, 16;
	add.s32 	%r882, %r881, %r805;
	xor.b32  	%r883, %r882, %r821;
	shf.l.wrap.b32 	%r884, %r883, %r883, 20;
	add.s32 	%r885, %r879, %r129;
	add.s32 	%r886, %r885, %r884;
	xor.b32  	%r887, %r886, %r881;
	shf.l.wrap.b32 	%r888, %r887, %r887, 24;
	add.s32 	%r889, %r888, %r882;
	xor.b32  	%r890, %r889, %r884;
	shf.l.wrap.b32 	%r891, %r890, %r890, 25;
	add.s32 	%r892, %r844, %r121;
	add.s32 	%r893, %r892, %r835;
	xor.b32  	%r894, %r893, %r804;
	shf.l.wrap.b32 	%r895, %r894, %r894, 16;
	add.s32 	%r896, %r895, %r819;
	xor.b32  	%r897, %r896, %r835;
	shf.l.wrap.b32 	%r898, %r897, %r897, 20;
	add.s32 	%r899, %r893, %r105;
	add.s32 	%r900, %r899, %r898;
	xor.b32  	%r901, %r900, %r895;
	shf.l.wrap.b32 	%r902, %r901, %r901, 24;
	add.s32 	%r903, %r902, %r896;
	xor.b32  	%r904, %r903, %r898;
	shf.l.wrap.b32 	%r905, %r904, %r904, 25;
	add.s32 	%r906, %r858, %r169;
	add.s32 	%r907, %r906, %r877;
	xor.b32  	%r908, %r907, %r902;
	shf.l.wrap.b32 	%r909, %r908, %r908, 16;
	add.s32 	%r910, %r909, %r889;
	xor.b32  	%r911, %r910, %r877;
	shf.l.wrap.b32 	%r912, %r911, %r911, 20;
	add.s32 	%r913, %r907, %r137;
	add.s32 	%r914, %r913, %r912;
	xor.b32  	%r915, %r914, %r909;
	shf.l.wrap.b32 	%r916, %r915, %r915, 24;
	add.s32 	%r917, %r916, %r910;
	xor.b32  	%r918, %r917, %r912;
	shf.l.wrap.b32 	%r919, %r918, %r918, 25;
	add.s32 	%r920, %r872, %r81;
	add.s32 	%r921, %r920, %r891;
	xor.b32  	%r922, %r921, %r860;
	shf.l.wrap.b32 	%r923, %r922, %r922, 16;
	add.s32 	%r924, %r923, %r903;
	xor.b32  	%r925, %r924, %r891;
	shf.l.wrap.b32 	%r926, %r925, %r925, 20;
	add.s32 	%r927, %r921, %r153;
	add.s32 	%r928, %r927, %r926;
	xor.b32  	%r929, %r928, %r923;
	shf.l.wrap.b32 	%r930, %r929, %r929, 24;
	add.s32 	%r931, %r930, %r924;
	xor.b32  	%r932, %r931, %r926;
	shf.l.wrap.b32 	%r933, %r932, %r932, 25;
	add.s32 	%r934, %r886, %r89;
	add.s32 	%r935, %r934, %r905;
	xor.b32  	%r936, %r935, %r874;
	shf.l.wrap.b32 	%r937, %r936, %r936, 16;
	add.s32 	%r938, %r937, %r861;
	xor.b32  	%r939, %r938, %r905;
	shf.l.wrap.b32 	%r940, %r939, %r939, 20;
	add.s32 	%r941, %r935, %r93;
	add.s32 	%r942, %r941, %r940;
	xor.b32  	%r943, %r942, %r937;
	shf.l.wrap.b32 	%r944, %r943, %r943, 24;
	add.s32 	%r945, %r944, %r938;
	xor.b32  	%r946, %r945, %r940;
	shf.l.wrap.b32 	%r947, %r946, %r946, 25;
	add.s32 	%r948, %r900, %r113;
	add.s32 	%r949, %r948, %r863;
	xor.b32  	%r950, %r949, %r888;
	shf.l.wrap.b32 	%r951, %r950, %r950, 16;
	add.s32 	%r952, %r951, %r875;
	xor.b32  	%r953, %r952, %r863;
	shf.l.wrap.b32 	%r954, %r953, %r953, 20;
	add.s32 	%r955, %r949, %r161;
	add.s32 	%r956, %r955, %r954;
	xor.b32  	%r957, %r956, %r951;
	shf.l.wrap.b32 	%r958, %r957, %r957, 24;
	add.s32 	%r959, %r958, %r952;
	xor.b32  	%r960, %r959, %r954;
	shf.l.wrap.b32 	%r961, %r960, %r960, 25;
	xor.b32  	%r1, %r945, %r914;
	xor.b32  	%r2, %r959, %r928;
	st.local.v2.u32 	[%rd3+32], {%r1, %r2};
	xor.b32  	%r3, %r917, %r942;
	xor.b32  	%r4, %r956, %r931;
	st.local.v2.u32 	[%rd3+40], {%r3, %r4};
	xor.b32  	%r5, %r961, %r930;
	xor.b32  	%r6, %r919, %r944;
	st.local.v2.u32 	[%rd3+48], {%r5, %r6};
	xor.b32  	%r7, %r958, %r933;
	xor.b32  	%r8, %r947, %r916;
	st.local.v2.u32 	[%rd3+56], {%r7, %r8};
	st.local.u64 	[%rd3+72], %rd131;
	st.local.u64 	[%rd3+80], %rd8;
	add.s16 	%rs1, %rs61, 16;
	and.b16  	%rs70, %rs1, 255;
	add.s16 	%rs71, %rs62, 1;
	st.local.v2.u8 	[%rd3+136], {%rs1, %rs71};
	cvt.u32.u16 	%r962, %rs71;
	cvt.u32.u16 	%r963, %rs70;
	prmt.b32 	%r964, %r962, %r963, 30212;
	cvt.u16.u32 	%rs72, %r964;
	shr.u16 	%rs2, %rs72, 8;
	mov.b32 	{%rs5, %rs6}, %r53;
	mov.b32 	{%rs3, %rs4}, %r52;
	mov.b32 	{%rs9, %rs10}, %r27;
	mov.b32 	{%rs7, %rs8}, %r26;
	setp.eq.s16 	%p10, %rs2, 0;
	selp.u16 	%rs73, 1, 0, %p10;
	shr.u16 	%rs74, %rs3, 8;
	shr.u16 	%rs75, %rs4, 8;
	shr.u16 	%rs76, %rs5, 8;
	shr.u16 	%rs77, %rs6, 8;
	shr.u16 	%rs78, %rs7, 8;
	shr.u16 	%rs79, %rs8, 8;
	shr.u16 	%rs80, %rs9, 8;
	shr.u16 	%rs81, %rs10, 8;
	or.b16  	%rs82, %rs73, 10;
	cvt.u32.u16 	%r965, %rs3;
	and.b32  	%r966, %r965, 255;
	cvt.u32.u16 	%r967, %rs74;
	prmt.b32 	%r968, %r967, %r966, 30212;
	cvt.u32.u16 	%r969, %rs4;
	prmt.b32 	%r970, %r969, %r968, 28756;
	cvt.u32.u16 	%r971, %rs75;
	prmt.b32 	%r972, %r971, %r970, 1620;
	cvt.u32.u16 	%r973, %rs5;
	and.b32  	%r974, %r973, 255;
	cvt.u32.u16 	%r975, %rs76;
	prmt.b32 	%r976, %r975, %r974, 30212;
	cvt.u32.u16 	%r977, %rs6;
	prmt.b32 	%r978, %r977, %r976, 28756;
	cvt.u32.u16 	%r979, %rs77;
	prmt.b32 	%r980, %r979, %r978, 1620;
	cvt.u32.u16 	%r981, %rs7;
	and.b32  	%r982, %r981, 255;
	cvt.u32.u16 	%r983, %rs78;
	prmt.b32 	%r984, %r983, %r982, 30212;
	cvt.u32.u16 	%r985, %rs8;
	prmt.b32 	%r986, %r985, %r984, 28756;
	cvt.u32.u16 	%r987, %rs79;
	prmt.b32 	%r988, %r987, %r986, 1620;
	cvt.u32.u16 	%r989, %rs9;
	and.b32  	%r990, %r989, 255;
	cvt.u32.u16 	%r991, %rs80;
	prmt.b32 	%r992, %r991, %r990, 30212;
	cvt.u32.u16 	%r993, %rs10;
	prmt.b32 	%r994, %r993, %r992, 28756;
	cvt.u32.u16 	%r995, %rs81;
	prmt.b32 	%r996, %r995, %r994, 1620;
	cvt.u32.u16 	%r997, %rs82;
	add.s32 	%r998, %r5, %r1;
	add.s32 	%r999, %r998, %r972;
	add.s32 	%r1000, %r980, %r999;
	add.s32 	%r1001, %r6, %r2;
	add.s32 	%r1002, %r1001, %r988;
	add.s32 	%r1003, %r996, %r1002;
	add.s32 	%r1004, %r7, %r3;
	cvt.u32.u16 	%r1005, %rs1;
	and.b32  	%r1006, %r1005, 255;
	xor.b32  	%r1007, %r1004, %r1006;
	shr.u32 	%r1008, %r1004, 16;
	shl.b32 	%r1009, %r1007, 16;
	or.b32  	%r1010, %r1009, %r1008;
	add.s32 	%r1011, %r1010, 1013904242;
	xor.b32  	%r1012, %r1011, %r7;
	shf.l.wrap.b32 	%r1013, %r1012, %r1012, 20;
	add.s32 	%r1014, %r1004, %r1013;
	xor.b32  	%r1015, %r1014, %r1010;
	shf.l.wrap.b32 	%r1016, %r1015, %r1015, 24;
	add.s32 	%r1017, %r1016, %r1011;
	xor.b32  	%r1018, %r1017, %r1013;
	shf.l.wrap.b32 	%r1019, %r1018, %r1018, 25;
	add.s32 	%r1020, %r8, %r4;
	xor.b32  	%r1021, %r1020, %r997;
	shr.u32 	%r1022, %r1020, 16;
	shl.b32 	%r1023, %r1021, 16;
	or.b32  	%r1024, %r1023, %r1022;
	add.s32 	%r1025, %r1024, -1521486534;
	xor.b32  	%r1026, %r1025, %r8;
	shf.l.wrap.b32 	%r1027, %r1026, %r1026, 20;
	add.s32 	%r1028, %r1020, %r1027;
	xor.b32  	%r1029, %r1028, %r1024;
	shf.l.wrap.b32 	%r1030, %r1029, %r1029, 24;
	add.s32 	%r1031, %r1030, %r1025;
	xor.b32  	%r1032, %r1031, %r1027;
	shf.l.wrap.b32 	%r1033, %r1032, %r1032, 25;
	add.s32 	%r1034, %r1033, %r1014;
	shf.l.wrap.b32 	%r1035, %r999, %r999, 16;
	add.s32 	%r1036, %r1035, 1779033703;
	xor.b32  	%r1037, %r1036, %r5;
	shf.l.wrap.b32 	%r1038, %r1037, %r1037, 20;
	add.s32 	%r1039, %r1000, %r1038;
	xor.b32  	%r1040, %r1039, %r1035;
	shf.l.wrap.b32 	%r1041, %r1040, %r1040, 24;
	add.s32 	%r1042, %r1041, %r1036;
	xor.b32  	%r1043, %r1042, %r1038;
	shf.l.wrap.b32 	%r1044, %r1043, %r1043, 25;
	shf.l.wrap.b32 	%r1045, %r1002, %r1002, 16;
	add.s32 	%r1046, %r1045, -1150833019;
	xor.b32  	%r1047, %r1046, %r6;
	shf.l.wrap.b32 	%r1048, %r1047, %r1047, 20;
	add.s32 	%r1049, %r1003, %r1048;
	xor.b32  	%r1050, %r1049, %r1045;
	shf.l.wrap.b32 	%r1051, %r1050, %r1050, 24;
	add.s32 	%r1052, %r1051, %r1046;
	xor.b32  	%r1053, %r1052, %r1048;
	shf.l.wrap.b32 	%r1054, %r1053, %r1053, 25;
	add.s32 	%r1055, %r1039, %r1054;
	xor.b32  	%r1056, %r1055, %r1030;
	shf.l.wrap.b32 	%r1057, %r1056, %r1056, 16;
	add.s32 	%r1058, %r1057, %r1017;
	xor.b32  	%r1059, %r1058, %r1054;
	shf.l.wrap.b32 	%r1060, %r1059, %r1059, 20;
	add.s32 	%r1061, %r1055, %r1060;
	xor.b32  	%r1062, %r1061, %r1057;
	shf.l.wrap.b32 	%r1063, %r1062, %r1062, 24;
	add.s32 	%r1064, %r1063, %r1058;
	xor.b32  	%r1065, %r1064, %r1060;
	shf.l.wrap.b32 	%r1066, %r1065, %r1065, 25;
	add.s32 	%r1067, %r1019, %r1049;
	xor.b32  	%r1068, %r1041, %r1067;
	shf.l.wrap.b32 	%r1069, %r1068, %r1068, 16;
	add.s32 	%r1070, %r1069, %r1031;
	xor.b32  	%r1071, %r1070, %r1019;
	shf.l.wrap.b32 	%r1072, %r1071, %r1071, 20;
	add.s32 	%r1073, %r1067, %r1072;
	xor.b32  	%r1074, %r1073, %r1069;
	shf.l.wrap.b32 	%r1075, %r1074, %r1074, 24;
	add.s32 	%r1076, %r1075, %r1070;
	xor.b32  	%r1077, %r1076, %r1072;
	shf.l.wrap.b32 	%r1078, %r1077, %r1077, 25;
	xor.b32  	%r1079, %r1051, %r1034;
	shf.l.wrap.b32 	%r1080, %r1079, %r1079, 16;
	add.s32 	%r1081, %r1080, %r1042;
	xor.b32  	%r1082, %r1081, %r1033;
	shf.l.wrap.b32 	%r1083, %r1082, %r1082, 20;
	add.s32 	%r1084, %r1034, %r1083;
	xor.b32  	%r1085, %r1084, %r1080;
	shf.l.wrap.b32 	%r1086, %r1085, %r1085, 24;
	add.s32 	%r1087, %r1086, %r1081;
	xor.b32  	%r1088, %r1087, %r1083;
	shf.l.wrap.b32 	%r1089, %r1088, %r1088, 25;
	add.s32 	%r1090, %r1028, %r1044;
	xor.b32  	%r1091, %r1090, %r1016;
	shf.l.wrap.b32 	%r1092, %r1091, %r1091, 16;
	add.s32 	%r1093, %r1092, %r1052;
	xor.b32  	%r1094, %r1093, %r1044;
	shf.l.wrap.b32 	%r1095, %r1094, %r1094, 20;
	add.s32 	%r1096, %r1090, %r1095;
	xor.b32  	%r1097, %r1096, %r1092;
	shf.l.wrap.b32 	%r1098, %r1097, %r1097, 24;
	add.s32 	%r1099, %r1098, %r1093;
	xor.b32  	%r1100, %r1099, %r1095;
	shf.l.wrap.b32 	%r1101, %r1100, %r1100, 25;
	add.s32 	%r1102, %r1061, %r988;
	add.s32 	%r1103, %r1102, %r1101;
	xor.b32  	%r1104, %r1103, %r1075;
	shf.l.wrap.b32 	%r1105, %r1104, %r1104, 16;
	add.s32 	%r1106, %r1105, %r1087;
	xor.b32  	%r1107, %r1106, %r1101;
	shf.l.wrap.b32 	%r1108, %r1107, %r1107, 20;
	add.s32 	%r1109, %r1103, %r1108;
	xor.b32  	%r1110, %r1109, %r1105;
	shf.l.wrap.b32 	%r1111, %r1110, %r1110, 24;
	add.s32 	%r1112, %r1111, %r1106;
	xor.b32  	%r1113, %r1112, %r1108;
	shf.l.wrap.b32 	%r1114, %r1113, %r1113, 25;
	add.s32 	%r1115, %r1073, %r996;
	add.s32 	%r1116, %r1115, %r1066;
	xor.b32  	%r1117, %r1116, %r1086;
	shf.l.wrap.b32 	%r1118, %r1117, %r1117, 16;
	add.s32 	%r1119, %r1118, %r1099;
	xor.b32  	%r1120, %r1119, %r1066;
	shf.l.wrap.b32 	%r1121, %r1120, %r1120, 20;
	add.s32 	%r1122, %r1116, %r1121;
	xor.b32  	%r1123, %r1122, %r1118;
	shf.l.wrap.b32 	%r1124, %r1123, %r1123, 24;
	add.s32 	%r1125, %r1124, %r1119;
	xor.b32  	%r1126, %r1125, %r1121;
	shf.l.wrap.b32 	%r1127, %r1126, %r1126, 25;
	add.s32 	%r1128, %r1084, %r1078;
	xor.b32  	%r1129, %r1098, %r1128;
	shf.l.wrap.b32 	%r1130, %r1129, %r1129, 16;
	add.s32 	%r1131, %r1130, %r1064;
	xor.b32  	%r1132, %r1131, %r1078;
	shf.l.wrap.b32 	%r1133, %r1132, %r1132, 20;
	add.s32 	%r1134, %r1128, %r972;
	add.s32 	%r1135, %r1134, %r1133;
	xor.b32  	%r1136, %r1135, %r1130;
	shf.l.wrap.b32 	%r1137, %r1136, %r1136, 24;
	add.s32 	%r1138, %r1137, %r1131;
	xor.b32  	%r1139, %r1138, %r1133;
	shf.l.wrap.b32 	%r1140, %r1139, %r1139, 25;
	add.s32 	%r1141, %r1096, %r1089;
	xor.b32  	%r1142, %r1063, %r1141;
	shf.l.wrap.b32 	%r1143, %r1142, %r1142, 16;
	add.s32 	%r1144, %r1143, %r1076;
	xor.b32  	%r1145, %r1144, %r1089;
	shf.l.wrap.b32 	%r1146, %r1145, %r1145, 20;
	add.s32 	%r1147, %r1141, %r1146;
	xor.b32  	%r1148, %r1147, %r1143;
	shf.l.wrap.b32 	%r1149, %r1148, %r1148, 24;
	add.s32 	%r1150, %r1149, %r1144;
	xor.b32  	%r1151, %r1150, %r1146;
	shf.l.wrap.b32 	%r1152, %r1151, %r1151, 25;
	add.s32 	%r1153, %r1109, %r980;
	add.s32 	%r1154, %r1153, %r1127;
	xor.b32  	%r1155, %r1154, %r1149;
	shf.l.wrap.b32 	%r1156, %r1155, %r1155, 16;
	add.s32 	%r1157, %r1156, %r1138;
	xor.b32  	%r1158, %r1157, %r1127;
	shf.l.wrap.b32 	%r1159, %r1158, %r1158, 20;
	add.s32 	%r1160, %r1154, %r1159;
	xor.b32  	%r1161, %r1160, %r1156;
	shf.l.wrap.b32 	%r1162, %r1161, %r1161, 24;
	add.s32 	%r1163, %r1162, %r1157;
	xor.b32  	%r1164, %r1163, %r1159;
	shf.l.wrap.b32 	%r1165, %r1164, %r1164, 25;
	add.s32 	%r1166, %r1140, %r1122;
	xor.b32  	%r1167, %r1111, %r1166;
	shf.l.wrap.b32 	%r1168, %r1167, %r1167, 16;
	add.s32 	%r1169, %r1168, %r1150;
	xor.b32  	%r1170, %r1169, %r1140;
	shf.l.wrap.b32 	%r1171, %r1170, %r1170, 20;
	add.s32 	%r1172, %r1166, %r1171;
	xor.b32  	%r1173, %r1172, %r1168;
	shf.l.wrap.b32 	%r1174, %r1173, %r1173, 24;
	add.s32 	%r1175, %r1174, %r1169;
	xor.b32  	%r1176, %r1175, %r1171;
	shf.l.wrap.b32 	%r1177, %r1176, %r1176, 25;
	add.s32 	%r1178, %r1135, %r1152;
	xor.b32  	%r1179, %r1124, %r1178;
	shf.l.wrap.b32 	%r1180, %r1179, %r1179, 16;
	add.s32 	%r1181, %r1180, %r1112;
	xor.b32  	%r1182, %r1181, %r1152;
	shf.l.wrap.b32 	%r1183, %r1182, %r1182, 20;
	add.s32 	%r1184, %r1178, %r1183;
	xor.b32  	%r1185, %r1184, %r1180;
	shf.l.wrap.b32 	%r1186, %r1185, %r1185, 24;
	add.s32 	%r1187, %r1186, %r1181;
	xor.b32  	%r1188, %r1187, %r1183;
	shf.l.wrap.b32 	%r1189, %r1188, %r1188, 25;
	add.s32 	%r1190, %r1147, %r1114;
	xor.b32  	%r1191, %r1190, %r1137;
	shf.l.wrap.b32 	%r1192, %r1191, %r1191, 16;
	add.s32 	%r1193, %r1192, %r1125;
	xor.b32  	%r1194, %r1193, %r1114;
	shf.l.wrap.b32 	%r1195, %r1194, %r1194, 20;
	add.s32 	%r1196, %r1190, %r1195;
	xor.b32  	%r1197, %r1196, %r1192;
	shf.l.wrap.b32 	%r1198, %r1197, %r1197, 24;
	add.s32 	%r1199, %r1198, %r1193;
	xor.b32  	%r1200, %r1199, %r1195;
	shf.l.wrap.b32 	%r1201, %r1200, %r1200, 25;
	add.s32 	%r1202, %r1160, %r996;
	add.s32 	%r1203, %r1202, %r1201;
	xor.b32  	%r1204, %r1203, %r1174;
	shf.l.wrap.b32 	%r1205, %r1204, %r1204, 16;
	add.s32 	%r1206, %r1205, %r1187;
	xor.b32  	%r1207, %r1206, %r1201;
	shf.l.wrap.b32 	%r1208, %r1207, %r1207, 20;
	add.s32 	%r1209, %r1203, %r1208;
	xor.b32  	%r1210, %r1209, %r1205;
	shf.l.wrap.b32 	%r1211, %r1210, %r1210, 24;
	add.s32 	%r1212, %r1211, %r1206;
	xor.b32  	%r1213, %r1212, %r1208;
	shf.l.wrap.b32 	%r1214, %r1213, %r1213, 25;
	add.s32 	%r1215, %r1172, %r1165;
	xor.b32  	%r1216, %r1215, %r1186;
	shf.l.wrap.b32 	%r1217, %r1216, %r1216, 16;
	add.s32 	%r1218, %r1217, %r1199;
	xor.b32  	%r1219, %r1218, %r1165;
	shf.l.wrap.b32 	%r1220, %r1219, %r1219, 20;
	add.s32 	%r1221, %r1215, %r1220;
	xor.b32  	%r1222, %r1221, %r1217;
	shf.l.wrap.b32 	%r1223, %r1222, %r1222, 24;
	add.s32 	%r1224, %r1223, %r1218;
	xor.b32  	%r1225, %r1224, %r1220;
	shf.l.wrap.b32 	%r1226, %r1225, %r1225, 25;
	add.s32 	%r1227, %r1184, %r1177;
	xor.b32  	%r1228, %r1198, %r1227;
	shf.l.wrap.b32 	%r1229, %r1228, %r1228, 16;
	add.s32 	%r1230, %r1229, %r1163;
	xor.b32  	%r1231, %r1230, %r1177;
	shf.l.wrap.b32 	%r1232, %r1231, %r1231, 20;
	add.s32 	%r1233, %r1227, %r988;
	add.s32 	%r1234, %r1233, %r1232;
	xor.b32  	%r1235, %r1234, %r1229;
	shf.l.wrap.b32 	%r1236, %r1235, %r1235, 24;
	add.s32 	%r1237, %r1236, %r1230;
	xor.b32  	%r1238, %r1237, %r1232;
	shf.l.wrap.b32 	%r1239, %r1238, %r1238, 25;
	add.s32 	%r1240, %r1196, %r1189;
	xor.b32  	%r1241, %r1162, %r1240;
	shf.l.wrap.b32 	%r1242, %r1241, %r1241, 16;
	add.s32 	%r1243, %r1242, %r1175;
	xor.b32  	%r1244, %r1243, %r1189;
	shf.l.wrap.b32 	%r1245, %r1244, %r1244, 20;
	add.s32 	%r1246, %r1240, %r1245;
	xor.b32  	%r1247, %r1246, %r1242;
	shf.l.wrap.b32 	%r1248, %r1247, %r1247, 24;
	add.s32 	%r1249, %r1248, %r1243;
	xor.b32  	%r1250, %r1249, %r1245;
	shf.l.wrap.b32 	%r1251, %r1250, %r1250, 25;
	add.s32 	%r1252, %r1209, %r1226;
	xor.b32  	%r1253, %r1252, %r1248;
	shf.l.wrap.b32 	%r1254, %r1253, %r1253, 16;
	add.s32 	%r1255, %r1254, %r1237;
	xor.b32  	%r1256, %r1255, %r1226;
	shf.l.wrap.b32 	%r1257, %r1256, %r1256, 20;
	add.s32 	%r1258, %r1252, %r1257;
	xor.b32  	%r1259, %r1258, %r1254;
	shf.l.wrap.b32 	%r1260, %r1259, %r1259, 24;
	add.s32 	%r1261, %r1260, %r1255;
	xor.b32  	%r1262, %r1261, %r1257;
	shf.l.wrap.b32 	%r1263, %r1262, %r1262, 25;
	add.s32 	%r1264, %r1239, %r1221;
	xor.b32  	%r1265, %r1211, %r1264;
	shf.l.wrap.b32 	%r1266, %r1265, %r1265, 16;
	add.s32 	%r1267, %r1266, %r1249;
	xor.b32  	%r1268, %r1267, %r1239;
	shf.l.wrap.b32 	%r1269, %r1268, %r1268, 20;
	add.s32 	%r1270, %r1264, %r972;
	add.s32 	%r1271, %r1270, %r1269;
	xor.b32  	%r1272, %r1271, %r1266;
	shf.l.wrap.b32 	%r1273, %r1272, %r1272, 24;
	add.s32 	%r1274, %r1273, %r1267;
	xor.b32  	%r1275, %r1274, %r1269;
	shf.l.wrap.b32 	%r1276, %r1275, %r1275, 25;
	add.s32 	%r1277, %r1234, %r1251;
	xor.b32  	%r1278, %r1223, %r1277;
	shf.l.wrap.b32 	%r1279, %r1278, %r1278, 16;
	add.s32 	%r1280, %r1279, %r1212;
	xor.b32  	%r1281, %r1280, %r1251;
	shf.l.wrap.b32 	%r1282, %r1281, %r1281, 20;
	add.s32 	%r1283, %r1277, %r1282;
	xor.b32  	%r1284, %r1283, %r1279;
	shf.l.wrap.b32 	%r1285, %r1284, %r1284, 24;
	add.s32 	%r1286, %r1285, %r1280;
	xor.b32  	%r1287, %r1286, %r1282;
	shf.l.wrap.b32 	%r1288, %r1287, %r1287, 25;
	add.s32 	%r1289, %r1246, %r1214;
	xor.b32  	%r1290, %r1289, %r1236;
	shf.l.wrap.b32 	%r1291, %r1290, %r1290, 16;
	add.s32 	%r1292, %r1291, %r1224;
	xor.b32  	%r1293, %r1292, %r1214;
	shf.l.wrap.b32 	%r1294, %r1293, %r1293, 20;
	add.s32 	%r1295, %r1289, %r980;
	add.s32 	%r1296, %r1295, %r1294;
	xor.b32  	%r1297, %r1296, %r1291;
	shf.l.wrap.b32 	%r1298, %r1297, %r1297, 24;
	add.s32 	%r1299, %r1298, %r1292;
	xor.b32  	%r1300, %r1299, %r1294;
	shf.l.wrap.b32 	%r1301, %r1300, %r1300, 25;
	add.s32 	%r1302, %r1258, %r1301;
	xor.b32  	%r1303, %r1302, %r1273;
	shf.l.wrap.b32 	%r1304, %r1303, %r1303, 16;
	add.s32 	%r1305, %r1304, %r1286;
	xor.b32  	%r1306, %r1305, %r1301;
	shf.l.wrap.b32 	%r1307, %r1306, %r1306, 20;
	add.s32 	%r1308, %r1302, %r1307;
	xor.b32  	%r1309, %r1308, %r1304;
	shf.l.wrap.b32 	%r1310, %r1309, %r1309, 24;
	add.s32 	%r1311, %r1310, %r1305;
	xor.b32  	%r1312, %r1311, %r1307;
	shf.l.wrap.b32 	%r1313, %r1312, %r1312, 25;
	add.s32 	%r1314, %r1271, %r1263;
	xor.b32  	%r1315, %r1314, %r1285;
	shf.l.wrap.b32 	%r1316, %r1315, %r1315, 16;
	add.s32 	%r1317, %r1316, %r1299;
	xor.b32  	%r1318, %r1317, %r1263;
	shf.l.wrap.b32 	%r1319, %r1318, %r1318, 20;
	add.s32 	%r1320, %r1314, %r1319;
	xor.b32  	%r1321, %r1320, %r1316;
	shf.l.wrap.b32 	%r1322, %r1321, %r1321, 24;
	add.s32 	%r1323, %r1322, %r1317;
	xor.b32  	%r1324, %r1323, %r1319;
	shf.l.wrap.b32 	%r1325, %r1324, %r1324, 25;
	add.s32 	%r1326, %r1283, %r1276;
	xor.b32  	%r1327, %r1298, %r1326;
	shf.l.wrap.b32 	%r1328, %r1327, %r1327, 16;
	add.s32 	%r1329, %r1328, %r1261;
	xor.b32  	%r1330, %r1329, %r1276;
	shf.l.wrap.b32 	%r1331, %r1330, %r1330, 20;
	add.s32 	%r1332, %r1326, %r996;
	add.s32 	%r1333, %r1332, %r1331;
	xor.b32  	%r1334, %r1333, %r1328;
	shf.l.wrap.b32 	%r1335, %r1334, %r1334, 24;
	add.s32 	%r1336, %r1335, %r1329;
	xor.b32  	%r1337, %r1336, %r1331;
	shf.l.wrap.b32 	%r1338, %r1337, %r1337, 25;
	add.s32 	%r1339, %r1296, %r1288;
	xor.b32  	%r1340, %r1260, %r1339;
	shf.l.wrap.b32 	%r1341, %r1340, %r1340, 16;
	add.s32 	%r1342, %r1341, %r1274;
	xor.b32  	%r1343, %r1342, %r1288;
	shf.l.wrap.b32 	%r1344, %r1343, %r1343, 20;
	add.s32 	%r1345, %r1339, %r1344;
	xor.b32  	%r1346, %r1345, %r1341;
	shf.l.wrap.b32 	%r1347, %r1346, %r1346, 24;
	add.s32 	%r1348, %r1347, %r1342;
	xor.b32  	%r1349, %r1348, %r1344;
	shf.l.wrap.b32 	%r1350, %r1349, %r1349, 25;
	add.s32 	%r1351, %r1308, %r1325;
	xor.b32  	%r1352, %r1351, %r1347;
	shf.l.wrap.b32 	%r1353, %r1352, %r1352, 16;
	add.s32 	%r1354, %r1353, %r1336;
	xor.b32  	%r1355, %r1354, %r1325;
	shf.l.wrap.b32 	%r1356, %r1355, %r1355, 20;
	add.s32 	%r1357, %r1351, %r972;
	add.s32 	%r1358, %r1357, %r1356;
	xor.b32  	%r1359, %r1358, %r1353;
	shf.l.wrap.b32 	%r1360, %r1359, %r1359, 24;
	add.s32 	%r1361, %r1360, %r1354;
	xor.b32  	%r1362, %r1361, %r1356;
	shf.l.wrap.b32 	%r1363, %r1362, %r1362, 25;
	add.s32 	%r1364, %r1338, %r1320;
	xor.b32  	%r1365, %r1310, %r1364;
	shf.l.wrap.b32 	%r1366, %r1365, %r1365, 16;
	add.s32 	%r1367, %r1366, %r1348;
	xor.b32  	%r1368, %r1367, %r1338;
	shf.l.wrap.b32 	%r1369, %r1368, %r1368, 20;
	add.s32 	%r1370, %r1364, %r988;
	add.s32 	%r1371, %r1370, %r1369;
	xor.b32  	%r1372, %r1371, %r1366;
	shf.l.wrap.b32 	%r1373, %r1372, %r1372, 24;
	add.s32 	%r1374, %r1373, %r1367;
	xor.b32  	%r1375, %r1374, %r1369;
	shf.l.wrap.b32 	%r1376, %r1375, %r1375, 25;
	add.s32 	%r1377, %r1333, %r1350;
	xor.b32  	%r1378, %r1322, %r1377;
	shf.l.wrap.b32 	%r1379, %r1378, %r1378, 16;
	add.s32 	%r1380, %r1379, %r1311;
	xor.b32  	%r1381, %r1380, %r1350;
	shf.l.wrap.b32 	%r1382, %r1381, %r1381, 20;
	add.s32 	%r1383, %r1377, %r1382;
	xor.b32  	%r1384, %r1383, %r1379;
	shf.l.wrap.b32 	%r1385, %r1384, %r1384, 24;
	add.s32 	%r1386, %r1385, %r1380;
	xor.b32  	%r1387, %r1386, %r1382;
	shf.l.wrap.b32 	%r1388, %r1387, %r1387, 25;
	add.s32 	%r1389, %r1345, %r980;
	add.s32 	%r1390, %r1389, %r1313;
	xor.b32  	%r1391, %r1390, %r1335;
	shf.l.wrap.b32 	%r1392, %r1391, %r1391, 16;
	add.s32 	%r1393, %r1392, %r1323;
	xor.b32  	%r1394, %r1393, %r1313;
	shf.l.wrap.b32 	%r1395, %r1394, %r1394, 20;
	add.s32 	%r1396, %r1390, %r1395;
	xor.b32  	%r1397, %r1396, %r1392;
	shf.l.wrap.b32 	%r1398, %r1397, %r1397, 24;
	add.s32 	%r1399, %r1398, %r1393;
	xor.b32  	%r1400, %r1399, %r1395;
	shf.l.wrap.b32 	%r1401, %r1400, %r1400, 25;
	add.s32 	%r1402, %r1358, %r1401;
	xor.b32  	%r1403, %r1402, %r1373;
	shf.l.wrap.b32 	%r1404, %r1403, %r1403, 16;
	add.s32 	%r1405, %r1404, %r1386;
	xor.b32  	%r1406, %r1405, %r1401;
	shf.l.wrap.b32 	%r1407, %r1406, %r1406, 20;
	add.s32 	%r1408, %r1402, %r1407;
	xor.b32  	%r1409, %r1408, %r1404;
	shf.l.wrap.b32 	%r1410, %r1409, %r1409, 24;
	add.s32 	%r1411, %r1410, %r1405;
	xor.b32  	%r1412, %r1411, %r1407;
	shf.l.wrap.b32 	%r1413, %r1412, %r1412, 25;
	add.s32 	%r1414, %r1371, %r1363;
	xor.b32  	%r1415, %r1414, %r1385;
	shf.l.wrap.b32 	%r1416, %r1415, %r1415, 16;
	add.s32 	%r1417, %r1416, %r1399;
	xor.b32  	%r1418, %r1417, %r1363;
	shf.l.wrap.b32 	%r1419, %r1418, %r1418, 20;
	add.s32 	%r1420, %r1414, %r1419;
	xor.b32  	%r1421, %r1420, %r1416;
	shf.l.wrap.b32 	%r1422, %r1421, %r1421, 24;
	add.s32 	%r1423, %r1422, %r1417;
	xor.b32  	%r1424, %r1423, %r1419;
	shf.l.wrap.b32 	%r1425, %r1424, %r1424, 25;
	add.s32 	%r1426, %r1383, %r1376;
	xor.b32  	%r1427, %r1398, %r1426;
	shf.l.wrap.b32 	%r1428, %r1427, %r1427, 16;
	add.s32 	%r1429, %r1428, %r1361;
	xor.b32  	%r1430, %r1429, %r1376;
	shf.l.wrap.b32 	%r1431, %r1430, %r1430, 20;
	add.s32 	%r1432, %r1426, %r1431;
	xor.b32  	%r1433, %r1432, %r1428;
	shf.l.wrap.b32 	%r1434, %r1433, %r1433, 24;
	add.s32 	%r1435, %r1434, %r1429;
	xor.b32  	%r1436, %r1435, %r1431;
	shf.l.wrap.b32 	%r1437, %r1436, %r1436, 25;
	add.s32 	%r1438, %r1396, %r1388;
	xor.b32  	%r1439, %r1360, %r1438;
	shf.l.wrap.b32 	%r1440, %r1439, %r1439, 16;
	add.s32 	%r1441, %r1440, %r1374;
	xor.b32  	%r1442, %r1441, %r1388;
	shf.l.wrap.b32 	%r1443, %r1442, %r1442, 20;
	add.s32 	%r1444, %r1438, %r1443;
	xor.b32  	%r1445, %r1444, %r1440;
	shf.l.wrap.b32 	%r1446, %r1445, %r1445, 24;
	add.s32 	%r1447, %r1446, %r1441;
	xor.b32  	%r1448, %r1447, %r1443;
	shf.l.wrap.b32 	%r1449, %r1448, %r1448, 25;
	add.s32 	%r1450, %r1408, %r1425;
	xor.b32  	%r1451, %r1450, %r1446;
	shf.l.wrap.b32 	%r1452, %r1451, %r1451, 16;
	add.s32 	%r1453, %r1452, %r1435;
	xor.b32  	%r1454, %r1453, %r1425;
	shf.l.wrap.b32 	%r1455, %r1454, %r1454, 20;
	add.s32 	%r1456, %r1450, %r988;
	add.s32 	%r1457, %r1456, %r1455;
	xor.b32  	%r1458, %r1457, %r1452;
	shf.l.wrap.b32 	%r1459, %r1458, %r1458, 24;
	add.s32 	%r1460, %r1459, %r1453;
	xor.b32  	%r1461, %r1460, %r1455;
	shf.l.wrap.b32 	%r1462, %r1461, %r1461, 25;
	add.s32 	%r1463, %r1437, %r1420;
	xor.b32  	%r1464, %r1410, %r1463;
	shf.l.wrap.b32 	%r1465, %r1464, %r1464, 16;
	add.s32 	%r1466, %r1465, %r1447;
	xor.b32  	%r1467, %r1466, %r1437;
	shf.l.wrap.b32 	%r1468, %r1467, %r1467, 20;
	add.s32 	%r1469, %r1463, %r996;
	add.s32 	%r1470, %r1469, %r1468;
	xor.b32  	%r1471, %r1470, %r1465;
	shf.l.wrap.b32 	%r1472, %r1471, %r1471, 24;
	add.s32 	%r1473, %r1472, %r1466;
	xor.b32  	%r1474, %r1473, %r1468;
	shf.l.wrap.b32 	%r1475, %r1474, %r1474, 25;
	add.s32 	%r1476, %r1432, %r972;
	add.s32 	%r1477, %r1476, %r1449;
	xor.b32  	%r1478, %r1422, %r1477;
	shf.l.wrap.b32 	%r1479, %r1478, %r1478, 16;
	add.s32 	%r1480, %r1479, %r1411;
	xor.b32  	%r1481, %r1480, %r1449;
	shf.l.wrap.b32 	%r1482, %r1481, %r1481, 20;
	add.s32 	%r1483, %r1477, %r980;
	add.s32 	%r1484, %r1483, %r1482;
	xor.b32  	%r1485, %r1484, %r1479;
	shf.l.wrap.b32 	%r1486, %r1485, %r1485, 24;
	add.s32 	%r1487, %r1486, %r1480;
	xor.b32  	%r1488, %r1487, %r1482;
	shf.l.wrap.b32 	%r1489, %r1488, %r1488, 25;
	add.s32 	%r1490, %r1444, %r1413;
	xor.b32  	%r1491, %r1490, %r1434;
	shf.l.wrap.b32 	%r1492, %r1491, %r1491, 16;
	add.s32 	%r1493, %r1492, %r1423;
	xor.b32  	%r1494, %r1493, %r1413;
	shf.l.wrap.b32 	%r1495, %r1494, %r1494, 20;
	add.s32 	%r1496, %r1490, %r1495;
	xor.b32  	%r1497, %r1496, %r1492;
	shf.l.wrap.b32 	%r1498, %r1497, %r1497, 24;
	add.s32 	%r1499, %r1498, %r1493;
	xor.b32  	%r1500, %r1499, %r1495;
	shf.l.wrap.b32 	%r1501, %r1500, %r1500, 25;
	add.s32 	%r1502, %r1457, %r1501;
	xor.b32  	%r1503, %r1502, %r1472;
	shf.l.wrap.b32 	%r1504, %r1503, %r1503, 16;
	add.s32 	%r1505, %r1504, %r1487;
	xor.b32  	%r1506, %r1505, %r1501;
	shf.l.wrap.b32 	%r1507, %r1506, %r1506, 20;
	add.s32 	%r1508, %r1502, %r1507;
	xor.b32  	%r1509, %r1508, %r1504;
	shf.l.wrap.b32 	%r1510, %r1509, %r1509, 24;
	add.s32 	%r1511, %r1510, %r1505;
	xor.b32  	%r1512, %r1511, %r1507;
	shf.l.wrap.b32 	%r1513, %r1512, %r1512, 25;
	add.s32 	%r1514, %r1470, %r1462;
	xor.b32  	%r1515, %r1514, %r1486;
	shf.l.wrap.b32 	%r1516, %r1515, %r1515, 16;
	add.s32 	%r1517, %r1516, %r1499;
	xor.b32  	%r1518, %r1517, %r1462;
	shf.l.wrap.b32 	%r1519, %r1518, %r1518, 20;
	add.s32 	%r1520, %r1514, %r1519;
	xor.b32  	%r1521, %r1520, %r1516;
	shf.l.wrap.b32 	%r1522, %r1521, %r1521, 24;
	add.s32 	%r1523, %r1522, %r1517;
	xor.b32  	%r1524, %r1523, %r1519;
	shf.l.wrap.b32 	%r1525, %r1524, %r1524, 25;
	add.s32 	%r1526, %r1484, %r1475;
	xor.b32  	%r1527, %r1498, %r1526;
	shf.l.wrap.b32 	%r1528, %r1527, %r1527, 16;
	add.s32 	%r1529, %r1528, %r1460;
	xor.b32  	%r1530, %r1529, %r1475;
	shf.l.wrap.b32 	%r1531, %r1530, %r1530, 20;
	add.s32 	%r1532, %r1526, %r1531;
	xor.b32  	%r1533, %r1532, %r1528;
	shf.l.wrap.b32 	%r1534, %r1533, %r1533, 24;
	add.s32 	%r1535, %r1534, %r1529;
	xor.b32  	%r1536, %r1535, %r1531;
	shf.l.wrap.b32 	%r1537, %r1536, %r1536, 25;
	add.s32 	%r1538, %r1496, %r1489;
	xor.b32  	%r1539, %r1459, %r1538;
	shf.l.wrap.b32 	%r1540, %r1539, %r1539, 16;
	add.s32 	%r1541, %r1540, %r1473;
	xor.b32  	%r1542, %r1541, %r1489;
	shf.l.wrap.b32 	%r1543, %r1542, %r1542, 20;
	add.s32 	%r1544, %r1538, %r980;
	add.s32 	%r1545, %r1544, %r1543;
	xor.b32  	%r1546, %r1545, %r1540;
	shf.l.wrap.b32 	%r1547, %r1546, %r1546, 24;
	add.s32 	%r1548, %r1547, %r1541;
	xor.b32  	%r1549, %r1548, %r1543;
	shf.l.wrap.b32 	%r1550, %r1549, %r1549, 25;
	add.s32 	%r1551, %r1508, %r1525;
	xor.b32  	%r1552, %r1551, %r1547;
	shf.l.wrap.b32 	%r1553, %r1552, %r1552, 16;
	add.s32 	%r1554, %r1553, %r1535;
	xor.b32  	%r1555, %r1554, %r1525;
	shf.l.wrap.b32 	%r1556, %r1555, %r1555, 20;
	add.s32 	%r1557, %r1551, %r996;
	add.s32 	%r1558, %r1557, %r1556;
	xor.b32  	%r1559, %r1558, %r1553;
	shf.l.wrap.b32 	%r1560, %r1559, %r1559, 24;
	add.s32 	%r1561, %r1560, %r1554;
	xor.b32  	%r1562, %r1561, %r1556;
	shf.l.wrap.b32 	%r1563, %r1562, %r1562, 25;
	add.s32 	%r1564, %r1537, %r972;
	add.s32 	%r1565, %r1564, %r1520;
	xor.b32  	%r1566, %r1510, %r1565;
	shf.l.wrap.b32 	%r1567, %r1566, %r1566, 16;
	add.s32 	%r1568, %r1567, %r1548;
	xor.b32  	%r1569, %r1568, %r1537;
	shf.l.wrap.b32 	%r1570, %r1569, %r1569, 20;
	add.s32 	%r1571, %r1565, %r1570;
	xor.b32  	%r1572, %r1571, %r1567;
	shf.l.wrap.b32 	%r1573, %r1572, %r1572, 24;
	add.s32 	%r1574, %r1573, %r1568;
	xor.b32  	%r1575, %r1574, %r1570;
	shf.l.wrap.b32 	%r1576, %r1575, %r1575, 25;
	add.s32 	%r1577, %r1532, %r988;
	add.s32 	%r1578, %r1577, %r1550;
	xor.b32  	%r1579, %r1522, %r1578;
	shf.l.wrap.b32 	%r1580, %r1579, %r1579, 16;
	add.s32 	%r1581, %r1580, %r1511;
	xor.b32  	%r1582, %r1581, %r1550;
	shf.l.wrap.b32 	%r1583, %r1582, %r1582, 20;
	add.s32 	%r1584, %r1578, %r1583;
	xor.b32  	%r1585, %r1584, %r1580;
	shf.l.wrap.b32 	%r1586, %r1585, %r1585, 24;
	add.s32 	%r1587, %r1586, %r1581;
	xor.b32  	%r1588, %r1587, %r1583;
	shf.l.wrap.b32 	%r1589, %r1588, %r1588, 25;
	add.s32 	%r1590, %r1545, %r1513;
	xor.b32  	%r1591, %r1590, %r1534;
	shf.l.wrap.b32 	%r1592, %r1591, %r1591, 16;
	add.s32 	%r1593, %r1592, %r1523;
	xor.b32  	%r1594, %r1593, %r1513;
	shf.l.wrap.b32 	%r1595, %r1594, %r1594, 20;
	add.s32 	%r1596, %r1590, %r1595;
	xor.b32  	%r1597, %r1596, %r1592;
	shf.l.wrap.b32 	%r1598, %r1597, %r1597, 24;
	add.s32 	%r1599, %r1598, %r1593;
	xor.b32  	%r1600, %r1599, %r1595;
	shf.l.wrap.b32 	%r1601, %r1600, %r1600, 25;
	add.s32 	%r1602, %r1558, %r1601;
	xor.b32  	%r1603, %r1602, %r1573;
	shf.l.wrap.b32 	%r1604, %r1603, %r1603, 16;
	add.s32 	%r1605, %r1604, %r1587;
	xor.b32  	%r1606, %r1605, %r1601;
	shf.l.wrap.b32 	%r1607, %r1606, %r1606, 20;
	add.s32 	%r1608, %r1602, %r1607;
	xor.b32  	%r1609, %r1608, %r1604;
	shf.l.wrap.b32 	%r1610, %r1609, %r1609, 24;
	add.s32 	%r1611, %r1610, %r1605;
	xor.b32  	%r1612, %r1611, %r1607;
	shf.l.wrap.b32 	%r1613, %r1612, %r1612, 25;
	add.s32 	%r1614, %r1571, %r1563;
	xor.b32  	%r1615, %r1614, %r1586;
	shf.l.wrap.b32 	%r1616, %r1615, %r1615, 16;
	add.s32 	%r1617, %r1616, %r1599;
	xor.b32  	%r1618, %r1617, %r1563;
	shf.l.wrap.b32 	%r1619, %r1618, %r1618, 20;
	add.s32 	%r1620, %r1614, %r972;
	add.s32 	%r1621, %r1620, %r1619;
	xor.b32  	%r1622, %r1621, %r1616;
	shf.l.wrap.b32 	%r1623, %r1622, %r1622, 24;
	add.s32 	%r1624, %r1623, %r1617;
	xor.b32  	%r1625, %r1624, %r1619;
	shf.l.wrap.b32 	%r1626, %r1625, %r1625, 25;
	add.s32 	%r1627, %r1584, %r980;
	add.s32 	%r1628, %r1627, %r1576;
	xor.b32  	%r1629, %r1598, %r1628;
	shf.l.wrap.b32 	%r1630, %r1629, %r1629, 16;
	add.s32 	%r1631, %r1630, %r1561;
	xor.b32  	%r1632, %r1631, %r1576;
	shf.l.wrap.b32 	%r1633, %r1632, %r1632, 20;
	add.s32 	%r1634, %r1628, %r1633;
	xor.b32  	%r1635, %r1634, %r1630;
	shf.l.wrap.b32 	%r1636, %r1635, %r1635, 24;
	add.s32 	%r1637, %r1636, %r1631;
	xor.b32  	%r1638, %r1637, %r1633;
	shf.l.wrap.b32 	%r1639, %r1638, %r1638, 25;
	add.s32 	%r1640, %r1596, %r1589;
	xor.b32  	%r1641, %r1560, %r1640;
	shf.l.wrap.b32 	%r1642, %r1641, %r1641, 16;
	add.s32 	%r1643, %r1642, %r1574;
	xor.b32  	%r1644, %r1643, %r1589;
	shf.l.wrap.b32 	%r1645, %r1644, %r1644, 20;
	add.s32 	%r1646, %r1640, %r1645;
	xor.b32  	%r1647, %r1646, %r1642;
	shf.l.wrap.b32 	%r1648, %r1647, %r1647, 24;
	add.s32 	%r1649, %r1648, %r1643;
	xor.b32  	%r1650, %r1649, %r1645;
	shf.l.wrap.b32 	%r1651, %r1650, %r1650, 25;
	add.s32 	%r1652, %r1608, %r1626;
	xor.b32  	%r1653, %r1652, %r1648;
	shf.l.wrap.b32 	%r1654, %r1653, %r1653, 16;
	add.s32 	%r1655, %r1654, %r1637;
	xor.b32  	%r1656, %r1655, %r1626;
	shf.l.wrap.b32 	%r1657, %r1656, %r1656, 20;
	add.s32 	%r1658, %r1652, %r1657;
	xor.b32  	%r1659, %r1658, %r1654;
	shf.l.wrap.b32 	%r1660, %r1659, %r1659, 24;
	add.s32 	%r1661, %r1660, %r1655;
	xor.b32  	%r1662, %r1661, %r1657;
	shf.l.wrap.b32 	%r1663, %r1662, %r1662, 25;
	add.s32 	%r1664, %r1639, %r988;
	add.s32 	%r1665, %r1664, %r1621;
	xor.b32  	%r1666, %r1610, %r1665;
	shf.l.wrap.b32 	%r1667, %r1666, %r1666, 16;
	add.s32 	%r1668, %r1667, %r1649;
	xor.b32  	%r1669, %r1668, %r1639;
	shf.l.wrap.b32 	%r1670, %r1669, %r1669, 20;
	add.s32 	%r1671, %r1665, %r1670;
	xor.b32  	%r1672, %r1671, %r1667;
	shf.l.wrap.b32 	%r1673, %r1672, %r1672, 24;
	add.s32 	%r1674, %r1673, %r1668;
	xor.b32  	%r1675, %r1674, %r1670;
	shf.l.wrap.b32 	%r1676, %r1675, %r1675, 25;
	add.s32 	%r1677, %r1634, %r996;
	add.s32 	%r1678, %r1677, %r1651;
	xor.b32  	%r1679, %r1623, %r1678;
	shf.l.wrap.b32 	%r1680, %r1679, %r1679, 16;
	add.s32 	%r1681, %r1680, %r1611;
	xor.b32  	%r1682, %r1681, %r1651;
	shf.l.wrap.b32 	%r1683, %r1682, %r1682, 20;
	add.s32 	%r1684, %r1678, %r1683;
	xor.b32  	%r1685, %r1684, %r1680;
	shf.l.wrap.b32 	%r1686, %r1685, %r1685, 24;
	add.s32 	%r1687, %r1686, %r1681;
	xor.b32  	%r1688, %r1687, %r1683;
	shf.l.wrap.b32 	%r1689, %r1688, %r1688, 25;
	add.s32 	%r1690, %r1646, %r1613;
	xor.b32  	%r1691, %r1690, %r1636;
	shf.l.wrap.b32 	%r1692, %r1691, %r1691, 16;
	add.s32 	%r1693, %r1692, %r1624;
	xor.b32  	%r1694, %r1693, %r1613;
	shf.l.wrap.b32 	%r1695, %r1694, %r1694, 20;
	add.s32 	%r1696, %r1690, %r1695;
	xor.b32  	%r1697, %r1696, %r1692;
	shf.l.wrap.b32 	%r1698, %r1697, %r1697, 24;
	add.s32 	%r1699, %r1698, %r1693;
	xor.b32  	%r1700, %r1699, %r1695;
	shf.l.wrap.b32 	%r1701, %r1700, %r1700, 25;
	xor.b32  	%r9, %r1658, %r1687;
	cvt.u64.u32 	%rd132, %r9;
	xor.b32  	%r1702, %r1699, %r1671;
	and.b32  	%r1703, %r1702, 255;
	cvt.u64.u32 	%rd133, %r1703;
	bfi.b64 	%rd134, %rd133, %rd132, 32, 32;
	cvt.u64.u32 	%rd135, %r1702;
	shl.b64 	%rd136, %rd135, 32;
	and.b64  	%rd137, %rd136, 280375465082880;
	or.b64  	%rd138, %rd134, %rd137;
	and.b64  	%rd139, %rd136, 71776119061217280;
	shr.u32 	%r10, %r1702, 24;
	cvt.u64.u32 	%rd140, %r10;
	shl.b64 	%rd141, %rd140, 56;
	or.b64  	%rd142, %rd138, %rd139;
	or.b64  	%rd143, %rd142, %rd141;
	xor.b32  	%r11, %r1661, %r1684;
	cvt.u64.u32 	%rd144, %r11;
	xor.b32  	%r1704, %r1696, %r1674;
	and.b32  	%r1705, %r1704, 255;
	cvt.u64.u32 	%rd145, %r1705;
	bfi.b64 	%rd146, %rd145, %rd144, 32, 32;
	cvt.u64.u32 	%rd147, %r1704;
	shl.b64 	%rd148, %rd147, 32;
	and.b64  	%rd149, %rd148, 280375465082880;
	or.b64  	%rd150, %rd146, %rd149;
	and.b64  	%rd151, %rd148, 71776119061217280;
	shr.u32 	%r12, %r1704, 24;
	cvt.u64.u32 	%rd152, %r12;
	shl.b64 	%rd153, %rd152, 56;
	or.b64  	%rd154, %rd150, %rd151;
	or.b64  	%rd155, %rd154, %rd153;
	xor.b32  	%r13, %r1701, %r1673;
	cvt.u64.u32 	%rd156, %r13;
	xor.b32  	%r1706, %r1663, %r1686;
	and.b32  	%r1707, %r1706, 255;
	cvt.u64.u32 	%rd157, %r1707;
	bfi.b64 	%rd158, %rd157, %rd156, 32, 32;
	cvt.u64.u32 	%rd159, %r1706;
	shl.b64 	%rd160, %rd159, 32;
	and.b64  	%rd161, %rd160, 280375465082880;
	or.b64  	%rd162, %rd158, %rd161;
	and.b64  	%rd163, %rd160, 71776119061217280;
	shr.u32 	%r14, %r1706, 24;
	cvt.u64.u32 	%rd164, %r14;
	shl.b64 	%rd165, %rd164, 56;
	or.b64  	%rd166, %rd162, %rd163;
	or.b64  	%rd167, %rd166, %rd165;
	xor.b32  	%r1708, %r1698, %r1676;
	cvt.u64.u32 	%rd168, %r1708;
	xor.b32  	%r1709, %r1660, %r1689;
	and.b32  	%r1710, %r1709, 255;
	cvt.u64.u32 	%rd169, %r1710;
	bfi.b64 	%rd170, %rd169, %rd168, 32, 32;
	cvt.u64.u32 	%rd171, %r1709;
	shl.b64 	%rd172, %rd171, 32;
	and.b64  	%rd173, %rd172, 280375465082880;
	or.b64  	%rd174, %rd170, %rd173;
	and.b64  	%rd175, %rd172, 71776119061217280;
	shr.u32 	%r1711, %r1709, 24;
	cvt.u64.u32 	%rd176, %r1711;
	shl.b64 	%rd177, %rd176, 56;
	or.b64  	%rd178, %rd174, %rd175;
	or.b64  	%rd9, %rd178, %rd177;
	shr.u64 	%rd10, %rd143, 32;
	shr.u64 	%rd11, %rd143, 40;
	shr.u64 	%rd12, %rd143, 48;
	shr.u64 	%rd13, %rd155, 32;
	shr.u64 	%rd14, %rd155, 40;
	shr.u64 	%rd15, %rd155, 48;
	shr.u64 	%rd16, %rd167, 32;
	shr.u64 	%rd17, %rd167, 40;
	shr.u64 	%rd18, %rd167, 48;
	shr.u32 	%r5809, %r9, 12;
	shr.u32 	%r5810, %r9, 8;
	shr.u32 	%r5811, %r9, 4;
	and.b32  	%r5812, %r5811, 15;
	and.b32  	%r5813, %r9, 15;
	bfi.b32 	%r5814, %r5813, %r5812, 8, 4;
	shl.b32 	%r5815, %r9, 4;
	and.b32  	%r5816, %r5815, 983040;
	or.b32  	%r5817, %r5814, %r5816;
	shl.b32 	%r5818, %r9, 16;
	and.b32  	%r5819, %r5818, 251658240;
	or.b32  	%r5746, %r5817, %r5819;
	shr.u32 	%r5820, %r9, 20;
	and.b32  	%r5821, %r5820, 15;
	shr.u32 	%r5822, %r9, 16;
	and.b32  	%r5823, %r5822, 15;
	shr.u32 	%r5824, %r9, 24;
	bfi.b32 	%r5825, %r5823, %r5821, 8, 4;
	and.b32  	%r5826, %r5809, 983040;
	or.b32  	%r5827, %r5825, %r5826;
	and.b32  	%r5828, %r9, 251658240;
	or.b32  	%r5750, %r5827, %r5828;
	cvt.u16.u64 	%rs83, %rd10;
	and.b16  	%rs84, %rs83, 240;
	shr.u16 	%rs85, %rs84, 4;
	cvt.u16.u64 	%rs86, %rd11;
	and.b16  	%rs87, %rs86, 240;
	shr.u16 	%rs88, %rs87, 4;
	cvt.u32.u16 	%r5829, %rs85;
	cvt.u32.u64 	%r5830, %rd10;
	and.b32  	%r5831, %r5830, 15;
	prmt.b32 	%r5832, %r5831, %r5829, 30212;
	cvt.u32.u16 	%r5833, %rs88;
	prmt.b32 	%r5834, %r5833, %r5832, 28756;
	cvt.u32.u64 	%r5835, %rd11;
	shl.b32 	%r5836, %r5835, 24;
	and.b32  	%r5837, %r5836, 251658240;
	or.b32  	%r5754, %r5834, %r5837;
	cvt.u16.u64 	%rs89, %rd12;
	and.b16  	%rs90, %rs89, 240;
	shr.u16 	%rs91, %rs90, 4;
	cvt.u32.u16 	%r5838, %rs91;
	cvt.u32.u64 	%r5839, %rd12;
	and.b32  	%r5840, %r5839, 15;
	prmt.b32 	%r5841, %r5840, %r5838, 30212;
	shl.b32 	%r5842, %r10, 12;
	and.b32  	%r5843, %r5842, 983040;
	or.b32  	%r5844, %r5841, %r5843;
	shl.b32 	%r5845, %r10, 24;
	and.b32  	%r5846, %r5845, 251658240;
	or.b32  	%r5758, %r5844, %r5846;
	shr.u32 	%r5847, %r11, 12;
	shr.u32 	%r5848, %r11, 8;
	shr.u32 	%r5849, %r11, 4;
	and.b32  	%r5850, %r5849, 15;
	and.b32  	%r5851, %r11, 15;
	bfi.b32 	%r5852, %r5851, %r5850, 8, 4;
	shl.b32 	%r5853, %r11, 4;
	and.b32  	%r5854, %r5853, 983040;
	or.b32  	%r5855, %r5852, %r5854;
	shl.b32 	%r5856, %r11, 16;
	and.b32  	%r5857, %r5856, 251658240;
	or.b32  	%r5762, %r5855, %r5857;
	shr.u32 	%r5858, %r11, 20;
	and.b32  	%r5859, %r5858, 15;
	shr.u32 	%r5860, %r11, 16;
	and.b32  	%r5861, %r5860, 15;
	shr.u32 	%r5862, %r11, 24;
	bfi.b32 	%r5863, %r5861, %r5859, 8, 4;
	and.b32  	%r5864, %r5847, 983040;
	or.b32  	%r5865, %r5863, %r5864;
	and.b32  	%r5866, %r11, 251658240;
	or.b32  	%r5766, %r5865, %r5866;
	cvt.u16.u64 	%rs92, %rd13;
	and.b16  	%rs93, %rs92, 240;
	shr.u16 	%rs94, %rs93, 4;
	cvt.u16.u64 	%rs95, %rd14;
	and.b16  	%rs96, %rs95, 240;
	shr.u16 	%rs97, %rs96, 4;
	cvt.u32.u16 	%r5867, %rs94;
	cvt.u32.u64 	%r5868, %rd13;
	and.b32  	%r5869, %r5868, 15;
	prmt.b32 	%r5870, %r5869, %r5867, 30212;
	cvt.u32.u16 	%r5871, %rs97;
	prmt.b32 	%r5872, %r5871, %r5870, 28756;
	cvt.u32.u64 	%r5873, %rd14;
	shl.b32 	%r5874, %r5873, 24;
	and.b32  	%r5875, %r5874, 251658240;
	or.b32  	%r5770, %r5872, %r5875;
	cvt.u16.u64 	%rs98, %rd15;
	and.b16  	%rs99, %rs98, 240;
	shr.u16 	%rs100, %rs99, 4;
	cvt.u32.u16 	%r5876, %rs100;
	cvt.u32.u64 	%r5877, %rd15;
	and.b32  	%r5878, %r5877, 15;
	prmt.b32 	%r5879, %r5878, %r5876, 30212;
	shl.b32 	%r5880, %r12, 12;
	and.b32  	%r5881, %r5880, 983040;
	or.b32  	%r5882, %r5879, %r5881;
	shl.b32 	%r5883, %r12, 24;
	and.b32  	%r5884, %r5883, 251658240;
	or.b32  	%r5774, %r5882, %r5884;
	shr.u32 	%r5885, %r13, 12;
	shr.u32 	%r5886, %r13, 8;
	shr.u32 	%r5887, %r13, 4;
	and.b32  	%r5888, %r5887, 15;
	and.b32  	%r5889, %r13, 15;
	bfi.b32 	%r5890, %r5889, %r5888, 8, 4;
	shl.b32 	%r5891, %r13, 4;
	and.b32  	%r5892, %r5891, 983040;
	or.b32  	%r5893, %r5890, %r5892;
	shl.b32 	%r5894, %r13, 16;
	and.b32  	%r5895, %r5894, 251658240;
	or.b32  	%r5778, %r5893, %r5895;
	shr.u32 	%r5896, %r13, 20;
	and.b32  	%r5897, %r5896, 15;
	shr.u32 	%r5898, %r13, 16;
	and.b32  	%r5899, %r5898, 15;
	shr.u32 	%r5900, %r13, 24;
	bfi.b32 	%r5901, %r5899, %r5897, 8, 4;
	and.b32  	%r5902, %r5885, 983040;
	or.b32  	%r5903, %r5901, %r5902;
	and.b32  	%r5904, %r13, 251658240;
	or.b32  	%r5782, %r5903, %r5904;
	cvt.u16.u64 	%rs101, %rd16;
	and.b16  	%rs102, %rs101, 240;
	shr.u16 	%rs103, %rs102, 4;
	cvt.u16.u64 	%rs104, %rd17;
	and.b16  	%rs105, %rs104, 240;
	shr.u16 	%rs106, %rs105, 4;
	cvt.u32.u16 	%r5905, %rs103;
	cvt.u32.u64 	%r5906, %rd16;
	and.b32  	%r5907, %r5906, 15;
	prmt.b32 	%r5908, %r5907, %r5905, 30212;
	cvt.u32.u16 	%r5909, %rs106;
	prmt.b32 	%r5910, %r5909, %r5908, 28756;
	cvt.u32.u64 	%r5911, %rd17;
	shl.b32 	%r5912, %r5911, 24;
	and.b32  	%r5913, %r5912, 251658240;
	or.b32  	%r5786, %r5910, %r5913;
	cvt.u16.u64 	%rs107, %rd18;
	and.b16  	%rs108, %rs107, 240;
	shr.u16 	%rs109, %rs108, 4;
	cvt.u32.u16 	%r5914, %rs109;
	cvt.u32.u64 	%r5915, %rd18;
	and.b32  	%r5916, %r5915, 15;
	prmt.b32 	%r5917, %r5916, %r5914, 30212;
	shl.b32 	%r5918, %r14, 12;
	and.b32  	%r5919, %r5918, 983040;
	or.b32  	%r5920, %r5917, %r5919;
	shl.b32 	%r5921, %r14, 24;
	and.b32  	%r5922, %r5921, 251658240;
	or.b32  	%r5790, %r5920, %r5922;
	cvt.u16.u64 	%rs110, %rd9;
	and.b16  	%rs111, %rs110, 240;
	shr.u16 	%rs112, %rs111, 4;
	shr.u64 	%rd201, %rd9, 8;
	cvt.u32.u64 	%r5923, %rd201;
	cvt.u32.u64 	%r5924, %rd9;
	shr.u32 	%r5925, %r5924, 12;
	cvt.u32.u16 	%r5926, %rs112;
	and.b32  	%r5927, %r5924, 15;
	prmt.b32 	%r5928, %r5927, %r5926, 30212;
	shl.b32 	%r5929, %r5924, 4;
	and.b32  	%r5930, %r5929, 983040;
	or.b32  	%r5931, %r5928, %r5930;
	shl.b32 	%r5932, %r5923, 24;
	and.b32  	%r5933, %r5932, 251658240;
	or.b32  	%r5794, %r5931, %r5933;
	shr.u64 	%rd202, %rd9, 16;
	cvt.u32.u64 	%r5934, %rd202;
	shr.u32 	%r5935, %r5924, 20;
	and.b32  	%r5936, %r5935, 15;
	and.b32  	%r5937, %r5934, 15;
	shr.u64 	%rd203, %rd9, 24;
	cvt.u32.u64 	%r5938, %rd203;
	bfi.b32 	%r5939, %r5937, %r5936, 8, 4;
	and.b32  	%r5940, %r5925, 983040;
	or.b32  	%r5941, %r5939, %r5940;
	shl.b32 	%r5942, %r5938, 24;
	and.b32  	%r5943, %r5942, 251658240;
	or.b32  	%r5798, %r5941, %r5943;
	shr.u64 	%rd204, %rd9, 32;
	cvt.u32.u64 	%r5944, %rd204;
	shr.u64 	%rd205, %rd9, 36;
	cvt.u32.u64 	%r5945, %rd205;
	and.b32  	%r5946, %r5945, 15;
	and.b32  	%r5947, %r5944, 15;
	shr.u64 	%rd206, %rd9, 40;
	cvt.u32.u64 	%r5948, %rd206;
	shr.u64 	%rd207, %rd9, 44;
	cvt.u32.u64 	%r5949, %rd207;
	bfi.b32 	%r5950, %r5947, %r5946, 8, 4;
	shl.b32 	%r5951, %r5949, 16;
	and.b32  	%r5952, %r5951, 983040;
	or.b32  	%r5953, %r5950, %r5952;
	shl.b32 	%r5954, %r5948, 24;
	and.b32  	%r5955, %r5954, 251658240;
	or.b32  	%r5802, %r5953, %r5955;
	shr.u64 	%rd208, %rd9, 48;
	cvt.u32.u64 	%r5956, %rd208;
	shr.u64 	%rd209, %rd9, 52;
	cvt.u32.u64 	%r5957, %rd209;
	and.b32  	%r5958, %r5957, 15;
	and.b32  	%r5959, %r5956, 15;
	shr.u64 	%rd210, %rd9, 56;
	cvt.u32.u64 	%r5960, %rd210;
	bfi.b32 	%r5961, %r5959, %r5958, 8, 4;
	and.b32  	%r5962, %r5949, 983040;
	or.b32  	%r5963, %r5961, %r5962;
	shl.b32 	%r5964, %r5960, 24;
	and.b32  	%r5965, %r5964, 251658240;
	or.b32  	%r5806, %r5963, %r5965;
	ld.const.u32 	%r1713, [matrix];
	mov.u32 	%r6244, 0;
	// begin inline asm
	dp4a.u32.u32 %r1712, %r1713, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r1717, [matrix+4];
	// begin inline asm
	dp4a.u32.u32 %r1716, %r1717, %r5750, %r1712;
	// end inline asm
	ld.const.u32 	%r1721, [matrix+8];
	// begin inline asm
	dp4a.u32.u32 %r1720, %r1721, %r5754, %r1716;
	// end inline asm
	ld.const.u32 	%r1725, [matrix+12];
	// begin inline asm
	dp4a.u32.u32 %r1724, %r1725, %r5758, %r1720;
	// end inline asm
	ld.const.u32 	%r1729, [matrix+16];
	// begin inline asm
	dp4a.u32.u32 %r1728, %r1729, %r5762, %r1724;
	// end inline asm
	ld.const.u32 	%r1733, [matrix+20];
	// begin inline asm
	dp4a.u32.u32 %r1732, %r1733, %r5766, %r1728;
	// end inline asm
	ld.const.u32 	%r1737, [matrix+24];
	// begin inline asm
	dp4a.u32.u32 %r1736, %r1737, %r5770, %r1732;
	// end inline asm
	ld.const.u32 	%r1741, [matrix+28];
	// begin inline asm
	dp4a.u32.u32 %r1740, %r1741, %r5774, %r1736;
	// end inline asm
	ld.const.u32 	%r1745, [matrix+32];
	// begin inline asm
	dp4a.u32.u32 %r1744, %r1745, %r5778, %r1740;
	// end inline asm
	ld.const.u32 	%r1749, [matrix+36];
	// begin inline asm
	dp4a.u32.u32 %r1748, %r1749, %r5782, %r1744;
	// end inline asm
	ld.const.u32 	%r1753, [matrix+40];
	// begin inline asm
	dp4a.u32.u32 %r1752, %r1753, %r5786, %r1748;
	// end inline asm
	ld.const.u32 	%r1757, [matrix+44];
	// begin inline asm
	dp4a.u32.u32 %r1756, %r1757, %r5790, %r1752;
	// end inline asm
	ld.const.u32 	%r1761, [matrix+48];
	// begin inline asm
	dp4a.u32.u32 %r1760, %r1761, %r5794, %r1756;
	// end inline asm
	ld.const.u32 	%r1765, [matrix+52];
	// begin inline asm
	dp4a.u32.u32 %r1764, %r1765, %r5798, %r1760;
	// end inline asm
	ld.const.u32 	%r1769, [matrix+56];
	// begin inline asm
	dp4a.u32.u32 %r1768, %r1769, %r5802, %r1764;
	// end inline asm
	ld.const.u32 	%r1773, [matrix+60];
	// begin inline asm
	dp4a.u32.u32 %r1772, %r1773, %r5806, %r1768;
	// end inline asm
	ld.const.u32 	%r1777, [matrix+64];
	// begin inline asm
	dp4a.u32.u32 %r1776, %r1777, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r1781, [matrix+68];
	// begin inline asm
	dp4a.u32.u32 %r1780, %r1781, %r5750, %r1776;
	// end inline asm
	ld.const.u32 	%r1785, [matrix+72];
	// begin inline asm
	dp4a.u32.u32 %r1784, %r1785, %r5754, %r1780;
	// end inline asm
	ld.const.u32 	%r1789, [matrix+76];
	// begin inline asm
	dp4a.u32.u32 %r1788, %r1789, %r5758, %r1784;
	// end inline asm
	ld.const.u32 	%r1793, [matrix+80];
	// begin inline asm
	dp4a.u32.u32 %r1792, %r1793, %r5762, %r1788;
	// end inline asm
	ld.const.u32 	%r1797, [matrix+84];
	// begin inline asm
	dp4a.u32.u32 %r1796, %r1797, %r5766, %r1792;
	// end inline asm
	ld.const.u32 	%r1801, [matrix+88];
	// begin inline asm
	dp4a.u32.u32 %r1800, %r1801, %r5770, %r1796;
	// end inline asm
	ld.const.u32 	%r1805, [matrix+92];
	// begin inline asm
	dp4a.u32.u32 %r1804, %r1805, %r5774, %r1800;
	// end inline asm
	ld.const.u32 	%r1809, [matrix+96];
	// begin inline asm
	dp4a.u32.u32 %r1808, %r1809, %r5778, %r1804;
	// end inline asm
	ld.const.u32 	%r1813, [matrix+100];
	// begin inline asm
	dp4a.u32.u32 %r1812, %r1813, %r5782, %r1808;
	// end inline asm
	ld.const.u32 	%r1817, [matrix+104];
	// begin inline asm
	dp4a.u32.u32 %r1816, %r1817, %r5786, %r1812;
	// end inline asm
	ld.const.u32 	%r1821, [matrix+108];
	// begin inline asm
	dp4a.u32.u32 %r1820, %r1821, %r5790, %r1816;
	// end inline asm
	ld.const.u32 	%r1825, [matrix+112];
	// begin inline asm
	dp4a.u32.u32 %r1824, %r1825, %r5794, %r1820;
	// end inline asm
	ld.const.u32 	%r1829, [matrix+116];
	// begin inline asm
	dp4a.u32.u32 %r1828, %r1829, %r5798, %r1824;
	// end inline asm
	ld.const.u32 	%r1833, [matrix+120];
	// begin inline asm
	dp4a.u32.u32 %r1832, %r1833, %r5802, %r1828;
	// end inline asm
	ld.const.u32 	%r1837, [matrix+124];
	// begin inline asm
	dp4a.u32.u32 %r1836, %r1837, %r5806, %r1832;
	// end inline asm
	shr.u32 	%r5966, %r1772, 6;
	and.b32  	%r5967, %r5966, 240;
	shr.u32 	%r5968, %r1836, 10;
	or.b32  	%r5969, %r5968, %r5967;
	xor.b32  	%r5970, %r9, %r5969;
	ld.const.u32 	%r1841, [matrix+128];
	// begin inline asm
	dp4a.u32.u32 %r1840, %r1841, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r1845, [matrix+132];
	// begin inline asm
	dp4a.u32.u32 %r1844, %r1845, %r5750, %r1840;
	// end inline asm
	ld.const.u32 	%r1849, [matrix+136];
	// begin inline asm
	dp4a.u32.u32 %r1848, %r1849, %r5754, %r1844;
	// end inline asm
	ld.const.u32 	%r1853, [matrix+140];
	// begin inline asm
	dp4a.u32.u32 %r1852, %r1853, %r5758, %r1848;
	// end inline asm
	ld.const.u32 	%r1857, [matrix+144];
	// begin inline asm
	dp4a.u32.u32 %r1856, %r1857, %r5762, %r1852;
	// end inline asm
	ld.const.u32 	%r1861, [matrix+148];
	// begin inline asm
	dp4a.u32.u32 %r1860, %r1861, %r5766, %r1856;
	// end inline asm
	ld.const.u32 	%r1865, [matrix+152];
	// begin inline asm
	dp4a.u32.u32 %r1864, %r1865, %r5770, %r1860;
	// end inline asm
	ld.const.u32 	%r1869, [matrix+156];
	// begin inline asm
	dp4a.u32.u32 %r1868, %r1869, %r5774, %r1864;
	// end inline asm
	ld.const.u32 	%r1873, [matrix+160];
	// begin inline asm
	dp4a.u32.u32 %r1872, %r1873, %r5778, %r1868;
	// end inline asm
	ld.const.u32 	%r1877, [matrix+164];
	// begin inline asm
	dp4a.u32.u32 %r1876, %r1877, %r5782, %r1872;
	// end inline asm
	ld.const.u32 	%r1881, [matrix+168];
	// begin inline asm
	dp4a.u32.u32 %r1880, %r1881, %r5786, %r1876;
	// end inline asm
	ld.const.u32 	%r1885, [matrix+172];
	// begin inline asm
	dp4a.u32.u32 %r1884, %r1885, %r5790, %r1880;
	// end inline asm
	ld.const.u32 	%r1889, [matrix+176];
	// begin inline asm
	dp4a.u32.u32 %r1888, %r1889, %r5794, %r1884;
	// end inline asm
	ld.const.u32 	%r1893, [matrix+180];
	// begin inline asm
	dp4a.u32.u32 %r1892, %r1893, %r5798, %r1888;
	// end inline asm
	ld.const.u32 	%r1897, [matrix+184];
	// begin inline asm
	dp4a.u32.u32 %r1896, %r1897, %r5802, %r1892;
	// end inline asm
	ld.const.u32 	%r1901, [matrix+188];
	// begin inline asm
	dp4a.u32.u32 %r1900, %r1901, %r5806, %r1896;
	// end inline asm
	ld.const.u32 	%r1905, [matrix+192];
	// begin inline asm
	dp4a.u32.u32 %r1904, %r1905, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r1909, [matrix+196];
	// begin inline asm
	dp4a.u32.u32 %r1908, %r1909, %r5750, %r1904;
	// end inline asm
	ld.const.u32 	%r1913, [matrix+200];
	// begin inline asm
	dp4a.u32.u32 %r1912, %r1913, %r5754, %r1908;
	// end inline asm
	ld.const.u32 	%r1917, [matrix+204];
	// begin inline asm
	dp4a.u32.u32 %r1916, %r1917, %r5758, %r1912;
	// end inline asm
	ld.const.u32 	%r1921, [matrix+208];
	// begin inline asm
	dp4a.u32.u32 %r1920, %r1921, %r5762, %r1916;
	// end inline asm
	ld.const.u32 	%r1925, [matrix+212];
	// begin inline asm
	dp4a.u32.u32 %r1924, %r1925, %r5766, %r1920;
	// end inline asm
	ld.const.u32 	%r1929, [matrix+216];
	// begin inline asm
	dp4a.u32.u32 %r1928, %r1929, %r5770, %r1924;
	// end inline asm
	ld.const.u32 	%r1933, [matrix+220];
	// begin inline asm
	dp4a.u32.u32 %r1932, %r1933, %r5774, %r1928;
	// end inline asm
	ld.const.u32 	%r1937, [matrix+224];
	// begin inline asm
	dp4a.u32.u32 %r1936, %r1937, %r5778, %r1932;
	// end inline asm
	ld.const.u32 	%r1941, [matrix+228];
	// begin inline asm
	dp4a.u32.u32 %r1940, %r1941, %r5782, %r1936;
	// end inline asm
	ld.const.u32 	%r1945, [matrix+232];
	// begin inline asm
	dp4a.u32.u32 %r1944, %r1945, %r5786, %r1940;
	// end inline asm
	ld.const.u32 	%r1949, [matrix+236];
	// begin inline asm
	dp4a.u32.u32 %r1948, %r1949, %r5790, %r1944;
	// end inline asm
	ld.const.u32 	%r1953, [matrix+240];
	// begin inline asm
	dp4a.u32.u32 %r1952, %r1953, %r5794, %r1948;
	// end inline asm
	ld.const.u32 	%r1957, [matrix+244];
	// begin inline asm
	dp4a.u32.u32 %r1956, %r1957, %r5798, %r1952;
	// end inline asm
	ld.const.u32 	%r1961, [matrix+248];
	// begin inline asm
	dp4a.u32.u32 %r1960, %r1961, %r5802, %r1956;
	// end inline asm
	ld.const.u32 	%r1965, [matrix+252];
	// begin inline asm
	dp4a.u32.u32 %r1964, %r1965, %r5806, %r1960;
	// end inline asm
	shr.u32 	%r5971, %r1900, 6;
	and.b32  	%r5972, %r5971, 240;
	shr.u32 	%r5973, %r1964, 10;
	or.b32  	%r5974, %r5973, %r5972;
	xor.b32  	%r5975, %r5810, %r5974;
	ld.const.u32 	%r1969, [matrix+256];
	// begin inline asm
	dp4a.u32.u32 %r1968, %r1969, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r1973, [matrix+260];
	// begin inline asm
	dp4a.u32.u32 %r1972, %r1973, %r5750, %r1968;
	// end inline asm
	ld.const.u32 	%r1977, [matrix+264];
	// begin inline asm
	dp4a.u32.u32 %r1976, %r1977, %r5754, %r1972;
	// end inline asm
	ld.const.u32 	%r1981, [matrix+268];
	// begin inline asm
	dp4a.u32.u32 %r1980, %r1981, %r5758, %r1976;
	// end inline asm
	ld.const.u32 	%r1985, [matrix+272];
	// begin inline asm
	dp4a.u32.u32 %r1984, %r1985, %r5762, %r1980;
	// end inline asm
	ld.const.u32 	%r1989, [matrix+276];
	// begin inline asm
	dp4a.u32.u32 %r1988, %r1989, %r5766, %r1984;
	// end inline asm
	ld.const.u32 	%r1993, [matrix+280];
	// begin inline asm
	dp4a.u32.u32 %r1992, %r1993, %r5770, %r1988;
	// end inline asm
	ld.const.u32 	%r1997, [matrix+284];
	// begin inline asm
	dp4a.u32.u32 %r1996, %r1997, %r5774, %r1992;
	// end inline asm
	ld.const.u32 	%r2001, [matrix+288];
	// begin inline asm
	dp4a.u32.u32 %r2000, %r2001, %r5778, %r1996;
	// end inline asm
	ld.const.u32 	%r2005, [matrix+292];
	// begin inline asm
	dp4a.u32.u32 %r2004, %r2005, %r5782, %r2000;
	// end inline asm
	ld.const.u32 	%r2009, [matrix+296];
	// begin inline asm
	dp4a.u32.u32 %r2008, %r2009, %r5786, %r2004;
	// end inline asm
	ld.const.u32 	%r2013, [matrix+300];
	// begin inline asm
	dp4a.u32.u32 %r2012, %r2013, %r5790, %r2008;
	// end inline asm
	ld.const.u32 	%r2017, [matrix+304];
	// begin inline asm
	dp4a.u32.u32 %r2016, %r2017, %r5794, %r2012;
	// end inline asm
	ld.const.u32 	%r2021, [matrix+308];
	// begin inline asm
	dp4a.u32.u32 %r2020, %r2021, %r5798, %r2016;
	// end inline asm
	ld.const.u32 	%r2025, [matrix+312];
	// begin inline asm
	dp4a.u32.u32 %r2024, %r2025, %r5802, %r2020;
	// end inline asm
	ld.const.u32 	%r2029, [matrix+316];
	// begin inline asm
	dp4a.u32.u32 %r2028, %r2029, %r5806, %r2024;
	// end inline asm
	ld.const.u32 	%r2033, [matrix+320];
	// begin inline asm
	dp4a.u32.u32 %r2032, %r2033, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2037, [matrix+324];
	// begin inline asm
	dp4a.u32.u32 %r2036, %r2037, %r5750, %r2032;
	// end inline asm
	ld.const.u32 	%r2041, [matrix+328];
	// begin inline asm
	dp4a.u32.u32 %r2040, %r2041, %r5754, %r2036;
	// end inline asm
	ld.const.u32 	%r2045, [matrix+332];
	// begin inline asm
	dp4a.u32.u32 %r2044, %r2045, %r5758, %r2040;
	// end inline asm
	ld.const.u32 	%r2049, [matrix+336];
	// begin inline asm
	dp4a.u32.u32 %r2048, %r2049, %r5762, %r2044;
	// end inline asm
	ld.const.u32 	%r2053, [matrix+340];
	// begin inline asm
	dp4a.u32.u32 %r2052, %r2053, %r5766, %r2048;
	// end inline asm
	ld.const.u32 	%r2057, [matrix+344];
	// begin inline asm
	dp4a.u32.u32 %r2056, %r2057, %r5770, %r2052;
	// end inline asm
	ld.const.u32 	%r2061, [matrix+348];
	// begin inline asm
	dp4a.u32.u32 %r2060, %r2061, %r5774, %r2056;
	// end inline asm
	ld.const.u32 	%r2065, [matrix+352];
	// begin inline asm
	dp4a.u32.u32 %r2064, %r2065, %r5778, %r2060;
	// end inline asm
	ld.const.u32 	%r2069, [matrix+356];
	// begin inline asm
	dp4a.u32.u32 %r2068, %r2069, %r5782, %r2064;
	// end inline asm
	ld.const.u32 	%r2073, [matrix+360];
	// begin inline asm
	dp4a.u32.u32 %r2072, %r2073, %r5786, %r2068;
	// end inline asm
	ld.const.u32 	%r2077, [matrix+364];
	// begin inline asm
	dp4a.u32.u32 %r2076, %r2077, %r5790, %r2072;
	// end inline asm
	ld.const.u32 	%r2081, [matrix+368];
	// begin inline asm
	dp4a.u32.u32 %r2080, %r2081, %r5794, %r2076;
	// end inline asm
	ld.const.u32 	%r2085, [matrix+372];
	// begin inline asm
	dp4a.u32.u32 %r2084, %r2085, %r5798, %r2080;
	// end inline asm
	ld.const.u32 	%r2089, [matrix+376];
	// begin inline asm
	dp4a.u32.u32 %r2088, %r2089, %r5802, %r2084;
	// end inline asm
	ld.const.u32 	%r2093, [matrix+380];
	// begin inline asm
	dp4a.u32.u32 %r2092, %r2093, %r5806, %r2088;
	// end inline asm
	shr.u32 	%r5976, %r2028, 6;
	and.b32  	%r5977, %r5976, 240;
	shr.u32 	%r5978, %r2092, 10;
	or.b32  	%r5979, %r5978, %r5977;
	xor.b32  	%r5980, %r5822, %r5979;
	ld.const.u32 	%r2097, [matrix+384];
	// begin inline asm
	dp4a.u32.u32 %r2096, %r2097, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2101, [matrix+388];
	// begin inline asm
	dp4a.u32.u32 %r2100, %r2101, %r5750, %r2096;
	// end inline asm
	ld.const.u32 	%r2105, [matrix+392];
	// begin inline asm
	dp4a.u32.u32 %r2104, %r2105, %r5754, %r2100;
	// end inline asm
	ld.const.u32 	%r2109, [matrix+396];
	// begin inline asm
	dp4a.u32.u32 %r2108, %r2109, %r5758, %r2104;
	// end inline asm
	ld.const.u32 	%r2113, [matrix+400];
	// begin inline asm
	dp4a.u32.u32 %r2112, %r2113, %r5762, %r2108;
	// end inline asm
	ld.const.u32 	%r2117, [matrix+404];
	// begin inline asm
	dp4a.u32.u32 %r2116, %r2117, %r5766, %r2112;
	// end inline asm
	ld.const.u32 	%r2121, [matrix+408];
	// begin inline asm
	dp4a.u32.u32 %r2120, %r2121, %r5770, %r2116;
	// end inline asm
	ld.const.u32 	%r2125, [matrix+412];
	// begin inline asm
	dp4a.u32.u32 %r2124, %r2125, %r5774, %r2120;
	// end inline asm
	ld.const.u32 	%r2129, [matrix+416];
	// begin inline asm
	dp4a.u32.u32 %r2128, %r2129, %r5778, %r2124;
	// end inline asm
	ld.const.u32 	%r2133, [matrix+420];
	// begin inline asm
	dp4a.u32.u32 %r2132, %r2133, %r5782, %r2128;
	// end inline asm
	ld.const.u32 	%r2137, [matrix+424];
	// begin inline asm
	dp4a.u32.u32 %r2136, %r2137, %r5786, %r2132;
	// end inline asm
	ld.const.u32 	%r2141, [matrix+428];
	// begin inline asm
	dp4a.u32.u32 %r2140, %r2141, %r5790, %r2136;
	// end inline asm
	ld.const.u32 	%r2145, [matrix+432];
	// begin inline asm
	dp4a.u32.u32 %r2144, %r2145, %r5794, %r2140;
	// end inline asm
	ld.const.u32 	%r2149, [matrix+436];
	// begin inline asm
	dp4a.u32.u32 %r2148, %r2149, %r5798, %r2144;
	// end inline asm
	ld.const.u32 	%r2153, [matrix+440];
	// begin inline asm
	dp4a.u32.u32 %r2152, %r2153, %r5802, %r2148;
	// end inline asm
	ld.const.u32 	%r2157, [matrix+444];
	// begin inline asm
	dp4a.u32.u32 %r2156, %r2157, %r5806, %r2152;
	// end inline asm
	ld.const.u32 	%r2161, [matrix+448];
	// begin inline asm
	dp4a.u32.u32 %r2160, %r2161, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2165, [matrix+452];
	// begin inline asm
	dp4a.u32.u32 %r2164, %r2165, %r5750, %r2160;
	// end inline asm
	ld.const.u32 	%r2169, [matrix+456];
	// begin inline asm
	dp4a.u32.u32 %r2168, %r2169, %r5754, %r2164;
	// end inline asm
	ld.const.u32 	%r2173, [matrix+460];
	// begin inline asm
	dp4a.u32.u32 %r2172, %r2173, %r5758, %r2168;
	// end inline asm
	ld.const.u32 	%r2177, [matrix+464];
	// begin inline asm
	dp4a.u32.u32 %r2176, %r2177, %r5762, %r2172;
	// end inline asm
	ld.const.u32 	%r2181, [matrix+468];
	// begin inline asm
	dp4a.u32.u32 %r2180, %r2181, %r5766, %r2176;
	// end inline asm
	ld.const.u32 	%r2185, [matrix+472];
	// begin inline asm
	dp4a.u32.u32 %r2184, %r2185, %r5770, %r2180;
	// end inline asm
	ld.const.u32 	%r2189, [matrix+476];
	// begin inline asm
	dp4a.u32.u32 %r2188, %r2189, %r5774, %r2184;
	// end inline asm
	ld.const.u32 	%r2193, [matrix+480];
	// begin inline asm
	dp4a.u32.u32 %r2192, %r2193, %r5778, %r2188;
	// end inline asm
	ld.const.u32 	%r2197, [matrix+484];
	// begin inline asm
	dp4a.u32.u32 %r2196, %r2197, %r5782, %r2192;
	// end inline asm
	ld.const.u32 	%r2201, [matrix+488];
	// begin inline asm
	dp4a.u32.u32 %r2200, %r2201, %r5786, %r2196;
	// end inline asm
	ld.const.u32 	%r2205, [matrix+492];
	// begin inline asm
	dp4a.u32.u32 %r2204, %r2205, %r5790, %r2200;
	// end inline asm
	ld.const.u32 	%r2209, [matrix+496];
	// begin inline asm
	dp4a.u32.u32 %r2208, %r2209, %r5794, %r2204;
	// end inline asm
	ld.const.u32 	%r2213, [matrix+500];
	// begin inline asm
	dp4a.u32.u32 %r2212, %r2213, %r5798, %r2208;
	// end inline asm
	ld.const.u32 	%r2217, [matrix+504];
	// begin inline asm
	dp4a.u32.u32 %r2216, %r2217, %r5802, %r2212;
	// end inline asm
	ld.const.u32 	%r2221, [matrix+508];
	// begin inline asm
	dp4a.u32.u32 %r2220, %r2221, %r5806, %r2216;
	// end inline asm
	shr.u32 	%r5981, %r2156, 6;
	and.b32  	%r5982, %r5981, 240;
	shr.u32 	%r5983, %r2220, 10;
	or.b32  	%r5984, %r5983, %r5982;
	xor.b32  	%r5985, %r5824, %r5984;
	ld.const.u32 	%r2225, [matrix+512];
	// begin inline asm
	dp4a.u32.u32 %r2224, %r2225, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2229, [matrix+516];
	// begin inline asm
	dp4a.u32.u32 %r2228, %r2229, %r5750, %r2224;
	// end inline asm
	ld.const.u32 	%r2233, [matrix+520];
	// begin inline asm
	dp4a.u32.u32 %r2232, %r2233, %r5754, %r2228;
	// end inline asm
	ld.const.u32 	%r2237, [matrix+524];
	// begin inline asm
	dp4a.u32.u32 %r2236, %r2237, %r5758, %r2232;
	// end inline asm
	ld.const.u32 	%r2241, [matrix+528];
	// begin inline asm
	dp4a.u32.u32 %r2240, %r2241, %r5762, %r2236;
	// end inline asm
	ld.const.u32 	%r2245, [matrix+532];
	// begin inline asm
	dp4a.u32.u32 %r2244, %r2245, %r5766, %r2240;
	// end inline asm
	ld.const.u32 	%r2249, [matrix+536];
	// begin inline asm
	dp4a.u32.u32 %r2248, %r2249, %r5770, %r2244;
	// end inline asm
	ld.const.u32 	%r2253, [matrix+540];
	// begin inline asm
	dp4a.u32.u32 %r2252, %r2253, %r5774, %r2248;
	// end inline asm
	ld.const.u32 	%r2257, [matrix+544];
	// begin inline asm
	dp4a.u32.u32 %r2256, %r2257, %r5778, %r2252;
	// end inline asm
	ld.const.u32 	%r2261, [matrix+548];
	// begin inline asm
	dp4a.u32.u32 %r2260, %r2261, %r5782, %r2256;
	// end inline asm
	ld.const.u32 	%r2265, [matrix+552];
	// begin inline asm
	dp4a.u32.u32 %r2264, %r2265, %r5786, %r2260;
	// end inline asm
	ld.const.u32 	%r2269, [matrix+556];
	// begin inline asm
	dp4a.u32.u32 %r2268, %r2269, %r5790, %r2264;
	// end inline asm
	ld.const.u32 	%r2273, [matrix+560];
	// begin inline asm
	dp4a.u32.u32 %r2272, %r2273, %r5794, %r2268;
	// end inline asm
	ld.const.u32 	%r2277, [matrix+564];
	// begin inline asm
	dp4a.u32.u32 %r2276, %r2277, %r5798, %r2272;
	// end inline asm
	ld.const.u32 	%r2281, [matrix+568];
	// begin inline asm
	dp4a.u32.u32 %r2280, %r2281, %r5802, %r2276;
	// end inline asm
	ld.const.u32 	%r2285, [matrix+572];
	// begin inline asm
	dp4a.u32.u32 %r2284, %r2285, %r5806, %r2280;
	// end inline asm
	ld.const.u32 	%r2289, [matrix+576];
	// begin inline asm
	dp4a.u32.u32 %r2288, %r2289, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2293, [matrix+580];
	// begin inline asm
	dp4a.u32.u32 %r2292, %r2293, %r5750, %r2288;
	// end inline asm
	ld.const.u32 	%r2297, [matrix+584];
	// begin inline asm
	dp4a.u32.u32 %r2296, %r2297, %r5754, %r2292;
	// end inline asm
	ld.const.u32 	%r2301, [matrix+588];
	// begin inline asm
	dp4a.u32.u32 %r2300, %r2301, %r5758, %r2296;
	// end inline asm
	ld.const.u32 	%r2305, [matrix+592];
	// begin inline asm
	dp4a.u32.u32 %r2304, %r2305, %r5762, %r2300;
	// end inline asm
	ld.const.u32 	%r2309, [matrix+596];
	// begin inline asm
	dp4a.u32.u32 %r2308, %r2309, %r5766, %r2304;
	// end inline asm
	ld.const.u32 	%r2313, [matrix+600];
	// begin inline asm
	dp4a.u32.u32 %r2312, %r2313, %r5770, %r2308;
	// end inline asm
	ld.const.u32 	%r2317, [matrix+604];
	// begin inline asm
	dp4a.u32.u32 %r2316, %r2317, %r5774, %r2312;
	// end inline asm
	ld.const.u32 	%r2321, [matrix+608];
	// begin inline asm
	dp4a.u32.u32 %r2320, %r2321, %r5778, %r2316;
	// end inline asm
	ld.const.u32 	%r2325, [matrix+612];
	// begin inline asm
	dp4a.u32.u32 %r2324, %r2325, %r5782, %r2320;
	// end inline asm
	ld.const.u32 	%r2329, [matrix+616];
	// begin inline asm
	dp4a.u32.u32 %r2328, %r2329, %r5786, %r2324;
	// end inline asm
	ld.const.u32 	%r2333, [matrix+620];
	// begin inline asm
	dp4a.u32.u32 %r2332, %r2333, %r5790, %r2328;
	// end inline asm
	ld.const.u32 	%r2337, [matrix+624];
	// begin inline asm
	dp4a.u32.u32 %r2336, %r2337, %r5794, %r2332;
	// end inline asm
	ld.const.u32 	%r2341, [matrix+628];
	// begin inline asm
	dp4a.u32.u32 %r2340, %r2341, %r5798, %r2336;
	// end inline asm
	ld.const.u32 	%r2345, [matrix+632];
	// begin inline asm
	dp4a.u32.u32 %r2344, %r2345, %r5802, %r2340;
	// end inline asm
	ld.const.u32 	%r2349, [matrix+636];
	// begin inline asm
	dp4a.u32.u32 %r2348, %r2349, %r5806, %r2344;
	// end inline asm
	shr.u32 	%r5986, %r2284, 6;
	and.b32  	%r5987, %r5986, 240;
	shr.u32 	%r5988, %r2348, 10;
	or.b32  	%r5989, %r5988, %r5987;
	cvt.u64.u32 	%rd211, %r5989;
	xor.b64  	%rd212, %rd10, %rd211;
	ld.const.u32 	%r2353, [matrix+640];
	// begin inline asm
	dp4a.u32.u32 %r2352, %r2353, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2357, [matrix+644];
	// begin inline asm
	dp4a.u32.u32 %r2356, %r2357, %r5750, %r2352;
	// end inline asm
	ld.const.u32 	%r2361, [matrix+648];
	// begin inline asm
	dp4a.u32.u32 %r2360, %r2361, %r5754, %r2356;
	// end inline asm
	ld.const.u32 	%r2365, [matrix+652];
	// begin inline asm
	dp4a.u32.u32 %r2364, %r2365, %r5758, %r2360;
	// end inline asm
	ld.const.u32 	%r2369, [matrix+656];
	// begin inline asm
	dp4a.u32.u32 %r2368, %r2369, %r5762, %r2364;
	// end inline asm
	ld.const.u32 	%r2373, [matrix+660];
	// begin inline asm
	dp4a.u32.u32 %r2372, %r2373, %r5766, %r2368;
	// end inline asm
	ld.const.u32 	%r2377, [matrix+664];
	// begin inline asm
	dp4a.u32.u32 %r2376, %r2377, %r5770, %r2372;
	// end inline asm
	ld.const.u32 	%r2381, [matrix+668];
	// begin inline asm
	dp4a.u32.u32 %r2380, %r2381, %r5774, %r2376;
	// end inline asm
	ld.const.u32 	%r2385, [matrix+672];
	// begin inline asm
	dp4a.u32.u32 %r2384, %r2385, %r5778, %r2380;
	// end inline asm
	ld.const.u32 	%r2389, [matrix+676];
	// begin inline asm
	dp4a.u32.u32 %r2388, %r2389, %r5782, %r2384;
	// end inline asm
	ld.const.u32 	%r2393, [matrix+680];
	// begin inline asm
	dp4a.u32.u32 %r2392, %r2393, %r5786, %r2388;
	// end inline asm
	ld.const.u32 	%r2397, [matrix+684];
	// begin inline asm
	dp4a.u32.u32 %r2396, %r2397, %r5790, %r2392;
	// end inline asm
	ld.const.u32 	%r2401, [matrix+688];
	// begin inline asm
	dp4a.u32.u32 %r2400, %r2401, %r5794, %r2396;
	// end inline asm
	ld.const.u32 	%r2405, [matrix+692];
	// begin inline asm
	dp4a.u32.u32 %r2404, %r2405, %r5798, %r2400;
	// end inline asm
	ld.const.u32 	%r2409, [matrix+696];
	// begin inline asm
	dp4a.u32.u32 %r2408, %r2409, %r5802, %r2404;
	// end inline asm
	ld.const.u32 	%r2413, [matrix+700];
	// begin inline asm
	dp4a.u32.u32 %r2412, %r2413, %r5806, %r2408;
	// end inline asm
	ld.const.u32 	%r2417, [matrix+704];
	// begin inline asm
	dp4a.u32.u32 %r2416, %r2417, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2421, [matrix+708];
	// begin inline asm
	dp4a.u32.u32 %r2420, %r2421, %r5750, %r2416;
	// end inline asm
	ld.const.u32 	%r2425, [matrix+712];
	// begin inline asm
	dp4a.u32.u32 %r2424, %r2425, %r5754, %r2420;
	// end inline asm
	ld.const.u32 	%r2429, [matrix+716];
	// begin inline asm
	dp4a.u32.u32 %r2428, %r2429, %r5758, %r2424;
	// end inline asm
	ld.const.u32 	%r2433, [matrix+720];
	// begin inline asm
	dp4a.u32.u32 %r2432, %r2433, %r5762, %r2428;
	// end inline asm
	ld.const.u32 	%r2437, [matrix+724];
	// begin inline asm
	dp4a.u32.u32 %r2436, %r2437, %r5766, %r2432;
	// end inline asm
	ld.const.u32 	%r2441, [matrix+728];
	// begin inline asm
	dp4a.u32.u32 %r2440, %r2441, %r5770, %r2436;
	// end inline asm
	ld.const.u32 	%r2445, [matrix+732];
	// begin inline asm
	dp4a.u32.u32 %r2444, %r2445, %r5774, %r2440;
	// end inline asm
	ld.const.u32 	%r2449, [matrix+736];
	// begin inline asm
	dp4a.u32.u32 %r2448, %r2449, %r5778, %r2444;
	// end inline asm
	ld.const.u32 	%r2453, [matrix+740];
	// begin inline asm
	dp4a.u32.u32 %r2452, %r2453, %r5782, %r2448;
	// end inline asm
	ld.const.u32 	%r2457, [matrix+744];
	// begin inline asm
	dp4a.u32.u32 %r2456, %r2457, %r5786, %r2452;
	// end inline asm
	ld.const.u32 	%r2461, [matrix+748];
	// begin inline asm
	dp4a.u32.u32 %r2460, %r2461, %r5790, %r2456;
	// end inline asm
	ld.const.u32 	%r2465, [matrix+752];
	// begin inline asm
	dp4a.u32.u32 %r2464, %r2465, %r5794, %r2460;
	// end inline asm
	ld.const.u32 	%r2469, [matrix+756];
	// begin inline asm
	dp4a.u32.u32 %r2468, %r2469, %r5798, %r2464;
	// end inline asm
	ld.const.u32 	%r2473, [matrix+760];
	// begin inline asm
	dp4a.u32.u32 %r2472, %r2473, %r5802, %r2468;
	// end inline asm
	ld.const.u32 	%r2477, [matrix+764];
	// begin inline asm
	dp4a.u32.u32 %r2476, %r2477, %r5806, %r2472;
	// end inline asm
	shr.u32 	%r5990, %r2412, 6;
	and.b32  	%r5991, %r5990, 240;
	shr.u32 	%r5992, %r2476, 10;
	or.b32  	%r5993, %r5992, %r5991;
	cvt.u64.u32 	%rd213, %r5993;
	xor.b64  	%rd214, %rd11, %rd213;
	ld.const.u32 	%r2481, [matrix+768];
	// begin inline asm
	dp4a.u32.u32 %r2480, %r2481, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2485, [matrix+772];
	// begin inline asm
	dp4a.u32.u32 %r2484, %r2485, %r5750, %r2480;
	// end inline asm
	ld.const.u32 	%r2489, [matrix+776];
	// begin inline asm
	dp4a.u32.u32 %r2488, %r2489, %r5754, %r2484;
	// end inline asm
	ld.const.u32 	%r2493, [matrix+780];
	// begin inline asm
	dp4a.u32.u32 %r2492, %r2493, %r5758, %r2488;
	// end inline asm
	ld.const.u32 	%r2497, [matrix+784];
	// begin inline asm
	dp4a.u32.u32 %r2496, %r2497, %r5762, %r2492;
	// end inline asm
	ld.const.u32 	%r2501, [matrix+788];
	// begin inline asm
	dp4a.u32.u32 %r2500, %r2501, %r5766, %r2496;
	// end inline asm
	ld.const.u32 	%r2505, [matrix+792];
	// begin inline asm
	dp4a.u32.u32 %r2504, %r2505, %r5770, %r2500;
	// end inline asm
	ld.const.u32 	%r2509, [matrix+796];
	// begin inline asm
	dp4a.u32.u32 %r2508, %r2509, %r5774, %r2504;
	// end inline asm
	ld.const.u32 	%r2513, [matrix+800];
	// begin inline asm
	dp4a.u32.u32 %r2512, %r2513, %r5778, %r2508;
	// end inline asm
	ld.const.u32 	%r2517, [matrix+804];
	// begin inline asm
	dp4a.u32.u32 %r2516, %r2517, %r5782, %r2512;
	// end inline asm
	ld.const.u32 	%r2521, [matrix+808];
	// begin inline asm
	dp4a.u32.u32 %r2520, %r2521, %r5786, %r2516;
	// end inline asm
	ld.const.u32 	%r2525, [matrix+812];
	// begin inline asm
	dp4a.u32.u32 %r2524, %r2525, %r5790, %r2520;
	// end inline asm
	ld.const.u32 	%r2529, [matrix+816];
	// begin inline asm
	dp4a.u32.u32 %r2528, %r2529, %r5794, %r2524;
	// end inline asm
	ld.const.u32 	%r2533, [matrix+820];
	// begin inline asm
	dp4a.u32.u32 %r2532, %r2533, %r5798, %r2528;
	// end inline asm
	ld.const.u32 	%r2537, [matrix+824];
	// begin inline asm
	dp4a.u32.u32 %r2536, %r2537, %r5802, %r2532;
	// end inline asm
	ld.const.u32 	%r2541, [matrix+828];
	// begin inline asm
	dp4a.u32.u32 %r2540, %r2541, %r5806, %r2536;
	// end inline asm
	ld.const.u32 	%r2545, [matrix+832];
	// begin inline asm
	dp4a.u32.u32 %r2544, %r2545, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2549, [matrix+836];
	// begin inline asm
	dp4a.u32.u32 %r2548, %r2549, %r5750, %r2544;
	// end inline asm
	ld.const.u32 	%r2553, [matrix+840];
	// begin inline asm
	dp4a.u32.u32 %r2552, %r2553, %r5754, %r2548;
	// end inline asm
	ld.const.u32 	%r2557, [matrix+844];
	// begin inline asm
	dp4a.u32.u32 %r2556, %r2557, %r5758, %r2552;
	// end inline asm
	ld.const.u32 	%r2561, [matrix+848];
	// begin inline asm
	dp4a.u32.u32 %r2560, %r2561, %r5762, %r2556;
	// end inline asm
	ld.const.u32 	%r2565, [matrix+852];
	// begin inline asm
	dp4a.u32.u32 %r2564, %r2565, %r5766, %r2560;
	// end inline asm
	ld.const.u32 	%r2569, [matrix+856];
	// begin inline asm
	dp4a.u32.u32 %r2568, %r2569, %r5770, %r2564;
	// end inline asm
	ld.const.u32 	%r2573, [matrix+860];
	// begin inline asm
	dp4a.u32.u32 %r2572, %r2573, %r5774, %r2568;
	// end inline asm
	ld.const.u32 	%r2577, [matrix+864];
	// begin inline asm
	dp4a.u32.u32 %r2576, %r2577, %r5778, %r2572;
	// end inline asm
	ld.const.u32 	%r2581, [matrix+868];
	// begin inline asm
	dp4a.u32.u32 %r2580, %r2581, %r5782, %r2576;
	// end inline asm
	ld.const.u32 	%r2585, [matrix+872];
	// begin inline asm
	dp4a.u32.u32 %r2584, %r2585, %r5786, %r2580;
	// end inline asm
	ld.const.u32 	%r2589, [matrix+876];
	// begin inline asm
	dp4a.u32.u32 %r2588, %r2589, %r5790, %r2584;
	// end inline asm
	ld.const.u32 	%r2593, [matrix+880];
	// begin inline asm
	dp4a.u32.u32 %r2592, %r2593, %r5794, %r2588;
	// end inline asm
	ld.const.u32 	%r2597, [matrix+884];
	// begin inline asm
	dp4a.u32.u32 %r2596, %r2597, %r5798, %r2592;
	// end inline asm
	ld.const.u32 	%r2601, [matrix+888];
	// begin inline asm
	dp4a.u32.u32 %r2600, %r2601, %r5802, %r2596;
	// end inline asm
	ld.const.u32 	%r2605, [matrix+892];
	// begin inline asm
	dp4a.u32.u32 %r2604, %r2605, %r5806, %r2600;
	// end inline asm
	shr.u32 	%r5994, %r2540, 6;
	and.b32  	%r5995, %r5994, 240;
	shr.u32 	%r5996, %r2604, 10;
	or.b32  	%r5997, %r5996, %r5995;
	cvt.u64.u32 	%rd215, %r5997;
	xor.b64  	%rd216, %rd12, %rd215;
	ld.const.u32 	%r2609, [matrix+896];
	// begin inline asm
	dp4a.u32.u32 %r2608, %r2609, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2613, [matrix+900];
	// begin inline asm
	dp4a.u32.u32 %r2612, %r2613, %r5750, %r2608;
	// end inline asm
	ld.const.u32 	%r2617, [matrix+904];
	// begin inline asm
	dp4a.u32.u32 %r2616, %r2617, %r5754, %r2612;
	// end inline asm
	ld.const.u32 	%r2621, [matrix+908];
	// begin inline asm
	dp4a.u32.u32 %r2620, %r2621, %r5758, %r2616;
	// end inline asm
	ld.const.u32 	%r2625, [matrix+912];
	// begin inline asm
	dp4a.u32.u32 %r2624, %r2625, %r5762, %r2620;
	// end inline asm
	ld.const.u32 	%r2629, [matrix+916];
	// begin inline asm
	dp4a.u32.u32 %r2628, %r2629, %r5766, %r2624;
	// end inline asm
	ld.const.u32 	%r2633, [matrix+920];
	// begin inline asm
	dp4a.u32.u32 %r2632, %r2633, %r5770, %r2628;
	// end inline asm
	ld.const.u32 	%r2637, [matrix+924];
	// begin inline asm
	dp4a.u32.u32 %r2636, %r2637, %r5774, %r2632;
	// end inline asm
	ld.const.u32 	%r2641, [matrix+928];
	// begin inline asm
	dp4a.u32.u32 %r2640, %r2641, %r5778, %r2636;
	// end inline asm
	ld.const.u32 	%r2645, [matrix+932];
	// begin inline asm
	dp4a.u32.u32 %r2644, %r2645, %r5782, %r2640;
	// end inline asm
	ld.const.u32 	%r2649, [matrix+936];
	// begin inline asm
	dp4a.u32.u32 %r2648, %r2649, %r5786, %r2644;
	// end inline asm
	ld.const.u32 	%r2653, [matrix+940];
	// begin inline asm
	dp4a.u32.u32 %r2652, %r2653, %r5790, %r2648;
	// end inline asm
	ld.const.u32 	%r2657, [matrix+944];
	// begin inline asm
	dp4a.u32.u32 %r2656, %r2657, %r5794, %r2652;
	// end inline asm
	ld.const.u32 	%r2661, [matrix+948];
	// begin inline asm
	dp4a.u32.u32 %r2660, %r2661, %r5798, %r2656;
	// end inline asm
	ld.const.u32 	%r2665, [matrix+952];
	// begin inline asm
	dp4a.u32.u32 %r2664, %r2665, %r5802, %r2660;
	// end inline asm
	ld.const.u32 	%r2669, [matrix+956];
	// begin inline asm
	dp4a.u32.u32 %r2668, %r2669, %r5806, %r2664;
	// end inline asm
	ld.const.u32 	%r2673, [matrix+960];
	// begin inline asm
	dp4a.u32.u32 %r2672, %r2673, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2677, [matrix+964];
	// begin inline asm
	dp4a.u32.u32 %r2676, %r2677, %r5750, %r2672;
	// end inline asm
	ld.const.u32 	%r2681, [matrix+968];
	// begin inline asm
	dp4a.u32.u32 %r2680, %r2681, %r5754, %r2676;
	// end inline asm
	ld.const.u32 	%r2685, [matrix+972];
	// begin inline asm
	dp4a.u32.u32 %r2684, %r2685, %r5758, %r2680;
	// end inline asm
	ld.const.u32 	%r2689, [matrix+976];
	// begin inline asm
	dp4a.u32.u32 %r2688, %r2689, %r5762, %r2684;
	// end inline asm
	ld.const.u32 	%r2693, [matrix+980];
	// begin inline asm
	dp4a.u32.u32 %r2692, %r2693, %r5766, %r2688;
	// end inline asm
	ld.const.u32 	%r2697, [matrix+984];
	// begin inline asm
	dp4a.u32.u32 %r2696, %r2697, %r5770, %r2692;
	// end inline asm
	ld.const.u32 	%r2701, [matrix+988];
	// begin inline asm
	dp4a.u32.u32 %r2700, %r2701, %r5774, %r2696;
	// end inline asm
	ld.const.u32 	%r2705, [matrix+992];
	// begin inline asm
	dp4a.u32.u32 %r2704, %r2705, %r5778, %r2700;
	// end inline asm
	ld.const.u32 	%r2709, [matrix+996];
	// begin inline asm
	dp4a.u32.u32 %r2708, %r2709, %r5782, %r2704;
	// end inline asm
	ld.const.u32 	%r2713, [matrix+1000];
	// begin inline asm
	dp4a.u32.u32 %r2712, %r2713, %r5786, %r2708;
	// end inline asm
	ld.const.u32 	%r2717, [matrix+1004];
	// begin inline asm
	dp4a.u32.u32 %r2716, %r2717, %r5790, %r2712;
	// end inline asm
	ld.const.u32 	%r2721, [matrix+1008];
	// begin inline asm
	dp4a.u32.u32 %r2720, %r2721, %r5794, %r2716;
	// end inline asm
	ld.const.u32 	%r2725, [matrix+1012];
	// begin inline asm
	dp4a.u32.u32 %r2724, %r2725, %r5798, %r2720;
	// end inline asm
	ld.const.u32 	%r2729, [matrix+1016];
	// begin inline asm
	dp4a.u32.u32 %r2728, %r2729, %r5802, %r2724;
	// end inline asm
	ld.const.u32 	%r2733, [matrix+1020];
	// begin inline asm
	dp4a.u32.u32 %r2732, %r2733, %r5806, %r2728;
	// end inline asm
	shr.u32 	%r5998, %r2668, 6;
	and.b32  	%r5999, %r5998, 240;
	ld.const.u32 	%r2737, [matrix+1024];
	// begin inline asm
	dp4a.u32.u32 %r2736, %r2737, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2741, [matrix+1028];
	// begin inline asm
	dp4a.u32.u32 %r2740, %r2741, %r5750, %r2736;
	// end inline asm
	ld.const.u32 	%r2745, [matrix+1032];
	// begin inline asm
	dp4a.u32.u32 %r2744, %r2745, %r5754, %r2740;
	// end inline asm
	ld.const.u32 	%r2749, [matrix+1036];
	// begin inline asm
	dp4a.u32.u32 %r2748, %r2749, %r5758, %r2744;
	// end inline asm
	ld.const.u32 	%r2753, [matrix+1040];
	// begin inline asm
	dp4a.u32.u32 %r2752, %r2753, %r5762, %r2748;
	// end inline asm
	ld.const.u32 	%r2757, [matrix+1044];
	// begin inline asm
	dp4a.u32.u32 %r2756, %r2757, %r5766, %r2752;
	// end inline asm
	ld.const.u32 	%r2761, [matrix+1048];
	// begin inline asm
	dp4a.u32.u32 %r2760, %r2761, %r5770, %r2756;
	// end inline asm
	ld.const.u32 	%r2765, [matrix+1052];
	// begin inline asm
	dp4a.u32.u32 %r2764, %r2765, %r5774, %r2760;
	// end inline asm
	ld.const.u32 	%r2769, [matrix+1056];
	// begin inline asm
	dp4a.u32.u32 %r2768, %r2769, %r5778, %r2764;
	// end inline asm
	ld.const.u32 	%r2773, [matrix+1060];
	// begin inline asm
	dp4a.u32.u32 %r2772, %r2773, %r5782, %r2768;
	// end inline asm
	ld.const.u32 	%r2777, [matrix+1064];
	// begin inline asm
	dp4a.u32.u32 %r2776, %r2777, %r5786, %r2772;
	// end inline asm
	ld.const.u32 	%r2781, [matrix+1068];
	// begin inline asm
	dp4a.u32.u32 %r2780, %r2781, %r5790, %r2776;
	// end inline asm
	ld.const.u32 	%r2785, [matrix+1072];
	// begin inline asm
	dp4a.u32.u32 %r2784, %r2785, %r5794, %r2780;
	// end inline asm
	ld.const.u32 	%r2789, [matrix+1076];
	// begin inline asm
	dp4a.u32.u32 %r2788, %r2789, %r5798, %r2784;
	// end inline asm
	ld.const.u32 	%r2793, [matrix+1080];
	// begin inline asm
	dp4a.u32.u32 %r2792, %r2793, %r5802, %r2788;
	// end inline asm
	ld.const.u32 	%r2797, [matrix+1084];
	// begin inline asm
	dp4a.u32.u32 %r2796, %r2797, %r5806, %r2792;
	// end inline asm
	ld.const.u32 	%r2801, [matrix+1088];
	// begin inline asm
	dp4a.u32.u32 %r2800, %r2801, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2805, [matrix+1092];
	// begin inline asm
	dp4a.u32.u32 %r2804, %r2805, %r5750, %r2800;
	// end inline asm
	ld.const.u32 	%r2809, [matrix+1096];
	// begin inline asm
	dp4a.u32.u32 %r2808, %r2809, %r5754, %r2804;
	// end inline asm
	ld.const.u32 	%r2813, [matrix+1100];
	// begin inline asm
	dp4a.u32.u32 %r2812, %r2813, %r5758, %r2808;
	// end inline asm
	ld.const.u32 	%r2817, [matrix+1104];
	// begin inline asm
	dp4a.u32.u32 %r2816, %r2817, %r5762, %r2812;
	// end inline asm
	ld.const.u32 	%r2821, [matrix+1108];
	// begin inline asm
	dp4a.u32.u32 %r2820, %r2821, %r5766, %r2816;
	// end inline asm
	ld.const.u32 	%r2825, [matrix+1112];
	// begin inline asm
	dp4a.u32.u32 %r2824, %r2825, %r5770, %r2820;
	// end inline asm
	ld.const.u32 	%r2829, [matrix+1116];
	// begin inline asm
	dp4a.u32.u32 %r2828, %r2829, %r5774, %r2824;
	// end inline asm
	ld.const.u32 	%r2833, [matrix+1120];
	// begin inline asm
	dp4a.u32.u32 %r2832, %r2833, %r5778, %r2828;
	// end inline asm
	ld.const.u32 	%r2837, [matrix+1124];
	// begin inline asm
	dp4a.u32.u32 %r2836, %r2837, %r5782, %r2832;
	// end inline asm
	ld.const.u32 	%r2841, [matrix+1128];
	// begin inline asm
	dp4a.u32.u32 %r2840, %r2841, %r5786, %r2836;
	// end inline asm
	ld.const.u32 	%r2845, [matrix+1132];
	// begin inline asm
	dp4a.u32.u32 %r2844, %r2845, %r5790, %r2840;
	// end inline asm
	ld.const.u32 	%r2849, [matrix+1136];
	// begin inline asm
	dp4a.u32.u32 %r2848, %r2849, %r5794, %r2844;
	// end inline asm
	ld.const.u32 	%r2853, [matrix+1140];
	// begin inline asm
	dp4a.u32.u32 %r2852, %r2853, %r5798, %r2848;
	// end inline asm
	ld.const.u32 	%r2857, [matrix+1144];
	// begin inline asm
	dp4a.u32.u32 %r2856, %r2857, %r5802, %r2852;
	// end inline asm
	ld.const.u32 	%r2861, [matrix+1148];
	// begin inline asm
	dp4a.u32.u32 %r2860, %r2861, %r5806, %r2856;
	// end inline asm
	shr.u32 	%r6000, %r2796, 6;
	and.b32  	%r6001, %r6000, 240;
	shr.u32 	%r6002, %r2860, 10;
	or.b32  	%r6003, %r6002, %r6001;
	xor.b32  	%r6004, %r11, %r6003;
	ld.const.u32 	%r2865, [matrix+1152];
	// begin inline asm
	dp4a.u32.u32 %r2864, %r2865, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2869, [matrix+1156];
	// begin inline asm
	dp4a.u32.u32 %r2868, %r2869, %r5750, %r2864;
	// end inline asm
	ld.const.u32 	%r2873, [matrix+1160];
	// begin inline asm
	dp4a.u32.u32 %r2872, %r2873, %r5754, %r2868;
	// end inline asm
	ld.const.u32 	%r2877, [matrix+1164];
	// begin inline asm
	dp4a.u32.u32 %r2876, %r2877, %r5758, %r2872;
	// end inline asm
	ld.const.u32 	%r2881, [matrix+1168];
	// begin inline asm
	dp4a.u32.u32 %r2880, %r2881, %r5762, %r2876;
	// end inline asm
	ld.const.u32 	%r2885, [matrix+1172];
	// begin inline asm
	dp4a.u32.u32 %r2884, %r2885, %r5766, %r2880;
	// end inline asm
	ld.const.u32 	%r2889, [matrix+1176];
	// begin inline asm
	dp4a.u32.u32 %r2888, %r2889, %r5770, %r2884;
	// end inline asm
	ld.const.u32 	%r2893, [matrix+1180];
	// begin inline asm
	dp4a.u32.u32 %r2892, %r2893, %r5774, %r2888;
	// end inline asm
	ld.const.u32 	%r2897, [matrix+1184];
	// begin inline asm
	dp4a.u32.u32 %r2896, %r2897, %r5778, %r2892;
	// end inline asm
	ld.const.u32 	%r2901, [matrix+1188];
	// begin inline asm
	dp4a.u32.u32 %r2900, %r2901, %r5782, %r2896;
	// end inline asm
	ld.const.u32 	%r2905, [matrix+1192];
	// begin inline asm
	dp4a.u32.u32 %r2904, %r2905, %r5786, %r2900;
	// end inline asm
	ld.const.u32 	%r2909, [matrix+1196];
	// begin inline asm
	dp4a.u32.u32 %r2908, %r2909, %r5790, %r2904;
	// end inline asm
	ld.const.u32 	%r2913, [matrix+1200];
	// begin inline asm
	dp4a.u32.u32 %r2912, %r2913, %r5794, %r2908;
	// end inline asm
	ld.const.u32 	%r2917, [matrix+1204];
	// begin inline asm
	dp4a.u32.u32 %r2916, %r2917, %r5798, %r2912;
	// end inline asm
	ld.const.u32 	%r2921, [matrix+1208];
	// begin inline asm
	dp4a.u32.u32 %r2920, %r2921, %r5802, %r2916;
	// end inline asm
	ld.const.u32 	%r2925, [matrix+1212];
	// begin inline asm
	dp4a.u32.u32 %r2924, %r2925, %r5806, %r2920;
	// end inline asm
	ld.const.u32 	%r2929, [matrix+1216];
	// begin inline asm
	dp4a.u32.u32 %r2928, %r2929, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2933, [matrix+1220];
	// begin inline asm
	dp4a.u32.u32 %r2932, %r2933, %r5750, %r2928;
	// end inline asm
	ld.const.u32 	%r2937, [matrix+1224];
	// begin inline asm
	dp4a.u32.u32 %r2936, %r2937, %r5754, %r2932;
	// end inline asm
	ld.const.u32 	%r2941, [matrix+1228];
	// begin inline asm
	dp4a.u32.u32 %r2940, %r2941, %r5758, %r2936;
	// end inline asm
	ld.const.u32 	%r2945, [matrix+1232];
	// begin inline asm
	dp4a.u32.u32 %r2944, %r2945, %r5762, %r2940;
	// end inline asm
	ld.const.u32 	%r2949, [matrix+1236];
	// begin inline asm
	dp4a.u32.u32 %r2948, %r2949, %r5766, %r2944;
	// end inline asm
	ld.const.u32 	%r2953, [matrix+1240];
	// begin inline asm
	dp4a.u32.u32 %r2952, %r2953, %r5770, %r2948;
	// end inline asm
	ld.const.u32 	%r2957, [matrix+1244];
	// begin inline asm
	dp4a.u32.u32 %r2956, %r2957, %r5774, %r2952;
	// end inline asm
	ld.const.u32 	%r2961, [matrix+1248];
	// begin inline asm
	dp4a.u32.u32 %r2960, %r2961, %r5778, %r2956;
	// end inline asm
	ld.const.u32 	%r2965, [matrix+1252];
	// begin inline asm
	dp4a.u32.u32 %r2964, %r2965, %r5782, %r2960;
	// end inline asm
	ld.const.u32 	%r2969, [matrix+1256];
	// begin inline asm
	dp4a.u32.u32 %r2968, %r2969, %r5786, %r2964;
	// end inline asm
	ld.const.u32 	%r2973, [matrix+1260];
	// begin inline asm
	dp4a.u32.u32 %r2972, %r2973, %r5790, %r2968;
	// end inline asm
	ld.const.u32 	%r2977, [matrix+1264];
	// begin inline asm
	dp4a.u32.u32 %r2976, %r2977, %r5794, %r2972;
	// end inline asm
	ld.const.u32 	%r2981, [matrix+1268];
	// begin inline asm
	dp4a.u32.u32 %r2980, %r2981, %r5798, %r2976;
	// end inline asm
	ld.const.u32 	%r2985, [matrix+1272];
	// begin inline asm
	dp4a.u32.u32 %r2984, %r2985, %r5802, %r2980;
	// end inline asm
	ld.const.u32 	%r2989, [matrix+1276];
	// begin inline asm
	dp4a.u32.u32 %r2988, %r2989, %r5806, %r2984;
	// end inline asm
	shr.u32 	%r6005, %r2924, 6;
	and.b32  	%r6006, %r6005, 240;
	shr.u32 	%r6007, %r2988, 10;
	or.b32  	%r6008, %r6007, %r6006;
	xor.b32  	%r6009, %r5848, %r6008;
	ld.const.u32 	%r2993, [matrix+1280];
	// begin inline asm
	dp4a.u32.u32 %r2992, %r2993, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r2997, [matrix+1284];
	// begin inline asm
	dp4a.u32.u32 %r2996, %r2997, %r5750, %r2992;
	// end inline asm
	ld.const.u32 	%r3001, [matrix+1288];
	// begin inline asm
	dp4a.u32.u32 %r3000, %r3001, %r5754, %r2996;
	// end inline asm
	ld.const.u32 	%r3005, [matrix+1292];
	// begin inline asm
	dp4a.u32.u32 %r3004, %r3005, %r5758, %r3000;
	// end inline asm
	ld.const.u32 	%r3009, [matrix+1296];
	// begin inline asm
	dp4a.u32.u32 %r3008, %r3009, %r5762, %r3004;
	// end inline asm
	ld.const.u32 	%r3013, [matrix+1300];
	// begin inline asm
	dp4a.u32.u32 %r3012, %r3013, %r5766, %r3008;
	// end inline asm
	ld.const.u32 	%r3017, [matrix+1304];
	// begin inline asm
	dp4a.u32.u32 %r3016, %r3017, %r5770, %r3012;
	// end inline asm
	ld.const.u32 	%r3021, [matrix+1308];
	// begin inline asm
	dp4a.u32.u32 %r3020, %r3021, %r5774, %r3016;
	// end inline asm
	ld.const.u32 	%r3025, [matrix+1312];
	// begin inline asm
	dp4a.u32.u32 %r3024, %r3025, %r5778, %r3020;
	// end inline asm
	ld.const.u32 	%r3029, [matrix+1316];
	// begin inline asm
	dp4a.u32.u32 %r3028, %r3029, %r5782, %r3024;
	// end inline asm
	ld.const.u32 	%r3033, [matrix+1320];
	// begin inline asm
	dp4a.u32.u32 %r3032, %r3033, %r5786, %r3028;
	// end inline asm
	ld.const.u32 	%r3037, [matrix+1324];
	// begin inline asm
	dp4a.u32.u32 %r3036, %r3037, %r5790, %r3032;
	// end inline asm
	ld.const.u32 	%r3041, [matrix+1328];
	// begin inline asm
	dp4a.u32.u32 %r3040, %r3041, %r5794, %r3036;
	// end inline asm
	ld.const.u32 	%r3045, [matrix+1332];
	// begin inline asm
	dp4a.u32.u32 %r3044, %r3045, %r5798, %r3040;
	// end inline asm
	ld.const.u32 	%r3049, [matrix+1336];
	// begin inline asm
	dp4a.u32.u32 %r3048, %r3049, %r5802, %r3044;
	// end inline asm
	ld.const.u32 	%r3053, [matrix+1340];
	// begin inline asm
	dp4a.u32.u32 %r3052, %r3053, %r5806, %r3048;
	// end inline asm
	ld.const.u32 	%r3057, [matrix+1344];
	// begin inline asm
	dp4a.u32.u32 %r3056, %r3057, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3061, [matrix+1348];
	// begin inline asm
	dp4a.u32.u32 %r3060, %r3061, %r5750, %r3056;
	// end inline asm
	ld.const.u32 	%r3065, [matrix+1352];
	// begin inline asm
	dp4a.u32.u32 %r3064, %r3065, %r5754, %r3060;
	// end inline asm
	ld.const.u32 	%r3069, [matrix+1356];
	// begin inline asm
	dp4a.u32.u32 %r3068, %r3069, %r5758, %r3064;
	// end inline asm
	ld.const.u32 	%r3073, [matrix+1360];
	// begin inline asm
	dp4a.u32.u32 %r3072, %r3073, %r5762, %r3068;
	// end inline asm
	ld.const.u32 	%r3077, [matrix+1364];
	// begin inline asm
	dp4a.u32.u32 %r3076, %r3077, %r5766, %r3072;
	// end inline asm
	ld.const.u32 	%r3081, [matrix+1368];
	// begin inline asm
	dp4a.u32.u32 %r3080, %r3081, %r5770, %r3076;
	// end inline asm
	ld.const.u32 	%r3085, [matrix+1372];
	// begin inline asm
	dp4a.u32.u32 %r3084, %r3085, %r5774, %r3080;
	// end inline asm
	ld.const.u32 	%r3089, [matrix+1376];
	// begin inline asm
	dp4a.u32.u32 %r3088, %r3089, %r5778, %r3084;
	// end inline asm
	ld.const.u32 	%r3093, [matrix+1380];
	// begin inline asm
	dp4a.u32.u32 %r3092, %r3093, %r5782, %r3088;
	// end inline asm
	ld.const.u32 	%r3097, [matrix+1384];
	// begin inline asm
	dp4a.u32.u32 %r3096, %r3097, %r5786, %r3092;
	// end inline asm
	ld.const.u32 	%r3101, [matrix+1388];
	// begin inline asm
	dp4a.u32.u32 %r3100, %r3101, %r5790, %r3096;
	// end inline asm
	ld.const.u32 	%r3105, [matrix+1392];
	// begin inline asm
	dp4a.u32.u32 %r3104, %r3105, %r5794, %r3100;
	// end inline asm
	ld.const.u32 	%r3109, [matrix+1396];
	// begin inline asm
	dp4a.u32.u32 %r3108, %r3109, %r5798, %r3104;
	// end inline asm
	ld.const.u32 	%r3113, [matrix+1400];
	// begin inline asm
	dp4a.u32.u32 %r3112, %r3113, %r5802, %r3108;
	// end inline asm
	ld.const.u32 	%r3117, [matrix+1404];
	// begin inline asm
	dp4a.u32.u32 %r3116, %r3117, %r5806, %r3112;
	// end inline asm
	shr.u32 	%r6010, %r3052, 6;
	and.b32  	%r6011, %r6010, 240;
	shr.u32 	%r6012, %r3116, 10;
	or.b32  	%r6013, %r6012, %r6011;
	xor.b32  	%r6014, %r5860, %r6013;
	ld.const.u32 	%r3121, [matrix+1408];
	// begin inline asm
	dp4a.u32.u32 %r3120, %r3121, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3125, [matrix+1412];
	// begin inline asm
	dp4a.u32.u32 %r3124, %r3125, %r5750, %r3120;
	// end inline asm
	ld.const.u32 	%r3129, [matrix+1416];
	// begin inline asm
	dp4a.u32.u32 %r3128, %r3129, %r5754, %r3124;
	// end inline asm
	ld.const.u32 	%r3133, [matrix+1420];
	// begin inline asm
	dp4a.u32.u32 %r3132, %r3133, %r5758, %r3128;
	// end inline asm
	ld.const.u32 	%r3137, [matrix+1424];
	// begin inline asm
	dp4a.u32.u32 %r3136, %r3137, %r5762, %r3132;
	// end inline asm
	ld.const.u32 	%r3141, [matrix+1428];
	// begin inline asm
	dp4a.u32.u32 %r3140, %r3141, %r5766, %r3136;
	// end inline asm
	ld.const.u32 	%r3145, [matrix+1432];
	// begin inline asm
	dp4a.u32.u32 %r3144, %r3145, %r5770, %r3140;
	// end inline asm
	ld.const.u32 	%r3149, [matrix+1436];
	// begin inline asm
	dp4a.u32.u32 %r3148, %r3149, %r5774, %r3144;
	// end inline asm
	ld.const.u32 	%r3153, [matrix+1440];
	// begin inline asm
	dp4a.u32.u32 %r3152, %r3153, %r5778, %r3148;
	// end inline asm
	ld.const.u32 	%r3157, [matrix+1444];
	// begin inline asm
	dp4a.u32.u32 %r3156, %r3157, %r5782, %r3152;
	// end inline asm
	ld.const.u32 	%r3161, [matrix+1448];
	// begin inline asm
	dp4a.u32.u32 %r3160, %r3161, %r5786, %r3156;
	// end inline asm
	ld.const.u32 	%r3165, [matrix+1452];
	// begin inline asm
	dp4a.u32.u32 %r3164, %r3165, %r5790, %r3160;
	// end inline asm
	ld.const.u32 	%r3169, [matrix+1456];
	// begin inline asm
	dp4a.u32.u32 %r3168, %r3169, %r5794, %r3164;
	// end inline asm
	ld.const.u32 	%r3173, [matrix+1460];
	// begin inline asm
	dp4a.u32.u32 %r3172, %r3173, %r5798, %r3168;
	// end inline asm
	ld.const.u32 	%r3177, [matrix+1464];
	// begin inline asm
	dp4a.u32.u32 %r3176, %r3177, %r5802, %r3172;
	// end inline asm
	ld.const.u32 	%r3181, [matrix+1468];
	// begin inline asm
	dp4a.u32.u32 %r3180, %r3181, %r5806, %r3176;
	// end inline asm
	ld.const.u32 	%r3185, [matrix+1472];
	// begin inline asm
	dp4a.u32.u32 %r3184, %r3185, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3189, [matrix+1476];
	// begin inline asm
	dp4a.u32.u32 %r3188, %r3189, %r5750, %r3184;
	// end inline asm
	ld.const.u32 	%r3193, [matrix+1480];
	// begin inline asm
	dp4a.u32.u32 %r3192, %r3193, %r5754, %r3188;
	// end inline asm
	ld.const.u32 	%r3197, [matrix+1484];
	// begin inline asm
	dp4a.u32.u32 %r3196, %r3197, %r5758, %r3192;
	// end inline asm
	ld.const.u32 	%r3201, [matrix+1488];
	// begin inline asm
	dp4a.u32.u32 %r3200, %r3201, %r5762, %r3196;
	// end inline asm
	ld.const.u32 	%r3205, [matrix+1492];
	// begin inline asm
	dp4a.u32.u32 %r3204, %r3205, %r5766, %r3200;
	// end inline asm
	ld.const.u32 	%r3209, [matrix+1496];
	// begin inline asm
	dp4a.u32.u32 %r3208, %r3209, %r5770, %r3204;
	// end inline asm
	ld.const.u32 	%r3213, [matrix+1500];
	// begin inline asm
	dp4a.u32.u32 %r3212, %r3213, %r5774, %r3208;
	// end inline asm
	ld.const.u32 	%r3217, [matrix+1504];
	// begin inline asm
	dp4a.u32.u32 %r3216, %r3217, %r5778, %r3212;
	// end inline asm
	ld.const.u32 	%r3221, [matrix+1508];
	// begin inline asm
	dp4a.u32.u32 %r3220, %r3221, %r5782, %r3216;
	// end inline asm
	ld.const.u32 	%r3225, [matrix+1512];
	// begin inline asm
	dp4a.u32.u32 %r3224, %r3225, %r5786, %r3220;
	// end inline asm
	ld.const.u32 	%r3229, [matrix+1516];
	// begin inline asm
	dp4a.u32.u32 %r3228, %r3229, %r5790, %r3224;
	// end inline asm
	ld.const.u32 	%r3233, [matrix+1520];
	// begin inline asm
	dp4a.u32.u32 %r3232, %r3233, %r5794, %r3228;
	// end inline asm
	ld.const.u32 	%r3237, [matrix+1524];
	// begin inline asm
	dp4a.u32.u32 %r3236, %r3237, %r5798, %r3232;
	// end inline asm
	ld.const.u32 	%r3241, [matrix+1528];
	// begin inline asm
	dp4a.u32.u32 %r3240, %r3241, %r5802, %r3236;
	// end inline asm
	ld.const.u32 	%r3245, [matrix+1532];
	// begin inline asm
	dp4a.u32.u32 %r3244, %r3245, %r5806, %r3240;
	// end inline asm
	shr.u32 	%r6015, %r3180, 6;
	and.b32  	%r6016, %r6015, 240;
	shr.u32 	%r6017, %r3244, 10;
	or.b32  	%r6018, %r6017, %r6016;
	xor.b32  	%r6019, %r5862, %r6018;
	ld.const.u32 	%r3249, [matrix+1536];
	// begin inline asm
	dp4a.u32.u32 %r3248, %r3249, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3253, [matrix+1540];
	// begin inline asm
	dp4a.u32.u32 %r3252, %r3253, %r5750, %r3248;
	// end inline asm
	ld.const.u32 	%r3257, [matrix+1544];
	// begin inline asm
	dp4a.u32.u32 %r3256, %r3257, %r5754, %r3252;
	// end inline asm
	ld.const.u32 	%r3261, [matrix+1548];
	// begin inline asm
	dp4a.u32.u32 %r3260, %r3261, %r5758, %r3256;
	// end inline asm
	ld.const.u32 	%r3265, [matrix+1552];
	// begin inline asm
	dp4a.u32.u32 %r3264, %r3265, %r5762, %r3260;
	// end inline asm
	ld.const.u32 	%r3269, [matrix+1556];
	// begin inline asm
	dp4a.u32.u32 %r3268, %r3269, %r5766, %r3264;
	// end inline asm
	ld.const.u32 	%r3273, [matrix+1560];
	// begin inline asm
	dp4a.u32.u32 %r3272, %r3273, %r5770, %r3268;
	// end inline asm
	ld.const.u32 	%r3277, [matrix+1564];
	// begin inline asm
	dp4a.u32.u32 %r3276, %r3277, %r5774, %r3272;
	// end inline asm
	ld.const.u32 	%r3281, [matrix+1568];
	// begin inline asm
	dp4a.u32.u32 %r3280, %r3281, %r5778, %r3276;
	// end inline asm
	ld.const.u32 	%r3285, [matrix+1572];
	// begin inline asm
	dp4a.u32.u32 %r3284, %r3285, %r5782, %r3280;
	// end inline asm
	ld.const.u32 	%r3289, [matrix+1576];
	// begin inline asm
	dp4a.u32.u32 %r3288, %r3289, %r5786, %r3284;
	// end inline asm
	ld.const.u32 	%r3293, [matrix+1580];
	// begin inline asm
	dp4a.u32.u32 %r3292, %r3293, %r5790, %r3288;
	// end inline asm
	ld.const.u32 	%r3297, [matrix+1584];
	// begin inline asm
	dp4a.u32.u32 %r3296, %r3297, %r5794, %r3292;
	// end inline asm
	ld.const.u32 	%r3301, [matrix+1588];
	// begin inline asm
	dp4a.u32.u32 %r3300, %r3301, %r5798, %r3296;
	// end inline asm
	ld.const.u32 	%r3305, [matrix+1592];
	// begin inline asm
	dp4a.u32.u32 %r3304, %r3305, %r5802, %r3300;
	// end inline asm
	ld.const.u32 	%r3309, [matrix+1596];
	// begin inline asm
	dp4a.u32.u32 %r3308, %r3309, %r5806, %r3304;
	// end inline asm
	ld.const.u32 	%r3313, [matrix+1600];
	// begin inline asm
	dp4a.u32.u32 %r3312, %r3313, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3317, [matrix+1604];
	// begin inline asm
	dp4a.u32.u32 %r3316, %r3317, %r5750, %r3312;
	// end inline asm
	ld.const.u32 	%r3321, [matrix+1608];
	// begin inline asm
	dp4a.u32.u32 %r3320, %r3321, %r5754, %r3316;
	// end inline asm
	ld.const.u32 	%r3325, [matrix+1612];
	// begin inline asm
	dp4a.u32.u32 %r3324, %r3325, %r5758, %r3320;
	// end inline asm
	ld.const.u32 	%r3329, [matrix+1616];
	// begin inline asm
	dp4a.u32.u32 %r3328, %r3329, %r5762, %r3324;
	// end inline asm
	ld.const.u32 	%r3333, [matrix+1620];
	// begin inline asm
	dp4a.u32.u32 %r3332, %r3333, %r5766, %r3328;
	// end inline asm
	ld.const.u32 	%r3337, [matrix+1624];
	// begin inline asm
	dp4a.u32.u32 %r3336, %r3337, %r5770, %r3332;
	// end inline asm
	ld.const.u32 	%r3341, [matrix+1628];
	// begin inline asm
	dp4a.u32.u32 %r3340, %r3341, %r5774, %r3336;
	// end inline asm
	ld.const.u32 	%r3345, [matrix+1632];
	// begin inline asm
	dp4a.u32.u32 %r3344, %r3345, %r5778, %r3340;
	// end inline asm
	ld.const.u32 	%r3349, [matrix+1636];
	// begin inline asm
	dp4a.u32.u32 %r3348, %r3349, %r5782, %r3344;
	// end inline asm
	ld.const.u32 	%r3353, [matrix+1640];
	// begin inline asm
	dp4a.u32.u32 %r3352, %r3353, %r5786, %r3348;
	// end inline asm
	ld.const.u32 	%r3357, [matrix+1644];
	// begin inline asm
	dp4a.u32.u32 %r3356, %r3357, %r5790, %r3352;
	// end inline asm
	ld.const.u32 	%r3361, [matrix+1648];
	// begin inline asm
	dp4a.u32.u32 %r3360, %r3361, %r5794, %r3356;
	// end inline asm
	ld.const.u32 	%r3365, [matrix+1652];
	// begin inline asm
	dp4a.u32.u32 %r3364, %r3365, %r5798, %r3360;
	// end inline asm
	ld.const.u32 	%r3369, [matrix+1656];
	// begin inline asm
	dp4a.u32.u32 %r3368, %r3369, %r5802, %r3364;
	// end inline asm
	ld.const.u32 	%r3373, [matrix+1660];
	// begin inline asm
	dp4a.u32.u32 %r3372, %r3373, %r5806, %r3368;
	// end inline asm
	shr.u32 	%r6020, %r3308, 6;
	and.b32  	%r6021, %r6020, 240;
	shr.u32 	%r6022, %r3372, 10;
	or.b32  	%r6023, %r6022, %r6021;
	cvt.u64.u32 	%rd217, %r6023;
	xor.b64  	%rd218, %rd13, %rd217;
	ld.const.u32 	%r3377, [matrix+1664];
	// begin inline asm
	dp4a.u32.u32 %r3376, %r3377, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3381, [matrix+1668];
	// begin inline asm
	dp4a.u32.u32 %r3380, %r3381, %r5750, %r3376;
	// end inline asm
	ld.const.u32 	%r3385, [matrix+1672];
	// begin inline asm
	dp4a.u32.u32 %r3384, %r3385, %r5754, %r3380;
	// end inline asm
	ld.const.u32 	%r3389, [matrix+1676];
	// begin inline asm
	dp4a.u32.u32 %r3388, %r3389, %r5758, %r3384;
	// end inline asm
	ld.const.u32 	%r3393, [matrix+1680];
	// begin inline asm
	dp4a.u32.u32 %r3392, %r3393, %r5762, %r3388;
	// end inline asm
	ld.const.u32 	%r3397, [matrix+1684];
	// begin inline asm
	dp4a.u32.u32 %r3396, %r3397, %r5766, %r3392;
	// end inline asm
	ld.const.u32 	%r3401, [matrix+1688];
	// begin inline asm
	dp4a.u32.u32 %r3400, %r3401, %r5770, %r3396;
	// end inline asm
	ld.const.u32 	%r3405, [matrix+1692];
	// begin inline asm
	dp4a.u32.u32 %r3404, %r3405, %r5774, %r3400;
	// end inline asm
	ld.const.u32 	%r3409, [matrix+1696];
	// begin inline asm
	dp4a.u32.u32 %r3408, %r3409, %r5778, %r3404;
	// end inline asm
	ld.const.u32 	%r3413, [matrix+1700];
	// begin inline asm
	dp4a.u32.u32 %r3412, %r3413, %r5782, %r3408;
	// end inline asm
	ld.const.u32 	%r3417, [matrix+1704];
	// begin inline asm
	dp4a.u32.u32 %r3416, %r3417, %r5786, %r3412;
	// end inline asm
	ld.const.u32 	%r3421, [matrix+1708];
	// begin inline asm
	dp4a.u32.u32 %r3420, %r3421, %r5790, %r3416;
	// end inline asm
	ld.const.u32 	%r3425, [matrix+1712];
	// begin inline asm
	dp4a.u32.u32 %r3424, %r3425, %r5794, %r3420;
	// end inline asm
	ld.const.u32 	%r3429, [matrix+1716];
	// begin inline asm
	dp4a.u32.u32 %r3428, %r3429, %r5798, %r3424;
	// end inline asm
	ld.const.u32 	%r3433, [matrix+1720];
	// begin inline asm
	dp4a.u32.u32 %r3432, %r3433, %r5802, %r3428;
	// end inline asm
	ld.const.u32 	%r3437, [matrix+1724];
	// begin inline asm
	dp4a.u32.u32 %r3436, %r3437, %r5806, %r3432;
	// end inline asm
	ld.const.u32 	%r3441, [matrix+1728];
	// begin inline asm
	dp4a.u32.u32 %r3440, %r3441, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3445, [matrix+1732];
	// begin inline asm
	dp4a.u32.u32 %r3444, %r3445, %r5750, %r3440;
	// end inline asm
	ld.const.u32 	%r3449, [matrix+1736];
	// begin inline asm
	dp4a.u32.u32 %r3448, %r3449, %r5754, %r3444;
	// end inline asm
	ld.const.u32 	%r3453, [matrix+1740];
	// begin inline asm
	dp4a.u32.u32 %r3452, %r3453, %r5758, %r3448;
	// end inline asm
	ld.const.u32 	%r3457, [matrix+1744];
	// begin inline asm
	dp4a.u32.u32 %r3456, %r3457, %r5762, %r3452;
	// end inline asm
	ld.const.u32 	%r3461, [matrix+1748];
	// begin inline asm
	dp4a.u32.u32 %r3460, %r3461, %r5766, %r3456;
	// end inline asm
	ld.const.u32 	%r3465, [matrix+1752];
	// begin inline asm
	dp4a.u32.u32 %r3464, %r3465, %r5770, %r3460;
	// end inline asm
	ld.const.u32 	%r3469, [matrix+1756];
	// begin inline asm
	dp4a.u32.u32 %r3468, %r3469, %r5774, %r3464;
	// end inline asm
	ld.const.u32 	%r3473, [matrix+1760];
	// begin inline asm
	dp4a.u32.u32 %r3472, %r3473, %r5778, %r3468;
	// end inline asm
	ld.const.u32 	%r3477, [matrix+1764];
	// begin inline asm
	dp4a.u32.u32 %r3476, %r3477, %r5782, %r3472;
	// end inline asm
	ld.const.u32 	%r3481, [matrix+1768];
	// begin inline asm
	dp4a.u32.u32 %r3480, %r3481, %r5786, %r3476;
	// end inline asm
	ld.const.u32 	%r3485, [matrix+1772];
	// begin inline asm
	dp4a.u32.u32 %r3484, %r3485, %r5790, %r3480;
	// end inline asm
	ld.const.u32 	%r3489, [matrix+1776];
	// begin inline asm
	dp4a.u32.u32 %r3488, %r3489, %r5794, %r3484;
	// end inline asm
	ld.const.u32 	%r3493, [matrix+1780];
	// begin inline asm
	dp4a.u32.u32 %r3492, %r3493, %r5798, %r3488;
	// end inline asm
	ld.const.u32 	%r3497, [matrix+1784];
	// begin inline asm
	dp4a.u32.u32 %r3496, %r3497, %r5802, %r3492;
	// end inline asm
	ld.const.u32 	%r3501, [matrix+1788];
	// begin inline asm
	dp4a.u32.u32 %r3500, %r3501, %r5806, %r3496;
	// end inline asm
	shr.u32 	%r6024, %r3436, 6;
	and.b32  	%r6025, %r6024, 240;
	shr.u32 	%r6026, %r3500, 10;
	or.b32  	%r6027, %r6026, %r6025;
	cvt.u64.u32 	%rd219, %r6027;
	xor.b64  	%rd220, %rd14, %rd219;
	ld.const.u32 	%r3505, [matrix+1792];
	// begin inline asm
	dp4a.u32.u32 %r3504, %r3505, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3509, [matrix+1796];
	// begin inline asm
	dp4a.u32.u32 %r3508, %r3509, %r5750, %r3504;
	// end inline asm
	ld.const.u32 	%r3513, [matrix+1800];
	// begin inline asm
	dp4a.u32.u32 %r3512, %r3513, %r5754, %r3508;
	// end inline asm
	ld.const.u32 	%r3517, [matrix+1804];
	// begin inline asm
	dp4a.u32.u32 %r3516, %r3517, %r5758, %r3512;
	// end inline asm
	ld.const.u32 	%r3521, [matrix+1808];
	// begin inline asm
	dp4a.u32.u32 %r3520, %r3521, %r5762, %r3516;
	// end inline asm
	ld.const.u32 	%r3525, [matrix+1812];
	// begin inline asm
	dp4a.u32.u32 %r3524, %r3525, %r5766, %r3520;
	// end inline asm
	ld.const.u32 	%r3529, [matrix+1816];
	// begin inline asm
	dp4a.u32.u32 %r3528, %r3529, %r5770, %r3524;
	// end inline asm
	ld.const.u32 	%r3533, [matrix+1820];
	// begin inline asm
	dp4a.u32.u32 %r3532, %r3533, %r5774, %r3528;
	// end inline asm
	ld.const.u32 	%r3537, [matrix+1824];
	// begin inline asm
	dp4a.u32.u32 %r3536, %r3537, %r5778, %r3532;
	// end inline asm
	ld.const.u32 	%r3541, [matrix+1828];
	// begin inline asm
	dp4a.u32.u32 %r3540, %r3541, %r5782, %r3536;
	// end inline asm
	ld.const.u32 	%r3545, [matrix+1832];
	// begin inline asm
	dp4a.u32.u32 %r3544, %r3545, %r5786, %r3540;
	// end inline asm
	ld.const.u32 	%r3549, [matrix+1836];
	// begin inline asm
	dp4a.u32.u32 %r3548, %r3549, %r5790, %r3544;
	// end inline asm
	ld.const.u32 	%r3553, [matrix+1840];
	// begin inline asm
	dp4a.u32.u32 %r3552, %r3553, %r5794, %r3548;
	// end inline asm
	ld.const.u32 	%r3557, [matrix+1844];
	// begin inline asm
	dp4a.u32.u32 %r3556, %r3557, %r5798, %r3552;
	// end inline asm
	ld.const.u32 	%r3561, [matrix+1848];
	// begin inline asm
	dp4a.u32.u32 %r3560, %r3561, %r5802, %r3556;
	// end inline asm
	ld.const.u32 	%r3565, [matrix+1852];
	// begin inline asm
	dp4a.u32.u32 %r3564, %r3565, %r5806, %r3560;
	// end inline asm
	ld.const.u32 	%r3569, [matrix+1856];
	// begin inline asm
	dp4a.u32.u32 %r3568, %r3569, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3573, [matrix+1860];
	// begin inline asm
	dp4a.u32.u32 %r3572, %r3573, %r5750, %r3568;
	// end inline asm
	ld.const.u32 	%r3577, [matrix+1864];
	// begin inline asm
	dp4a.u32.u32 %r3576, %r3577, %r5754, %r3572;
	// end inline asm
	ld.const.u32 	%r3581, [matrix+1868];
	// begin inline asm
	dp4a.u32.u32 %r3580, %r3581, %r5758, %r3576;
	// end inline asm
	ld.const.u32 	%r3585, [matrix+1872];
	// begin inline asm
	dp4a.u32.u32 %r3584, %r3585, %r5762, %r3580;
	// end inline asm
	ld.const.u32 	%r3589, [matrix+1876];
	// begin inline asm
	dp4a.u32.u32 %r3588, %r3589, %r5766, %r3584;
	// end inline asm
	ld.const.u32 	%r3593, [matrix+1880];
	// begin inline asm
	dp4a.u32.u32 %r3592, %r3593, %r5770, %r3588;
	// end inline asm
	ld.const.u32 	%r3597, [matrix+1884];
	// begin inline asm
	dp4a.u32.u32 %r3596, %r3597, %r5774, %r3592;
	// end inline asm
	ld.const.u32 	%r3601, [matrix+1888];
	// begin inline asm
	dp4a.u32.u32 %r3600, %r3601, %r5778, %r3596;
	// end inline asm
	ld.const.u32 	%r3605, [matrix+1892];
	// begin inline asm
	dp4a.u32.u32 %r3604, %r3605, %r5782, %r3600;
	// end inline asm
	ld.const.u32 	%r3609, [matrix+1896];
	// begin inline asm
	dp4a.u32.u32 %r3608, %r3609, %r5786, %r3604;
	// end inline asm
	ld.const.u32 	%r3613, [matrix+1900];
	// begin inline asm
	dp4a.u32.u32 %r3612, %r3613, %r5790, %r3608;
	// end inline asm
	ld.const.u32 	%r3617, [matrix+1904];
	// begin inline asm
	dp4a.u32.u32 %r3616, %r3617, %r5794, %r3612;
	// end inline asm
	ld.const.u32 	%r3621, [matrix+1908];
	// begin inline asm
	dp4a.u32.u32 %r3620, %r3621, %r5798, %r3616;
	// end inline asm
	ld.const.u32 	%r3625, [matrix+1912];
	// begin inline asm
	dp4a.u32.u32 %r3624, %r3625, %r5802, %r3620;
	// end inline asm
	ld.const.u32 	%r3629, [matrix+1916];
	// begin inline asm
	dp4a.u32.u32 %r3628, %r3629, %r5806, %r3624;
	// end inline asm
	shr.u32 	%r6028, %r3564, 6;
	and.b32  	%r6029, %r6028, 240;
	shr.u32 	%r6030, %r3628, 10;
	or.b32  	%r6031, %r6030, %r6029;
	cvt.u64.u32 	%rd221, %r6031;
	xor.b64  	%rd222, %rd15, %rd221;
	ld.const.u32 	%r3633, [matrix+1920];
	// begin inline asm
	dp4a.u32.u32 %r3632, %r3633, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3637, [matrix+1924];
	// begin inline asm
	dp4a.u32.u32 %r3636, %r3637, %r5750, %r3632;
	// end inline asm
	ld.const.u32 	%r3641, [matrix+1928];
	// begin inline asm
	dp4a.u32.u32 %r3640, %r3641, %r5754, %r3636;
	// end inline asm
	ld.const.u32 	%r3645, [matrix+1932];
	// begin inline asm
	dp4a.u32.u32 %r3644, %r3645, %r5758, %r3640;
	// end inline asm
	ld.const.u32 	%r3649, [matrix+1936];
	// begin inline asm
	dp4a.u32.u32 %r3648, %r3649, %r5762, %r3644;
	// end inline asm
	ld.const.u32 	%r3653, [matrix+1940];
	// begin inline asm
	dp4a.u32.u32 %r3652, %r3653, %r5766, %r3648;
	// end inline asm
	ld.const.u32 	%r3657, [matrix+1944];
	// begin inline asm
	dp4a.u32.u32 %r3656, %r3657, %r5770, %r3652;
	// end inline asm
	ld.const.u32 	%r3661, [matrix+1948];
	// begin inline asm
	dp4a.u32.u32 %r3660, %r3661, %r5774, %r3656;
	// end inline asm
	ld.const.u32 	%r3665, [matrix+1952];
	// begin inline asm
	dp4a.u32.u32 %r3664, %r3665, %r5778, %r3660;
	// end inline asm
	ld.const.u32 	%r3669, [matrix+1956];
	// begin inline asm
	dp4a.u32.u32 %r3668, %r3669, %r5782, %r3664;
	// end inline asm
	ld.const.u32 	%r3673, [matrix+1960];
	// begin inline asm
	dp4a.u32.u32 %r3672, %r3673, %r5786, %r3668;
	// end inline asm
	ld.const.u32 	%r3677, [matrix+1964];
	// begin inline asm
	dp4a.u32.u32 %r3676, %r3677, %r5790, %r3672;
	// end inline asm
	ld.const.u32 	%r3681, [matrix+1968];
	// begin inline asm
	dp4a.u32.u32 %r3680, %r3681, %r5794, %r3676;
	// end inline asm
	ld.const.u32 	%r3685, [matrix+1972];
	// begin inline asm
	dp4a.u32.u32 %r3684, %r3685, %r5798, %r3680;
	// end inline asm
	ld.const.u32 	%r3689, [matrix+1976];
	// begin inline asm
	dp4a.u32.u32 %r3688, %r3689, %r5802, %r3684;
	// end inline asm
	ld.const.u32 	%r3693, [matrix+1980];
	// begin inline asm
	dp4a.u32.u32 %r3692, %r3693, %r5806, %r3688;
	// end inline asm
	ld.const.u32 	%r3697, [matrix+1984];
	// begin inline asm
	dp4a.u32.u32 %r3696, %r3697, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3701, [matrix+1988];
	// begin inline asm
	dp4a.u32.u32 %r3700, %r3701, %r5750, %r3696;
	// end inline asm
	ld.const.u32 	%r3705, [matrix+1992];
	// begin inline asm
	dp4a.u32.u32 %r3704, %r3705, %r5754, %r3700;
	// end inline asm
	ld.const.u32 	%r3709, [matrix+1996];
	// begin inline asm
	dp4a.u32.u32 %r3708, %r3709, %r5758, %r3704;
	// end inline asm
	ld.const.u32 	%r3713, [matrix+2000];
	// begin inline asm
	dp4a.u32.u32 %r3712, %r3713, %r5762, %r3708;
	// end inline asm
	ld.const.u32 	%r3717, [matrix+2004];
	// begin inline asm
	dp4a.u32.u32 %r3716, %r3717, %r5766, %r3712;
	// end inline asm
	ld.const.u32 	%r3721, [matrix+2008];
	// begin inline asm
	dp4a.u32.u32 %r3720, %r3721, %r5770, %r3716;
	// end inline asm
	ld.const.u32 	%r3725, [matrix+2012];
	// begin inline asm
	dp4a.u32.u32 %r3724, %r3725, %r5774, %r3720;
	// end inline asm
	ld.const.u32 	%r3729, [matrix+2016];
	// begin inline asm
	dp4a.u32.u32 %r3728, %r3729, %r5778, %r3724;
	// end inline asm
	ld.const.u32 	%r3733, [matrix+2020];
	// begin inline asm
	dp4a.u32.u32 %r3732, %r3733, %r5782, %r3728;
	// end inline asm
	ld.const.u32 	%r3737, [matrix+2024];
	// begin inline asm
	dp4a.u32.u32 %r3736, %r3737, %r5786, %r3732;
	// end inline asm
	ld.const.u32 	%r3741, [matrix+2028];
	// begin inline asm
	dp4a.u32.u32 %r3740, %r3741, %r5790, %r3736;
	// end inline asm
	ld.const.u32 	%r3745, [matrix+2032];
	// begin inline asm
	dp4a.u32.u32 %r3744, %r3745, %r5794, %r3740;
	// end inline asm
	ld.const.u32 	%r3749, [matrix+2036];
	// begin inline asm
	dp4a.u32.u32 %r3748, %r3749, %r5798, %r3744;
	// end inline asm
	ld.const.u32 	%r3753, [matrix+2040];
	// begin inline asm
	dp4a.u32.u32 %r3752, %r3753, %r5802, %r3748;
	// end inline asm
	ld.const.u32 	%r3757, [matrix+2044];
	// begin inline asm
	dp4a.u32.u32 %r3756, %r3757, %r5806, %r3752;
	// end inline asm
	shr.u32 	%r6032, %r3692, 6;
	and.b32  	%r6033, %r6032, 240;
	ld.const.u32 	%r3761, [matrix+2048];
	// begin inline asm
	dp4a.u32.u32 %r3760, %r3761, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3765, [matrix+2052];
	// begin inline asm
	dp4a.u32.u32 %r3764, %r3765, %r5750, %r3760;
	// end inline asm
	ld.const.u32 	%r3769, [matrix+2056];
	// begin inline asm
	dp4a.u32.u32 %r3768, %r3769, %r5754, %r3764;
	// end inline asm
	ld.const.u32 	%r3773, [matrix+2060];
	// begin inline asm
	dp4a.u32.u32 %r3772, %r3773, %r5758, %r3768;
	// end inline asm
	ld.const.u32 	%r3777, [matrix+2064];
	// begin inline asm
	dp4a.u32.u32 %r3776, %r3777, %r5762, %r3772;
	// end inline asm
	ld.const.u32 	%r3781, [matrix+2068];
	// begin inline asm
	dp4a.u32.u32 %r3780, %r3781, %r5766, %r3776;
	// end inline asm
	ld.const.u32 	%r3785, [matrix+2072];
	// begin inline asm
	dp4a.u32.u32 %r3784, %r3785, %r5770, %r3780;
	// end inline asm
	ld.const.u32 	%r3789, [matrix+2076];
	// begin inline asm
	dp4a.u32.u32 %r3788, %r3789, %r5774, %r3784;
	// end inline asm
	ld.const.u32 	%r3793, [matrix+2080];
	// begin inline asm
	dp4a.u32.u32 %r3792, %r3793, %r5778, %r3788;
	// end inline asm
	ld.const.u32 	%r3797, [matrix+2084];
	// begin inline asm
	dp4a.u32.u32 %r3796, %r3797, %r5782, %r3792;
	// end inline asm
	ld.const.u32 	%r3801, [matrix+2088];
	// begin inline asm
	dp4a.u32.u32 %r3800, %r3801, %r5786, %r3796;
	// end inline asm
	ld.const.u32 	%r3805, [matrix+2092];
	// begin inline asm
	dp4a.u32.u32 %r3804, %r3805, %r5790, %r3800;
	// end inline asm
	ld.const.u32 	%r3809, [matrix+2096];
	// begin inline asm
	dp4a.u32.u32 %r3808, %r3809, %r5794, %r3804;
	// end inline asm
	ld.const.u32 	%r3813, [matrix+2100];
	// begin inline asm
	dp4a.u32.u32 %r3812, %r3813, %r5798, %r3808;
	// end inline asm
	ld.const.u32 	%r3817, [matrix+2104];
	// begin inline asm
	dp4a.u32.u32 %r3816, %r3817, %r5802, %r3812;
	// end inline asm
	ld.const.u32 	%r3821, [matrix+2108];
	// begin inline asm
	dp4a.u32.u32 %r3820, %r3821, %r5806, %r3816;
	// end inline asm
	ld.const.u32 	%r3825, [matrix+2112];
	// begin inline asm
	dp4a.u32.u32 %r3824, %r3825, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3829, [matrix+2116];
	// begin inline asm
	dp4a.u32.u32 %r3828, %r3829, %r5750, %r3824;
	// end inline asm
	ld.const.u32 	%r3833, [matrix+2120];
	// begin inline asm
	dp4a.u32.u32 %r3832, %r3833, %r5754, %r3828;
	// end inline asm
	ld.const.u32 	%r3837, [matrix+2124];
	// begin inline asm
	dp4a.u32.u32 %r3836, %r3837, %r5758, %r3832;
	// end inline asm
	ld.const.u32 	%r3841, [matrix+2128];
	// begin inline asm
	dp4a.u32.u32 %r3840, %r3841, %r5762, %r3836;
	// end inline asm
	ld.const.u32 	%r3845, [matrix+2132];
	// begin inline asm
	dp4a.u32.u32 %r3844, %r3845, %r5766, %r3840;
	// end inline asm
	ld.const.u32 	%r3849, [matrix+2136];
	// begin inline asm
	dp4a.u32.u32 %r3848, %r3849, %r5770, %r3844;
	// end inline asm
	ld.const.u32 	%r3853, [matrix+2140];
	// begin inline asm
	dp4a.u32.u32 %r3852, %r3853, %r5774, %r3848;
	// end inline asm
	ld.const.u32 	%r3857, [matrix+2144];
	// begin inline asm
	dp4a.u32.u32 %r3856, %r3857, %r5778, %r3852;
	// end inline asm
	ld.const.u32 	%r3861, [matrix+2148];
	// begin inline asm
	dp4a.u32.u32 %r3860, %r3861, %r5782, %r3856;
	// end inline asm
	ld.const.u32 	%r3865, [matrix+2152];
	// begin inline asm
	dp4a.u32.u32 %r3864, %r3865, %r5786, %r3860;
	// end inline asm
	ld.const.u32 	%r3869, [matrix+2156];
	// begin inline asm
	dp4a.u32.u32 %r3868, %r3869, %r5790, %r3864;
	// end inline asm
	ld.const.u32 	%r3873, [matrix+2160];
	// begin inline asm
	dp4a.u32.u32 %r3872, %r3873, %r5794, %r3868;
	// end inline asm
	ld.const.u32 	%r3877, [matrix+2164];
	// begin inline asm
	dp4a.u32.u32 %r3876, %r3877, %r5798, %r3872;
	// end inline asm
	ld.const.u32 	%r3881, [matrix+2168];
	// begin inline asm
	dp4a.u32.u32 %r3880, %r3881, %r5802, %r3876;
	// end inline asm
	ld.const.u32 	%r3885, [matrix+2172];
	// begin inline asm
	dp4a.u32.u32 %r3884, %r3885, %r5806, %r3880;
	// end inline asm
	shr.u32 	%r6034, %r3820, 6;
	and.b32  	%r6035, %r6034, 240;
	shr.u32 	%r6036, %r3884, 10;
	or.b32  	%r6037, %r6036, %r6035;
	xor.b32  	%r6038, %r13, %r6037;
	ld.const.u32 	%r3889, [matrix+2176];
	// begin inline asm
	dp4a.u32.u32 %r3888, %r3889, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3893, [matrix+2180];
	// begin inline asm
	dp4a.u32.u32 %r3892, %r3893, %r5750, %r3888;
	// end inline asm
	ld.const.u32 	%r3897, [matrix+2184];
	// begin inline asm
	dp4a.u32.u32 %r3896, %r3897, %r5754, %r3892;
	// end inline asm
	ld.const.u32 	%r3901, [matrix+2188];
	// begin inline asm
	dp4a.u32.u32 %r3900, %r3901, %r5758, %r3896;
	// end inline asm
	ld.const.u32 	%r3905, [matrix+2192];
	// begin inline asm
	dp4a.u32.u32 %r3904, %r3905, %r5762, %r3900;
	// end inline asm
	ld.const.u32 	%r3909, [matrix+2196];
	// begin inline asm
	dp4a.u32.u32 %r3908, %r3909, %r5766, %r3904;
	// end inline asm
	ld.const.u32 	%r3913, [matrix+2200];
	// begin inline asm
	dp4a.u32.u32 %r3912, %r3913, %r5770, %r3908;
	// end inline asm
	ld.const.u32 	%r3917, [matrix+2204];
	// begin inline asm
	dp4a.u32.u32 %r3916, %r3917, %r5774, %r3912;
	// end inline asm
	ld.const.u32 	%r3921, [matrix+2208];
	// begin inline asm
	dp4a.u32.u32 %r3920, %r3921, %r5778, %r3916;
	// end inline asm
	ld.const.u32 	%r3925, [matrix+2212];
	// begin inline asm
	dp4a.u32.u32 %r3924, %r3925, %r5782, %r3920;
	// end inline asm
	ld.const.u32 	%r3929, [matrix+2216];
	// begin inline asm
	dp4a.u32.u32 %r3928, %r3929, %r5786, %r3924;
	// end inline asm
	ld.const.u32 	%r3933, [matrix+2220];
	// begin inline asm
	dp4a.u32.u32 %r3932, %r3933, %r5790, %r3928;
	// end inline asm
	ld.const.u32 	%r3937, [matrix+2224];
	// begin inline asm
	dp4a.u32.u32 %r3936, %r3937, %r5794, %r3932;
	// end inline asm
	ld.const.u32 	%r3941, [matrix+2228];
	// begin inline asm
	dp4a.u32.u32 %r3940, %r3941, %r5798, %r3936;
	// end inline asm
	ld.const.u32 	%r3945, [matrix+2232];
	// begin inline asm
	dp4a.u32.u32 %r3944, %r3945, %r5802, %r3940;
	// end inline asm
	ld.const.u32 	%r3949, [matrix+2236];
	// begin inline asm
	dp4a.u32.u32 %r3948, %r3949, %r5806, %r3944;
	// end inline asm
	ld.const.u32 	%r3953, [matrix+2240];
	// begin inline asm
	dp4a.u32.u32 %r3952, %r3953, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r3957, [matrix+2244];
	// begin inline asm
	dp4a.u32.u32 %r3956, %r3957, %r5750, %r3952;
	// end inline asm
	ld.const.u32 	%r3961, [matrix+2248];
	// begin inline asm
	dp4a.u32.u32 %r3960, %r3961, %r5754, %r3956;
	// end inline asm
	ld.const.u32 	%r3965, [matrix+2252];
	// begin inline asm
	dp4a.u32.u32 %r3964, %r3965, %r5758, %r3960;
	// end inline asm
	ld.const.u32 	%r3969, [matrix+2256];
	// begin inline asm
	dp4a.u32.u32 %r3968, %r3969, %r5762, %r3964;
	// end inline asm
	ld.const.u32 	%r3973, [matrix+2260];
	// begin inline asm
	dp4a.u32.u32 %r3972, %r3973, %r5766, %r3968;
	// end inline asm
	ld.const.u32 	%r3977, [matrix+2264];
	// begin inline asm
	dp4a.u32.u32 %r3976, %r3977, %r5770, %r3972;
	// end inline asm
	ld.const.u32 	%r3981, [matrix+2268];
	// begin inline asm
	dp4a.u32.u32 %r3980, %r3981, %r5774, %r3976;
	// end inline asm
	ld.const.u32 	%r3985, [matrix+2272];
	// begin inline asm
	dp4a.u32.u32 %r3984, %r3985, %r5778, %r3980;
	// end inline asm
	ld.const.u32 	%r3989, [matrix+2276];
	// begin inline asm
	dp4a.u32.u32 %r3988, %r3989, %r5782, %r3984;
	// end inline asm
	ld.const.u32 	%r3993, [matrix+2280];
	// begin inline asm
	dp4a.u32.u32 %r3992, %r3993, %r5786, %r3988;
	// end inline asm
	ld.const.u32 	%r3997, [matrix+2284];
	// begin inline asm
	dp4a.u32.u32 %r3996, %r3997, %r5790, %r3992;
	// end inline asm
	ld.const.u32 	%r4001, [matrix+2288];
	// begin inline asm
	dp4a.u32.u32 %r4000, %r4001, %r5794, %r3996;
	// end inline asm
	ld.const.u32 	%r4005, [matrix+2292];
	// begin inline asm
	dp4a.u32.u32 %r4004, %r4005, %r5798, %r4000;
	// end inline asm
	ld.const.u32 	%r4009, [matrix+2296];
	// begin inline asm
	dp4a.u32.u32 %r4008, %r4009, %r5802, %r4004;
	// end inline asm
	ld.const.u32 	%r4013, [matrix+2300];
	// begin inline asm
	dp4a.u32.u32 %r4012, %r4013, %r5806, %r4008;
	// end inline asm
	shr.u32 	%r6039, %r3948, 6;
	and.b32  	%r6040, %r6039, 240;
	shr.u32 	%r6041, %r4012, 10;
	or.b32  	%r6042, %r6041, %r6040;
	xor.b32  	%r6043, %r5886, %r6042;
	ld.const.u32 	%r4017, [matrix+2304];
	// begin inline asm
	dp4a.u32.u32 %r4016, %r4017, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4021, [matrix+2308];
	// begin inline asm
	dp4a.u32.u32 %r4020, %r4021, %r5750, %r4016;
	// end inline asm
	ld.const.u32 	%r4025, [matrix+2312];
	// begin inline asm
	dp4a.u32.u32 %r4024, %r4025, %r5754, %r4020;
	// end inline asm
	ld.const.u32 	%r4029, [matrix+2316];
	// begin inline asm
	dp4a.u32.u32 %r4028, %r4029, %r5758, %r4024;
	// end inline asm
	ld.const.u32 	%r4033, [matrix+2320];
	// begin inline asm
	dp4a.u32.u32 %r4032, %r4033, %r5762, %r4028;
	// end inline asm
	ld.const.u32 	%r4037, [matrix+2324];
	// begin inline asm
	dp4a.u32.u32 %r4036, %r4037, %r5766, %r4032;
	// end inline asm
	ld.const.u32 	%r4041, [matrix+2328];
	// begin inline asm
	dp4a.u32.u32 %r4040, %r4041, %r5770, %r4036;
	// end inline asm
	ld.const.u32 	%r4045, [matrix+2332];
	// begin inline asm
	dp4a.u32.u32 %r4044, %r4045, %r5774, %r4040;
	// end inline asm
	ld.const.u32 	%r4049, [matrix+2336];
	// begin inline asm
	dp4a.u32.u32 %r4048, %r4049, %r5778, %r4044;
	// end inline asm
	ld.const.u32 	%r4053, [matrix+2340];
	// begin inline asm
	dp4a.u32.u32 %r4052, %r4053, %r5782, %r4048;
	// end inline asm
	ld.const.u32 	%r4057, [matrix+2344];
	// begin inline asm
	dp4a.u32.u32 %r4056, %r4057, %r5786, %r4052;
	// end inline asm
	ld.const.u32 	%r4061, [matrix+2348];
	// begin inline asm
	dp4a.u32.u32 %r4060, %r4061, %r5790, %r4056;
	// end inline asm
	ld.const.u32 	%r4065, [matrix+2352];
	// begin inline asm
	dp4a.u32.u32 %r4064, %r4065, %r5794, %r4060;
	// end inline asm
	ld.const.u32 	%r4069, [matrix+2356];
	// begin inline asm
	dp4a.u32.u32 %r4068, %r4069, %r5798, %r4064;
	// end inline asm
	ld.const.u32 	%r4073, [matrix+2360];
	// begin inline asm
	dp4a.u32.u32 %r4072, %r4073, %r5802, %r4068;
	// end inline asm
	ld.const.u32 	%r4077, [matrix+2364];
	// begin inline asm
	dp4a.u32.u32 %r4076, %r4077, %r5806, %r4072;
	// end inline asm
	ld.const.u32 	%r4081, [matrix+2368];
	// begin inline asm
	dp4a.u32.u32 %r4080, %r4081, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4085, [matrix+2372];
	// begin inline asm
	dp4a.u32.u32 %r4084, %r4085, %r5750, %r4080;
	// end inline asm
	ld.const.u32 	%r4089, [matrix+2376];
	// begin inline asm
	dp4a.u32.u32 %r4088, %r4089, %r5754, %r4084;
	// end inline asm
	ld.const.u32 	%r4093, [matrix+2380];
	// begin inline asm
	dp4a.u32.u32 %r4092, %r4093, %r5758, %r4088;
	// end inline asm
	ld.const.u32 	%r4097, [matrix+2384];
	// begin inline asm
	dp4a.u32.u32 %r4096, %r4097, %r5762, %r4092;
	// end inline asm
	ld.const.u32 	%r4101, [matrix+2388];
	// begin inline asm
	dp4a.u32.u32 %r4100, %r4101, %r5766, %r4096;
	// end inline asm
	ld.const.u32 	%r4105, [matrix+2392];
	// begin inline asm
	dp4a.u32.u32 %r4104, %r4105, %r5770, %r4100;
	// end inline asm
	ld.const.u32 	%r4109, [matrix+2396];
	// begin inline asm
	dp4a.u32.u32 %r4108, %r4109, %r5774, %r4104;
	// end inline asm
	ld.const.u32 	%r4113, [matrix+2400];
	// begin inline asm
	dp4a.u32.u32 %r4112, %r4113, %r5778, %r4108;
	// end inline asm
	ld.const.u32 	%r4117, [matrix+2404];
	// begin inline asm
	dp4a.u32.u32 %r4116, %r4117, %r5782, %r4112;
	// end inline asm
	ld.const.u32 	%r4121, [matrix+2408];
	// begin inline asm
	dp4a.u32.u32 %r4120, %r4121, %r5786, %r4116;
	// end inline asm
	ld.const.u32 	%r4125, [matrix+2412];
	// begin inline asm
	dp4a.u32.u32 %r4124, %r4125, %r5790, %r4120;
	// end inline asm
	ld.const.u32 	%r4129, [matrix+2416];
	// begin inline asm
	dp4a.u32.u32 %r4128, %r4129, %r5794, %r4124;
	// end inline asm
	ld.const.u32 	%r4133, [matrix+2420];
	// begin inline asm
	dp4a.u32.u32 %r4132, %r4133, %r5798, %r4128;
	// end inline asm
	ld.const.u32 	%r4137, [matrix+2424];
	// begin inline asm
	dp4a.u32.u32 %r4136, %r4137, %r5802, %r4132;
	// end inline asm
	ld.const.u32 	%r4141, [matrix+2428];
	// begin inline asm
	dp4a.u32.u32 %r4140, %r4141, %r5806, %r4136;
	// end inline asm
	shr.u32 	%r6044, %r4076, 6;
	and.b32  	%r6045, %r6044, 240;
	shr.u32 	%r6046, %r4140, 10;
	or.b32  	%r6047, %r6046, %r6045;
	xor.b32  	%r6048, %r5898, %r6047;
	ld.const.u32 	%r4145, [matrix+2432];
	// begin inline asm
	dp4a.u32.u32 %r4144, %r4145, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4149, [matrix+2436];
	// begin inline asm
	dp4a.u32.u32 %r4148, %r4149, %r5750, %r4144;
	// end inline asm
	ld.const.u32 	%r4153, [matrix+2440];
	// begin inline asm
	dp4a.u32.u32 %r4152, %r4153, %r5754, %r4148;
	// end inline asm
	ld.const.u32 	%r4157, [matrix+2444];
	// begin inline asm
	dp4a.u32.u32 %r4156, %r4157, %r5758, %r4152;
	// end inline asm
	ld.const.u32 	%r4161, [matrix+2448];
	// begin inline asm
	dp4a.u32.u32 %r4160, %r4161, %r5762, %r4156;
	// end inline asm
	ld.const.u32 	%r4165, [matrix+2452];
	// begin inline asm
	dp4a.u32.u32 %r4164, %r4165, %r5766, %r4160;
	// end inline asm
	ld.const.u32 	%r4169, [matrix+2456];
	// begin inline asm
	dp4a.u32.u32 %r4168, %r4169, %r5770, %r4164;
	// end inline asm
	ld.const.u32 	%r4173, [matrix+2460];
	// begin inline asm
	dp4a.u32.u32 %r4172, %r4173, %r5774, %r4168;
	// end inline asm
	ld.const.u32 	%r4177, [matrix+2464];
	// begin inline asm
	dp4a.u32.u32 %r4176, %r4177, %r5778, %r4172;
	// end inline asm
	ld.const.u32 	%r4181, [matrix+2468];
	// begin inline asm
	dp4a.u32.u32 %r4180, %r4181, %r5782, %r4176;
	// end inline asm
	ld.const.u32 	%r4185, [matrix+2472];
	// begin inline asm
	dp4a.u32.u32 %r4184, %r4185, %r5786, %r4180;
	// end inline asm
	ld.const.u32 	%r4189, [matrix+2476];
	// begin inline asm
	dp4a.u32.u32 %r4188, %r4189, %r5790, %r4184;
	// end inline asm
	ld.const.u32 	%r4193, [matrix+2480];
	// begin inline asm
	dp4a.u32.u32 %r4192, %r4193, %r5794, %r4188;
	// end inline asm
	ld.const.u32 	%r4197, [matrix+2484];
	// begin inline asm
	dp4a.u32.u32 %r4196, %r4197, %r5798, %r4192;
	// end inline asm
	ld.const.u32 	%r4201, [matrix+2488];
	// begin inline asm
	dp4a.u32.u32 %r4200, %r4201, %r5802, %r4196;
	// end inline asm
	ld.const.u32 	%r4205, [matrix+2492];
	// begin inline asm
	dp4a.u32.u32 %r4204, %r4205, %r5806, %r4200;
	// end inline asm
	ld.const.u32 	%r4209, [matrix+2496];
	// begin inline asm
	dp4a.u32.u32 %r4208, %r4209, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4213, [matrix+2500];
	// begin inline asm
	dp4a.u32.u32 %r4212, %r4213, %r5750, %r4208;
	// end inline asm
	ld.const.u32 	%r4217, [matrix+2504];
	// begin inline asm
	dp4a.u32.u32 %r4216, %r4217, %r5754, %r4212;
	// end inline asm
	ld.const.u32 	%r4221, [matrix+2508];
	// begin inline asm
	dp4a.u32.u32 %r4220, %r4221, %r5758, %r4216;
	// end inline asm
	ld.const.u32 	%r4225, [matrix+2512];
	// begin inline asm
	dp4a.u32.u32 %r4224, %r4225, %r5762, %r4220;
	// end inline asm
	ld.const.u32 	%r4229, [matrix+2516];
	// begin inline asm
	dp4a.u32.u32 %r4228, %r4229, %r5766, %r4224;
	// end inline asm
	ld.const.u32 	%r4233, [matrix+2520];
	// begin inline asm
	dp4a.u32.u32 %r4232, %r4233, %r5770, %r4228;
	// end inline asm
	ld.const.u32 	%r4237, [matrix+2524];
	// begin inline asm
	dp4a.u32.u32 %r4236, %r4237, %r5774, %r4232;
	// end inline asm
	ld.const.u32 	%r4241, [matrix+2528];
	// begin inline asm
	dp4a.u32.u32 %r4240, %r4241, %r5778, %r4236;
	// end inline asm
	ld.const.u32 	%r4245, [matrix+2532];
	// begin inline asm
	dp4a.u32.u32 %r4244, %r4245, %r5782, %r4240;
	// end inline asm
	ld.const.u32 	%r4249, [matrix+2536];
	// begin inline asm
	dp4a.u32.u32 %r4248, %r4249, %r5786, %r4244;
	// end inline asm
	ld.const.u32 	%r4253, [matrix+2540];
	// begin inline asm
	dp4a.u32.u32 %r4252, %r4253, %r5790, %r4248;
	// end inline asm
	ld.const.u32 	%r4257, [matrix+2544];
	// begin inline asm
	dp4a.u32.u32 %r4256, %r4257, %r5794, %r4252;
	// end inline asm
	ld.const.u32 	%r4261, [matrix+2548];
	// begin inline asm
	dp4a.u32.u32 %r4260, %r4261, %r5798, %r4256;
	// end inline asm
	ld.const.u32 	%r4265, [matrix+2552];
	// begin inline asm
	dp4a.u32.u32 %r4264, %r4265, %r5802, %r4260;
	// end inline asm
	ld.const.u32 	%r4269, [matrix+2556];
	// begin inline asm
	dp4a.u32.u32 %r4268, %r4269, %r5806, %r4264;
	// end inline asm
	shr.u32 	%r6049, %r4204, 6;
	and.b32  	%r6050, %r6049, 240;
	shr.u32 	%r6051, %r4268, 10;
	or.b32  	%r6052, %r6051, %r6050;
	xor.b32  	%r6053, %r5900, %r6052;
	ld.const.u32 	%r4273, [matrix+2560];
	// begin inline asm
	dp4a.u32.u32 %r4272, %r4273, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4277, [matrix+2564];
	// begin inline asm
	dp4a.u32.u32 %r4276, %r4277, %r5750, %r4272;
	// end inline asm
	ld.const.u32 	%r4281, [matrix+2568];
	// begin inline asm
	dp4a.u32.u32 %r4280, %r4281, %r5754, %r4276;
	// end inline asm
	ld.const.u32 	%r4285, [matrix+2572];
	// begin inline asm
	dp4a.u32.u32 %r4284, %r4285, %r5758, %r4280;
	// end inline asm
	ld.const.u32 	%r4289, [matrix+2576];
	// begin inline asm
	dp4a.u32.u32 %r4288, %r4289, %r5762, %r4284;
	// end inline asm
	ld.const.u32 	%r4293, [matrix+2580];
	// begin inline asm
	dp4a.u32.u32 %r4292, %r4293, %r5766, %r4288;
	// end inline asm
	ld.const.u32 	%r4297, [matrix+2584];
	// begin inline asm
	dp4a.u32.u32 %r4296, %r4297, %r5770, %r4292;
	// end inline asm
	ld.const.u32 	%r4301, [matrix+2588];
	// begin inline asm
	dp4a.u32.u32 %r4300, %r4301, %r5774, %r4296;
	// end inline asm
	ld.const.u32 	%r4305, [matrix+2592];
	// begin inline asm
	dp4a.u32.u32 %r4304, %r4305, %r5778, %r4300;
	// end inline asm
	ld.const.u32 	%r4309, [matrix+2596];
	// begin inline asm
	dp4a.u32.u32 %r4308, %r4309, %r5782, %r4304;
	// end inline asm
	ld.const.u32 	%r4313, [matrix+2600];
	// begin inline asm
	dp4a.u32.u32 %r4312, %r4313, %r5786, %r4308;
	// end inline asm
	ld.const.u32 	%r4317, [matrix+2604];
	// begin inline asm
	dp4a.u32.u32 %r4316, %r4317, %r5790, %r4312;
	// end inline asm
	ld.const.u32 	%r4321, [matrix+2608];
	// begin inline asm
	dp4a.u32.u32 %r4320, %r4321, %r5794, %r4316;
	// end inline asm
	ld.const.u32 	%r4325, [matrix+2612];
	// begin inline asm
	dp4a.u32.u32 %r4324, %r4325, %r5798, %r4320;
	// end inline asm
	ld.const.u32 	%r4329, [matrix+2616];
	// begin inline asm
	dp4a.u32.u32 %r4328, %r4329, %r5802, %r4324;
	// end inline asm
	ld.const.u32 	%r4333, [matrix+2620];
	// begin inline asm
	dp4a.u32.u32 %r4332, %r4333, %r5806, %r4328;
	// end inline asm
	ld.const.u32 	%r4337, [matrix+2624];
	// begin inline asm
	dp4a.u32.u32 %r4336, %r4337, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4341, [matrix+2628];
	// begin inline asm
	dp4a.u32.u32 %r4340, %r4341, %r5750, %r4336;
	// end inline asm
	ld.const.u32 	%r4345, [matrix+2632];
	// begin inline asm
	dp4a.u32.u32 %r4344, %r4345, %r5754, %r4340;
	// end inline asm
	ld.const.u32 	%r4349, [matrix+2636];
	// begin inline asm
	dp4a.u32.u32 %r4348, %r4349, %r5758, %r4344;
	// end inline asm
	ld.const.u32 	%r4353, [matrix+2640];
	// begin inline asm
	dp4a.u32.u32 %r4352, %r4353, %r5762, %r4348;
	// end inline asm
	ld.const.u32 	%r4357, [matrix+2644];
	// begin inline asm
	dp4a.u32.u32 %r4356, %r4357, %r5766, %r4352;
	// end inline asm
	ld.const.u32 	%r4361, [matrix+2648];
	// begin inline asm
	dp4a.u32.u32 %r4360, %r4361, %r5770, %r4356;
	// end inline asm
	ld.const.u32 	%r4365, [matrix+2652];
	// begin inline asm
	dp4a.u32.u32 %r4364, %r4365, %r5774, %r4360;
	// end inline asm
	ld.const.u32 	%r4369, [matrix+2656];
	// begin inline asm
	dp4a.u32.u32 %r4368, %r4369, %r5778, %r4364;
	// end inline asm
	ld.const.u32 	%r4373, [matrix+2660];
	// begin inline asm
	dp4a.u32.u32 %r4372, %r4373, %r5782, %r4368;
	// end inline asm
	ld.const.u32 	%r4377, [matrix+2664];
	// begin inline asm
	dp4a.u32.u32 %r4376, %r4377, %r5786, %r4372;
	// end inline asm
	ld.const.u32 	%r4381, [matrix+2668];
	// begin inline asm
	dp4a.u32.u32 %r4380, %r4381, %r5790, %r4376;
	// end inline asm
	ld.const.u32 	%r4385, [matrix+2672];
	// begin inline asm
	dp4a.u32.u32 %r4384, %r4385, %r5794, %r4380;
	// end inline asm
	ld.const.u32 	%r4389, [matrix+2676];
	// begin inline asm
	dp4a.u32.u32 %r4388, %r4389, %r5798, %r4384;
	// end inline asm
	ld.const.u32 	%r4393, [matrix+2680];
	// begin inline asm
	dp4a.u32.u32 %r4392, %r4393, %r5802, %r4388;
	// end inline asm
	ld.const.u32 	%r4397, [matrix+2684];
	// begin inline asm
	dp4a.u32.u32 %r4396, %r4397, %r5806, %r4392;
	// end inline asm
	shr.u32 	%r6054, %r4332, 6;
	and.b32  	%r6055, %r6054, 240;
	shr.u32 	%r6056, %r4396, 10;
	or.b32  	%r6057, %r6056, %r6055;
	cvt.u64.u32 	%rd223, %r6057;
	xor.b64  	%rd224, %rd16, %rd223;
	ld.const.u32 	%r4401, [matrix+2688];
	// begin inline asm
	dp4a.u32.u32 %r4400, %r4401, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4405, [matrix+2692];
	// begin inline asm
	dp4a.u32.u32 %r4404, %r4405, %r5750, %r4400;
	// end inline asm
	ld.const.u32 	%r4409, [matrix+2696];
	// begin inline asm
	dp4a.u32.u32 %r4408, %r4409, %r5754, %r4404;
	// end inline asm
	ld.const.u32 	%r4413, [matrix+2700];
	// begin inline asm
	dp4a.u32.u32 %r4412, %r4413, %r5758, %r4408;
	// end inline asm
	ld.const.u32 	%r4417, [matrix+2704];
	// begin inline asm
	dp4a.u32.u32 %r4416, %r4417, %r5762, %r4412;
	// end inline asm
	ld.const.u32 	%r4421, [matrix+2708];
	// begin inline asm
	dp4a.u32.u32 %r4420, %r4421, %r5766, %r4416;
	// end inline asm
	ld.const.u32 	%r4425, [matrix+2712];
	// begin inline asm
	dp4a.u32.u32 %r4424, %r4425, %r5770, %r4420;
	// end inline asm
	ld.const.u32 	%r4429, [matrix+2716];
	// begin inline asm
	dp4a.u32.u32 %r4428, %r4429, %r5774, %r4424;
	// end inline asm
	ld.const.u32 	%r4433, [matrix+2720];
	// begin inline asm
	dp4a.u32.u32 %r4432, %r4433, %r5778, %r4428;
	// end inline asm
	ld.const.u32 	%r4437, [matrix+2724];
	// begin inline asm
	dp4a.u32.u32 %r4436, %r4437, %r5782, %r4432;
	// end inline asm
	ld.const.u32 	%r4441, [matrix+2728];
	// begin inline asm
	dp4a.u32.u32 %r4440, %r4441, %r5786, %r4436;
	// end inline asm
	ld.const.u32 	%r4445, [matrix+2732];
	// begin inline asm
	dp4a.u32.u32 %r4444, %r4445, %r5790, %r4440;
	// end inline asm
	ld.const.u32 	%r4449, [matrix+2736];
	// begin inline asm
	dp4a.u32.u32 %r4448, %r4449, %r5794, %r4444;
	// end inline asm
	ld.const.u32 	%r4453, [matrix+2740];
	// begin inline asm
	dp4a.u32.u32 %r4452, %r4453, %r5798, %r4448;
	// end inline asm
	ld.const.u32 	%r4457, [matrix+2744];
	// begin inline asm
	dp4a.u32.u32 %r4456, %r4457, %r5802, %r4452;
	// end inline asm
	ld.const.u32 	%r4461, [matrix+2748];
	// begin inline asm
	dp4a.u32.u32 %r4460, %r4461, %r5806, %r4456;
	// end inline asm
	ld.const.u32 	%r4465, [matrix+2752];
	// begin inline asm
	dp4a.u32.u32 %r4464, %r4465, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4469, [matrix+2756];
	// begin inline asm
	dp4a.u32.u32 %r4468, %r4469, %r5750, %r4464;
	// end inline asm
	ld.const.u32 	%r4473, [matrix+2760];
	// begin inline asm
	dp4a.u32.u32 %r4472, %r4473, %r5754, %r4468;
	// end inline asm
	ld.const.u32 	%r4477, [matrix+2764];
	// begin inline asm
	dp4a.u32.u32 %r4476, %r4477, %r5758, %r4472;
	// end inline asm
	ld.const.u32 	%r4481, [matrix+2768];
	// begin inline asm
	dp4a.u32.u32 %r4480, %r4481, %r5762, %r4476;
	// end inline asm
	ld.const.u32 	%r4485, [matrix+2772];
	// begin inline asm
	dp4a.u32.u32 %r4484, %r4485, %r5766, %r4480;
	// end inline asm
	ld.const.u32 	%r4489, [matrix+2776];
	// begin inline asm
	dp4a.u32.u32 %r4488, %r4489, %r5770, %r4484;
	// end inline asm
	ld.const.u32 	%r4493, [matrix+2780];
	// begin inline asm
	dp4a.u32.u32 %r4492, %r4493, %r5774, %r4488;
	// end inline asm
	ld.const.u32 	%r4497, [matrix+2784];
	// begin inline asm
	dp4a.u32.u32 %r4496, %r4497, %r5778, %r4492;
	// end inline asm
	ld.const.u32 	%r4501, [matrix+2788];
	// begin inline asm
	dp4a.u32.u32 %r4500, %r4501, %r5782, %r4496;
	// end inline asm
	ld.const.u32 	%r4505, [matrix+2792];
	// begin inline asm
	dp4a.u32.u32 %r4504, %r4505, %r5786, %r4500;
	// end inline asm
	ld.const.u32 	%r4509, [matrix+2796];
	// begin inline asm
	dp4a.u32.u32 %r4508, %r4509, %r5790, %r4504;
	// end inline asm
	ld.const.u32 	%r4513, [matrix+2800];
	// begin inline asm
	dp4a.u32.u32 %r4512, %r4513, %r5794, %r4508;
	// end inline asm
	ld.const.u32 	%r4517, [matrix+2804];
	// begin inline asm
	dp4a.u32.u32 %r4516, %r4517, %r5798, %r4512;
	// end inline asm
	ld.const.u32 	%r4521, [matrix+2808];
	// begin inline asm
	dp4a.u32.u32 %r4520, %r4521, %r5802, %r4516;
	// end inline asm
	ld.const.u32 	%r4525, [matrix+2812];
	// begin inline asm
	dp4a.u32.u32 %r4524, %r4525, %r5806, %r4520;
	// end inline asm
	shr.u32 	%r6058, %r4460, 6;
	and.b32  	%r6059, %r6058, 240;
	shr.u32 	%r6060, %r4524, 10;
	or.b32  	%r6061, %r6060, %r6059;
	cvt.u64.u32 	%rd225, %r6061;
	xor.b64  	%rd226, %rd17, %rd225;
	ld.const.u32 	%r4529, [matrix+2816];
	// begin inline asm
	dp4a.u32.u32 %r4528, %r4529, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4533, [matrix+2820];
	// begin inline asm
	dp4a.u32.u32 %r4532, %r4533, %r5750, %r4528;
	// end inline asm
	ld.const.u32 	%r4537, [matrix+2824];
	// begin inline asm
	dp4a.u32.u32 %r4536, %r4537, %r5754, %r4532;
	// end inline asm
	ld.const.u32 	%r4541, [matrix+2828];
	// begin inline asm
	dp4a.u32.u32 %r4540, %r4541, %r5758, %r4536;
	// end inline asm
	ld.const.u32 	%r4545, [matrix+2832];
	// begin inline asm
	dp4a.u32.u32 %r4544, %r4545, %r5762, %r4540;
	// end inline asm
	ld.const.u32 	%r4549, [matrix+2836];
	// begin inline asm
	dp4a.u32.u32 %r4548, %r4549, %r5766, %r4544;
	// end inline asm
	ld.const.u32 	%r4553, [matrix+2840];
	// begin inline asm
	dp4a.u32.u32 %r4552, %r4553, %r5770, %r4548;
	// end inline asm
	ld.const.u32 	%r4557, [matrix+2844];
	// begin inline asm
	dp4a.u32.u32 %r4556, %r4557, %r5774, %r4552;
	// end inline asm
	ld.const.u32 	%r4561, [matrix+2848];
	// begin inline asm
	dp4a.u32.u32 %r4560, %r4561, %r5778, %r4556;
	// end inline asm
	ld.const.u32 	%r4565, [matrix+2852];
	// begin inline asm
	dp4a.u32.u32 %r4564, %r4565, %r5782, %r4560;
	// end inline asm
	ld.const.u32 	%r4569, [matrix+2856];
	// begin inline asm
	dp4a.u32.u32 %r4568, %r4569, %r5786, %r4564;
	// end inline asm
	ld.const.u32 	%r4573, [matrix+2860];
	// begin inline asm
	dp4a.u32.u32 %r4572, %r4573, %r5790, %r4568;
	// end inline asm
	ld.const.u32 	%r4577, [matrix+2864];
	// begin inline asm
	dp4a.u32.u32 %r4576, %r4577, %r5794, %r4572;
	// end inline asm
	ld.const.u32 	%r4581, [matrix+2868];
	// begin inline asm
	dp4a.u32.u32 %r4580, %r4581, %r5798, %r4576;
	// end inline asm
	ld.const.u32 	%r4585, [matrix+2872];
	// begin inline asm
	dp4a.u32.u32 %r4584, %r4585, %r5802, %r4580;
	// end inline asm
	ld.const.u32 	%r4589, [matrix+2876];
	// begin inline asm
	dp4a.u32.u32 %r4588, %r4589, %r5806, %r4584;
	// end inline asm
	ld.const.u32 	%r4593, [matrix+2880];
	// begin inline asm
	dp4a.u32.u32 %r4592, %r4593, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4597, [matrix+2884];
	// begin inline asm
	dp4a.u32.u32 %r4596, %r4597, %r5750, %r4592;
	// end inline asm
	ld.const.u32 	%r4601, [matrix+2888];
	// begin inline asm
	dp4a.u32.u32 %r4600, %r4601, %r5754, %r4596;
	// end inline asm
	ld.const.u32 	%r4605, [matrix+2892];
	// begin inline asm
	dp4a.u32.u32 %r4604, %r4605, %r5758, %r4600;
	// end inline asm
	ld.const.u32 	%r4609, [matrix+2896];
	// begin inline asm
	dp4a.u32.u32 %r4608, %r4609, %r5762, %r4604;
	// end inline asm
	ld.const.u32 	%r4613, [matrix+2900];
	// begin inline asm
	dp4a.u32.u32 %r4612, %r4613, %r5766, %r4608;
	// end inline asm
	ld.const.u32 	%r4617, [matrix+2904];
	// begin inline asm
	dp4a.u32.u32 %r4616, %r4617, %r5770, %r4612;
	// end inline asm
	ld.const.u32 	%r4621, [matrix+2908];
	// begin inline asm
	dp4a.u32.u32 %r4620, %r4621, %r5774, %r4616;
	// end inline asm
	ld.const.u32 	%r4625, [matrix+2912];
	// begin inline asm
	dp4a.u32.u32 %r4624, %r4625, %r5778, %r4620;
	// end inline asm
	ld.const.u32 	%r4629, [matrix+2916];
	// begin inline asm
	dp4a.u32.u32 %r4628, %r4629, %r5782, %r4624;
	// end inline asm
	ld.const.u32 	%r4633, [matrix+2920];
	// begin inline asm
	dp4a.u32.u32 %r4632, %r4633, %r5786, %r4628;
	// end inline asm
	ld.const.u32 	%r4637, [matrix+2924];
	// begin inline asm
	dp4a.u32.u32 %r4636, %r4637, %r5790, %r4632;
	// end inline asm
	ld.const.u32 	%r4641, [matrix+2928];
	// begin inline asm
	dp4a.u32.u32 %r4640, %r4641, %r5794, %r4636;
	// end inline asm
	ld.const.u32 	%r4645, [matrix+2932];
	// begin inline asm
	dp4a.u32.u32 %r4644, %r4645, %r5798, %r4640;
	// end inline asm
	ld.const.u32 	%r4649, [matrix+2936];
	// begin inline asm
	dp4a.u32.u32 %r4648, %r4649, %r5802, %r4644;
	// end inline asm
	ld.const.u32 	%r4653, [matrix+2940];
	// begin inline asm
	dp4a.u32.u32 %r4652, %r4653, %r5806, %r4648;
	// end inline asm
	shr.u32 	%r6062, %r4588, 6;
	and.b32  	%r6063, %r6062, 240;
	shr.u32 	%r6064, %r4652, 10;
	or.b32  	%r6065, %r6064, %r6063;
	cvt.u64.u32 	%rd227, %r6065;
	xor.b64  	%rd228, %rd18, %rd227;
	ld.const.u32 	%r4657, [matrix+2944];
	// begin inline asm
	dp4a.u32.u32 %r4656, %r4657, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4661, [matrix+2948];
	// begin inline asm
	dp4a.u32.u32 %r4660, %r4661, %r5750, %r4656;
	// end inline asm
	ld.const.u32 	%r4665, [matrix+2952];
	// begin inline asm
	dp4a.u32.u32 %r4664, %r4665, %r5754, %r4660;
	// end inline asm
	ld.const.u32 	%r4669, [matrix+2956];
	// begin inline asm
	dp4a.u32.u32 %r4668, %r4669, %r5758, %r4664;
	// end inline asm
	ld.const.u32 	%r4673, [matrix+2960];
	// begin inline asm
	dp4a.u32.u32 %r4672, %r4673, %r5762, %r4668;
	// end inline asm
	ld.const.u32 	%r4677, [matrix+2964];
	// begin inline asm
	dp4a.u32.u32 %r4676, %r4677, %r5766, %r4672;
	// end inline asm
	ld.const.u32 	%r4681, [matrix+2968];
	// begin inline asm
	dp4a.u32.u32 %r4680, %r4681, %r5770, %r4676;
	// end inline asm
	ld.const.u32 	%r4685, [matrix+2972];
	// begin inline asm
	dp4a.u32.u32 %r4684, %r4685, %r5774, %r4680;
	// end inline asm
	ld.const.u32 	%r4689, [matrix+2976];
	// begin inline asm
	dp4a.u32.u32 %r4688, %r4689, %r5778, %r4684;
	// end inline asm
	ld.const.u32 	%r4693, [matrix+2980];
	// begin inline asm
	dp4a.u32.u32 %r4692, %r4693, %r5782, %r4688;
	// end inline asm
	ld.const.u32 	%r4697, [matrix+2984];
	// begin inline asm
	dp4a.u32.u32 %r4696, %r4697, %r5786, %r4692;
	// end inline asm
	ld.const.u32 	%r4701, [matrix+2988];
	// begin inline asm
	dp4a.u32.u32 %r4700, %r4701, %r5790, %r4696;
	// end inline asm
	ld.const.u32 	%r4705, [matrix+2992];
	// begin inline asm
	dp4a.u32.u32 %r4704, %r4705, %r5794, %r4700;
	// end inline asm
	ld.const.u32 	%r4709, [matrix+2996];
	// begin inline asm
	dp4a.u32.u32 %r4708, %r4709, %r5798, %r4704;
	// end inline asm
	ld.const.u32 	%r4713, [matrix+3000];
	// begin inline asm
	dp4a.u32.u32 %r4712, %r4713, %r5802, %r4708;
	// end inline asm
	ld.const.u32 	%r4717, [matrix+3004];
	// begin inline asm
	dp4a.u32.u32 %r4716, %r4717, %r5806, %r4712;
	// end inline asm
	ld.const.u32 	%r4721, [matrix+3008];
	// begin inline asm
	dp4a.u32.u32 %r4720, %r4721, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4725, [matrix+3012];
	// begin inline asm
	dp4a.u32.u32 %r4724, %r4725, %r5750, %r4720;
	// end inline asm
	ld.const.u32 	%r4729, [matrix+3016];
	// begin inline asm
	dp4a.u32.u32 %r4728, %r4729, %r5754, %r4724;
	// end inline asm
	ld.const.u32 	%r4733, [matrix+3020];
	// begin inline asm
	dp4a.u32.u32 %r4732, %r4733, %r5758, %r4728;
	// end inline asm
	ld.const.u32 	%r4737, [matrix+3024];
	// begin inline asm
	dp4a.u32.u32 %r4736, %r4737, %r5762, %r4732;
	// end inline asm
	ld.const.u32 	%r4741, [matrix+3028];
	// begin inline asm
	dp4a.u32.u32 %r4740, %r4741, %r5766, %r4736;
	// end inline asm
	ld.const.u32 	%r4745, [matrix+3032];
	// begin inline asm
	dp4a.u32.u32 %r4744, %r4745, %r5770, %r4740;
	// end inline asm
	ld.const.u32 	%r4749, [matrix+3036];
	// begin inline asm
	dp4a.u32.u32 %r4748, %r4749, %r5774, %r4744;
	// end inline asm
	ld.const.u32 	%r4753, [matrix+3040];
	// begin inline asm
	dp4a.u32.u32 %r4752, %r4753, %r5778, %r4748;
	// end inline asm
	ld.const.u32 	%r4757, [matrix+3044];
	// begin inline asm
	dp4a.u32.u32 %r4756, %r4757, %r5782, %r4752;
	// end inline asm
	ld.const.u32 	%r4761, [matrix+3048];
	// begin inline asm
	dp4a.u32.u32 %r4760, %r4761, %r5786, %r4756;
	// end inline asm
	ld.const.u32 	%r4765, [matrix+3052];
	// begin inline asm
	dp4a.u32.u32 %r4764, %r4765, %r5790, %r4760;
	// end inline asm
	ld.const.u32 	%r4769, [matrix+3056];
	// begin inline asm
	dp4a.u32.u32 %r4768, %r4769, %r5794, %r4764;
	// end inline asm
	ld.const.u32 	%r4773, [matrix+3060];
	// begin inline asm
	dp4a.u32.u32 %r4772, %r4773, %r5798, %r4768;
	// end inline asm
	ld.const.u32 	%r4777, [matrix+3064];
	// begin inline asm
	dp4a.u32.u32 %r4776, %r4777, %r5802, %r4772;
	// end inline asm
	ld.const.u32 	%r4781, [matrix+3068];
	// begin inline asm
	dp4a.u32.u32 %r4780, %r4781, %r5806, %r4776;
	// end inline asm
	shr.u32 	%r6066, %r4716, 6;
	and.b32  	%r6067, %r6066, 240;
	ld.const.u32 	%r4785, [matrix+3072];
	// begin inline asm
	dp4a.u32.u32 %r4784, %r4785, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4789, [matrix+3076];
	// begin inline asm
	dp4a.u32.u32 %r4788, %r4789, %r5750, %r4784;
	// end inline asm
	ld.const.u32 	%r4793, [matrix+3080];
	// begin inline asm
	dp4a.u32.u32 %r4792, %r4793, %r5754, %r4788;
	// end inline asm
	ld.const.u32 	%r4797, [matrix+3084];
	// begin inline asm
	dp4a.u32.u32 %r4796, %r4797, %r5758, %r4792;
	// end inline asm
	ld.const.u32 	%r4801, [matrix+3088];
	// begin inline asm
	dp4a.u32.u32 %r4800, %r4801, %r5762, %r4796;
	// end inline asm
	ld.const.u32 	%r4805, [matrix+3092];
	// begin inline asm
	dp4a.u32.u32 %r4804, %r4805, %r5766, %r4800;
	// end inline asm
	ld.const.u32 	%r4809, [matrix+3096];
	// begin inline asm
	dp4a.u32.u32 %r4808, %r4809, %r5770, %r4804;
	// end inline asm
	ld.const.u32 	%r4813, [matrix+3100];
	// begin inline asm
	dp4a.u32.u32 %r4812, %r4813, %r5774, %r4808;
	// end inline asm
	ld.const.u32 	%r4817, [matrix+3104];
	// begin inline asm
	dp4a.u32.u32 %r4816, %r4817, %r5778, %r4812;
	// end inline asm
	ld.const.u32 	%r4821, [matrix+3108];
	// begin inline asm
	dp4a.u32.u32 %r4820, %r4821, %r5782, %r4816;
	// end inline asm
	ld.const.u32 	%r4825, [matrix+3112];
	// begin inline asm
	dp4a.u32.u32 %r4824, %r4825, %r5786, %r4820;
	// end inline asm
	ld.const.u32 	%r4829, [matrix+3116];
	// begin inline asm
	dp4a.u32.u32 %r4828, %r4829, %r5790, %r4824;
	// end inline asm
	ld.const.u32 	%r4833, [matrix+3120];
	// begin inline asm
	dp4a.u32.u32 %r4832, %r4833, %r5794, %r4828;
	// end inline asm
	ld.const.u32 	%r4837, [matrix+3124];
	// begin inline asm
	dp4a.u32.u32 %r4836, %r4837, %r5798, %r4832;
	// end inline asm
	ld.const.u32 	%r4841, [matrix+3128];
	// begin inline asm
	dp4a.u32.u32 %r4840, %r4841, %r5802, %r4836;
	// end inline asm
	ld.const.u32 	%r4845, [matrix+3132];
	// begin inline asm
	dp4a.u32.u32 %r4844, %r4845, %r5806, %r4840;
	// end inline asm
	ld.const.u32 	%r4849, [matrix+3136];
	// begin inline asm
	dp4a.u32.u32 %r4848, %r4849, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4853, [matrix+3140];
	// begin inline asm
	dp4a.u32.u32 %r4852, %r4853, %r5750, %r4848;
	// end inline asm
	ld.const.u32 	%r4857, [matrix+3144];
	// begin inline asm
	dp4a.u32.u32 %r4856, %r4857, %r5754, %r4852;
	// end inline asm
	ld.const.u32 	%r4861, [matrix+3148];
	// begin inline asm
	dp4a.u32.u32 %r4860, %r4861, %r5758, %r4856;
	// end inline asm
	ld.const.u32 	%r4865, [matrix+3152];
	// begin inline asm
	dp4a.u32.u32 %r4864, %r4865, %r5762, %r4860;
	// end inline asm
	ld.const.u32 	%r4869, [matrix+3156];
	// begin inline asm
	dp4a.u32.u32 %r4868, %r4869, %r5766, %r4864;
	// end inline asm
	ld.const.u32 	%r4873, [matrix+3160];
	// begin inline asm
	dp4a.u32.u32 %r4872, %r4873, %r5770, %r4868;
	// end inline asm
	ld.const.u32 	%r4877, [matrix+3164];
	// begin inline asm
	dp4a.u32.u32 %r4876, %r4877, %r5774, %r4872;
	// end inline asm
	ld.const.u32 	%r4881, [matrix+3168];
	// begin inline asm
	dp4a.u32.u32 %r4880, %r4881, %r5778, %r4876;
	// end inline asm
	ld.const.u32 	%r4885, [matrix+3172];
	// begin inline asm
	dp4a.u32.u32 %r4884, %r4885, %r5782, %r4880;
	// end inline asm
	ld.const.u32 	%r4889, [matrix+3176];
	// begin inline asm
	dp4a.u32.u32 %r4888, %r4889, %r5786, %r4884;
	// end inline asm
	ld.const.u32 	%r4893, [matrix+3180];
	// begin inline asm
	dp4a.u32.u32 %r4892, %r4893, %r5790, %r4888;
	// end inline asm
	ld.const.u32 	%r4897, [matrix+3184];
	// begin inline asm
	dp4a.u32.u32 %r4896, %r4897, %r5794, %r4892;
	// end inline asm
	ld.const.u32 	%r4901, [matrix+3188];
	// begin inline asm
	dp4a.u32.u32 %r4900, %r4901, %r5798, %r4896;
	// end inline asm
	ld.const.u32 	%r4905, [matrix+3192];
	// begin inline asm
	dp4a.u32.u32 %r4904, %r4905, %r5802, %r4900;
	// end inline asm
	ld.const.u32 	%r4909, [matrix+3196];
	// begin inline asm
	dp4a.u32.u32 %r4908, %r4909, %r5806, %r4904;
	// end inline asm
	shr.u32 	%r6068, %r4844, 6;
	and.b32  	%r6069, %r6068, 240;
	shr.u32 	%r6070, %r4908, 10;
	and.b32  	%r6071, %r6070, 255;
	or.b32  	%r6072, %r6071, %r6069;
	cvt.u64.u32 	%rd229, %r6072;
	ld.const.u32 	%r4913, [matrix+3200];
	// begin inline asm
	dp4a.u32.u32 %r4912, %r4913, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4917, [matrix+3204];
	// begin inline asm
	dp4a.u32.u32 %r4916, %r4917, %r5750, %r4912;
	// end inline asm
	ld.const.u32 	%r4921, [matrix+3208];
	// begin inline asm
	dp4a.u32.u32 %r4920, %r4921, %r5754, %r4916;
	// end inline asm
	ld.const.u32 	%r4925, [matrix+3212];
	// begin inline asm
	dp4a.u32.u32 %r4924, %r4925, %r5758, %r4920;
	// end inline asm
	ld.const.u32 	%r4929, [matrix+3216];
	// begin inline asm
	dp4a.u32.u32 %r4928, %r4929, %r5762, %r4924;
	// end inline asm
	ld.const.u32 	%r4933, [matrix+3220];
	// begin inline asm
	dp4a.u32.u32 %r4932, %r4933, %r5766, %r4928;
	// end inline asm
	ld.const.u32 	%r4937, [matrix+3224];
	// begin inline asm
	dp4a.u32.u32 %r4936, %r4937, %r5770, %r4932;
	// end inline asm
	ld.const.u32 	%r4941, [matrix+3228];
	// begin inline asm
	dp4a.u32.u32 %r4940, %r4941, %r5774, %r4936;
	// end inline asm
	ld.const.u32 	%r4945, [matrix+3232];
	// begin inline asm
	dp4a.u32.u32 %r4944, %r4945, %r5778, %r4940;
	// end inline asm
	ld.const.u32 	%r4949, [matrix+3236];
	// begin inline asm
	dp4a.u32.u32 %r4948, %r4949, %r5782, %r4944;
	// end inline asm
	ld.const.u32 	%r4953, [matrix+3240];
	// begin inline asm
	dp4a.u32.u32 %r4952, %r4953, %r5786, %r4948;
	// end inline asm
	ld.const.u32 	%r4957, [matrix+3244];
	// begin inline asm
	dp4a.u32.u32 %r4956, %r4957, %r5790, %r4952;
	// end inline asm
	ld.const.u32 	%r4961, [matrix+3248];
	// begin inline asm
	dp4a.u32.u32 %r4960, %r4961, %r5794, %r4956;
	// end inline asm
	ld.const.u32 	%r4965, [matrix+3252];
	// begin inline asm
	dp4a.u32.u32 %r4964, %r4965, %r5798, %r4960;
	// end inline asm
	ld.const.u32 	%r4969, [matrix+3256];
	// begin inline asm
	dp4a.u32.u32 %r4968, %r4969, %r5802, %r4964;
	// end inline asm
	ld.const.u32 	%r4973, [matrix+3260];
	// begin inline asm
	dp4a.u32.u32 %r4972, %r4973, %r5806, %r4968;
	// end inline asm
	ld.const.u32 	%r4977, [matrix+3264];
	// begin inline asm
	dp4a.u32.u32 %r4976, %r4977, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r4981, [matrix+3268];
	// begin inline asm
	dp4a.u32.u32 %r4980, %r4981, %r5750, %r4976;
	// end inline asm
	ld.const.u32 	%r4985, [matrix+3272];
	// begin inline asm
	dp4a.u32.u32 %r4984, %r4985, %r5754, %r4980;
	// end inline asm
	ld.const.u32 	%r4989, [matrix+3276];
	// begin inline asm
	dp4a.u32.u32 %r4988, %r4989, %r5758, %r4984;
	// end inline asm
	ld.const.u32 	%r4993, [matrix+3280];
	// begin inline asm
	dp4a.u32.u32 %r4992, %r4993, %r5762, %r4988;
	// end inline asm
	ld.const.u32 	%r4997, [matrix+3284];
	// begin inline asm
	dp4a.u32.u32 %r4996, %r4997, %r5766, %r4992;
	// end inline asm
	ld.const.u32 	%r5001, [matrix+3288];
	// begin inline asm
	dp4a.u32.u32 %r5000, %r5001, %r5770, %r4996;
	// end inline asm
	ld.const.u32 	%r5005, [matrix+3292];
	// begin inline asm
	dp4a.u32.u32 %r5004, %r5005, %r5774, %r5000;
	// end inline asm
	ld.const.u32 	%r5009, [matrix+3296];
	// begin inline asm
	dp4a.u32.u32 %r5008, %r5009, %r5778, %r5004;
	// end inline asm
	ld.const.u32 	%r5013, [matrix+3300];
	// begin inline asm
	dp4a.u32.u32 %r5012, %r5013, %r5782, %r5008;
	// end inline asm
	ld.const.u32 	%r5017, [matrix+3304];
	// begin inline asm
	dp4a.u32.u32 %r5016, %r5017, %r5786, %r5012;
	// end inline asm
	ld.const.u32 	%r5021, [matrix+3308];
	// begin inline asm
	dp4a.u32.u32 %r5020, %r5021, %r5790, %r5016;
	// end inline asm
	ld.const.u32 	%r5025, [matrix+3312];
	// begin inline asm
	dp4a.u32.u32 %r5024, %r5025, %r5794, %r5020;
	// end inline asm
	ld.const.u32 	%r5029, [matrix+3316];
	// begin inline asm
	dp4a.u32.u32 %r5028, %r5029, %r5798, %r5024;
	// end inline asm
	ld.const.u32 	%r5033, [matrix+3320];
	// begin inline asm
	dp4a.u32.u32 %r5032, %r5033, %r5802, %r5028;
	// end inline asm
	ld.const.u32 	%r5037, [matrix+3324];
	// begin inline asm
	dp4a.u32.u32 %r5036, %r5037, %r5806, %r5032;
	// end inline asm
	shr.u32 	%r6073, %r4972, 6;
	and.b32  	%r6074, %r6073, 240;
	shr.u32 	%r6075, %r5036, 10;
	or.b32  	%r6076, %r6075, %r6074;
	cvt.u64.u32 	%rd230, %r6076;
	xor.b64  	%rd231, %rd201, %rd230;
	and.b64  	%rd232, %rd9, 255;
	xor.b64  	%rd233, %rd232, %rd229;
	ld.const.u32 	%r5041, [matrix+3328];
	// begin inline asm
	dp4a.u32.u32 %r5040, %r5041, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5045, [matrix+3332];
	// begin inline asm
	dp4a.u32.u32 %r5044, %r5045, %r5750, %r5040;
	// end inline asm
	ld.const.u32 	%r5049, [matrix+3336];
	// begin inline asm
	dp4a.u32.u32 %r5048, %r5049, %r5754, %r5044;
	// end inline asm
	ld.const.u32 	%r5053, [matrix+3340];
	// begin inline asm
	dp4a.u32.u32 %r5052, %r5053, %r5758, %r5048;
	// end inline asm
	ld.const.u32 	%r5057, [matrix+3344];
	// begin inline asm
	dp4a.u32.u32 %r5056, %r5057, %r5762, %r5052;
	// end inline asm
	ld.const.u32 	%r5061, [matrix+3348];
	// begin inline asm
	dp4a.u32.u32 %r5060, %r5061, %r5766, %r5056;
	// end inline asm
	ld.const.u32 	%r5065, [matrix+3352];
	// begin inline asm
	dp4a.u32.u32 %r5064, %r5065, %r5770, %r5060;
	// end inline asm
	ld.const.u32 	%r5069, [matrix+3356];
	// begin inline asm
	dp4a.u32.u32 %r5068, %r5069, %r5774, %r5064;
	// end inline asm
	ld.const.u32 	%r5073, [matrix+3360];
	// begin inline asm
	dp4a.u32.u32 %r5072, %r5073, %r5778, %r5068;
	// end inline asm
	ld.const.u32 	%r5077, [matrix+3364];
	// begin inline asm
	dp4a.u32.u32 %r5076, %r5077, %r5782, %r5072;
	// end inline asm
	ld.const.u32 	%r5081, [matrix+3368];
	// begin inline asm
	dp4a.u32.u32 %r5080, %r5081, %r5786, %r5076;
	// end inline asm
	ld.const.u32 	%r5085, [matrix+3372];
	// begin inline asm
	dp4a.u32.u32 %r5084, %r5085, %r5790, %r5080;
	// end inline asm
	ld.const.u32 	%r5089, [matrix+3376];
	// begin inline asm
	dp4a.u32.u32 %r5088, %r5089, %r5794, %r5084;
	// end inline asm
	ld.const.u32 	%r5093, [matrix+3380];
	// begin inline asm
	dp4a.u32.u32 %r5092, %r5093, %r5798, %r5088;
	// end inline asm
	ld.const.u32 	%r5097, [matrix+3384];
	// begin inline asm
	dp4a.u32.u32 %r5096, %r5097, %r5802, %r5092;
	// end inline asm
	ld.const.u32 	%r5101, [matrix+3388];
	// begin inline asm
	dp4a.u32.u32 %r5100, %r5101, %r5806, %r5096;
	// end inline asm
	ld.const.u32 	%r5105, [matrix+3392];
	// begin inline asm
	dp4a.u32.u32 %r5104, %r5105, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5109, [matrix+3396];
	// begin inline asm
	dp4a.u32.u32 %r5108, %r5109, %r5750, %r5104;
	// end inline asm
	ld.const.u32 	%r5113, [matrix+3400];
	// begin inline asm
	dp4a.u32.u32 %r5112, %r5113, %r5754, %r5108;
	// end inline asm
	ld.const.u32 	%r5117, [matrix+3404];
	// begin inline asm
	dp4a.u32.u32 %r5116, %r5117, %r5758, %r5112;
	// end inline asm
	ld.const.u32 	%r5121, [matrix+3408];
	// begin inline asm
	dp4a.u32.u32 %r5120, %r5121, %r5762, %r5116;
	// end inline asm
	ld.const.u32 	%r5125, [matrix+3412];
	// begin inline asm
	dp4a.u32.u32 %r5124, %r5125, %r5766, %r5120;
	// end inline asm
	ld.const.u32 	%r5129, [matrix+3416];
	// begin inline asm
	dp4a.u32.u32 %r5128, %r5129, %r5770, %r5124;
	// end inline asm
	ld.const.u32 	%r5133, [matrix+3420];
	// begin inline asm
	dp4a.u32.u32 %r5132, %r5133, %r5774, %r5128;
	// end inline asm
	ld.const.u32 	%r5137, [matrix+3424];
	// begin inline asm
	dp4a.u32.u32 %r5136, %r5137, %r5778, %r5132;
	// end inline asm
	ld.const.u32 	%r5141, [matrix+3428];
	// begin inline asm
	dp4a.u32.u32 %r5140, %r5141, %r5782, %r5136;
	// end inline asm
	ld.const.u32 	%r5145, [matrix+3432];
	// begin inline asm
	dp4a.u32.u32 %r5144, %r5145, %r5786, %r5140;
	// end inline asm
	ld.const.u32 	%r5149, [matrix+3436];
	// begin inline asm
	dp4a.u32.u32 %r5148, %r5149, %r5790, %r5144;
	// end inline asm
	ld.const.u32 	%r5153, [matrix+3440];
	// begin inline asm
	dp4a.u32.u32 %r5152, %r5153, %r5794, %r5148;
	// end inline asm
	ld.const.u32 	%r5157, [matrix+3444];
	// begin inline asm
	dp4a.u32.u32 %r5156, %r5157, %r5798, %r5152;
	// end inline asm
	ld.const.u32 	%r5161, [matrix+3448];
	// begin inline asm
	dp4a.u32.u32 %r5160, %r5161, %r5802, %r5156;
	// end inline asm
	ld.const.u32 	%r5165, [matrix+3452];
	// begin inline asm
	dp4a.u32.u32 %r5164, %r5165, %r5806, %r5160;
	// end inline asm
	shr.u32 	%r6077, %r5100, 6;
	and.b32  	%r6078, %r6077, 240;
	shr.u32 	%r6079, %r5164, 10;
	or.b32  	%r6080, %r6079, %r6078;
	cvt.u64.u32 	%rd234, %r6080;
	xor.b64  	%rd235, %rd202, %rd234;
	ld.const.u32 	%r5169, [matrix+3456];
	// begin inline asm
	dp4a.u32.u32 %r5168, %r5169, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5173, [matrix+3460];
	// begin inline asm
	dp4a.u32.u32 %r5172, %r5173, %r5750, %r5168;
	// end inline asm
	ld.const.u32 	%r5177, [matrix+3464];
	// begin inline asm
	dp4a.u32.u32 %r5176, %r5177, %r5754, %r5172;
	// end inline asm
	ld.const.u32 	%r5181, [matrix+3468];
	// begin inline asm
	dp4a.u32.u32 %r5180, %r5181, %r5758, %r5176;
	// end inline asm
	ld.const.u32 	%r5185, [matrix+3472];
	// begin inline asm
	dp4a.u32.u32 %r5184, %r5185, %r5762, %r5180;
	// end inline asm
	ld.const.u32 	%r5189, [matrix+3476];
	// begin inline asm
	dp4a.u32.u32 %r5188, %r5189, %r5766, %r5184;
	// end inline asm
	ld.const.u32 	%r5193, [matrix+3480];
	// begin inline asm
	dp4a.u32.u32 %r5192, %r5193, %r5770, %r5188;
	// end inline asm
	ld.const.u32 	%r5197, [matrix+3484];
	// begin inline asm
	dp4a.u32.u32 %r5196, %r5197, %r5774, %r5192;
	// end inline asm
	ld.const.u32 	%r5201, [matrix+3488];
	// begin inline asm
	dp4a.u32.u32 %r5200, %r5201, %r5778, %r5196;
	// end inline asm
	ld.const.u32 	%r5205, [matrix+3492];
	// begin inline asm
	dp4a.u32.u32 %r5204, %r5205, %r5782, %r5200;
	// end inline asm
	ld.const.u32 	%r5209, [matrix+3496];
	// begin inline asm
	dp4a.u32.u32 %r5208, %r5209, %r5786, %r5204;
	// end inline asm
	ld.const.u32 	%r5213, [matrix+3500];
	// begin inline asm
	dp4a.u32.u32 %r5212, %r5213, %r5790, %r5208;
	// end inline asm
	ld.const.u32 	%r5217, [matrix+3504];
	// begin inline asm
	dp4a.u32.u32 %r5216, %r5217, %r5794, %r5212;
	// end inline asm
	ld.const.u32 	%r5221, [matrix+3508];
	// begin inline asm
	dp4a.u32.u32 %r5220, %r5221, %r5798, %r5216;
	// end inline asm
	ld.const.u32 	%r5225, [matrix+3512];
	// begin inline asm
	dp4a.u32.u32 %r5224, %r5225, %r5802, %r5220;
	// end inline asm
	ld.const.u32 	%r5229, [matrix+3516];
	// begin inline asm
	dp4a.u32.u32 %r5228, %r5229, %r5806, %r5224;
	// end inline asm
	ld.const.u32 	%r5233, [matrix+3520];
	// begin inline asm
	dp4a.u32.u32 %r5232, %r5233, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5237, [matrix+3524];
	// begin inline asm
	dp4a.u32.u32 %r5236, %r5237, %r5750, %r5232;
	// end inline asm
	ld.const.u32 	%r5241, [matrix+3528];
	// begin inline asm
	dp4a.u32.u32 %r5240, %r5241, %r5754, %r5236;
	// end inline asm
	ld.const.u32 	%r5245, [matrix+3532];
	// begin inline asm
	dp4a.u32.u32 %r5244, %r5245, %r5758, %r5240;
	// end inline asm
	ld.const.u32 	%r5249, [matrix+3536];
	// begin inline asm
	dp4a.u32.u32 %r5248, %r5249, %r5762, %r5244;
	// end inline asm
	ld.const.u32 	%r5253, [matrix+3540];
	// begin inline asm
	dp4a.u32.u32 %r5252, %r5253, %r5766, %r5248;
	// end inline asm
	ld.const.u32 	%r5257, [matrix+3544];
	// begin inline asm
	dp4a.u32.u32 %r5256, %r5257, %r5770, %r5252;
	// end inline asm
	ld.const.u32 	%r5261, [matrix+3548];
	// begin inline asm
	dp4a.u32.u32 %r5260, %r5261, %r5774, %r5256;
	// end inline asm
	ld.const.u32 	%r5265, [matrix+3552];
	// begin inline asm
	dp4a.u32.u32 %r5264, %r5265, %r5778, %r5260;
	// end inline asm
	ld.const.u32 	%r5269, [matrix+3556];
	// begin inline asm
	dp4a.u32.u32 %r5268, %r5269, %r5782, %r5264;
	// end inline asm
	ld.const.u32 	%r5273, [matrix+3560];
	// begin inline asm
	dp4a.u32.u32 %r5272, %r5273, %r5786, %r5268;
	// end inline asm
	ld.const.u32 	%r5277, [matrix+3564];
	// begin inline asm
	dp4a.u32.u32 %r5276, %r5277, %r5790, %r5272;
	// end inline asm
	ld.const.u32 	%r5281, [matrix+3568];
	// begin inline asm
	dp4a.u32.u32 %r5280, %r5281, %r5794, %r5276;
	// end inline asm
	ld.const.u32 	%r5285, [matrix+3572];
	// begin inline asm
	dp4a.u32.u32 %r5284, %r5285, %r5798, %r5280;
	// end inline asm
	ld.const.u32 	%r5289, [matrix+3576];
	// begin inline asm
	dp4a.u32.u32 %r5288, %r5289, %r5802, %r5284;
	// end inline asm
	ld.const.u32 	%r5293, [matrix+3580];
	// begin inline asm
	dp4a.u32.u32 %r5292, %r5293, %r5806, %r5288;
	// end inline asm
	shr.u32 	%r6081, %r5228, 6;
	and.b32  	%r6082, %r6081, 240;
	shr.u32 	%r6083, %r5292, 10;
	or.b32  	%r6084, %r6083, %r6082;
	cvt.u64.u32 	%rd236, %r6084;
	xor.b64  	%rd237, %rd203, %rd236;
	ld.const.u32 	%r5297, [matrix+3584];
	// begin inline asm
	dp4a.u32.u32 %r5296, %r5297, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5301, [matrix+3588];
	// begin inline asm
	dp4a.u32.u32 %r5300, %r5301, %r5750, %r5296;
	// end inline asm
	ld.const.u32 	%r5305, [matrix+3592];
	// begin inline asm
	dp4a.u32.u32 %r5304, %r5305, %r5754, %r5300;
	// end inline asm
	ld.const.u32 	%r5309, [matrix+3596];
	// begin inline asm
	dp4a.u32.u32 %r5308, %r5309, %r5758, %r5304;
	// end inline asm
	ld.const.u32 	%r5313, [matrix+3600];
	// begin inline asm
	dp4a.u32.u32 %r5312, %r5313, %r5762, %r5308;
	// end inline asm
	ld.const.u32 	%r5317, [matrix+3604];
	// begin inline asm
	dp4a.u32.u32 %r5316, %r5317, %r5766, %r5312;
	// end inline asm
	ld.const.u32 	%r5321, [matrix+3608];
	// begin inline asm
	dp4a.u32.u32 %r5320, %r5321, %r5770, %r5316;
	// end inline asm
	ld.const.u32 	%r5325, [matrix+3612];
	// begin inline asm
	dp4a.u32.u32 %r5324, %r5325, %r5774, %r5320;
	// end inline asm
	ld.const.u32 	%r5329, [matrix+3616];
	// begin inline asm
	dp4a.u32.u32 %r5328, %r5329, %r5778, %r5324;
	// end inline asm
	ld.const.u32 	%r5333, [matrix+3620];
	// begin inline asm
	dp4a.u32.u32 %r5332, %r5333, %r5782, %r5328;
	// end inline asm
	ld.const.u32 	%r5337, [matrix+3624];
	// begin inline asm
	dp4a.u32.u32 %r5336, %r5337, %r5786, %r5332;
	// end inline asm
	ld.const.u32 	%r5341, [matrix+3628];
	// begin inline asm
	dp4a.u32.u32 %r5340, %r5341, %r5790, %r5336;
	// end inline asm
	ld.const.u32 	%r5345, [matrix+3632];
	// begin inline asm
	dp4a.u32.u32 %r5344, %r5345, %r5794, %r5340;
	// end inline asm
	ld.const.u32 	%r5349, [matrix+3636];
	// begin inline asm
	dp4a.u32.u32 %r5348, %r5349, %r5798, %r5344;
	// end inline asm
	ld.const.u32 	%r5353, [matrix+3640];
	// begin inline asm
	dp4a.u32.u32 %r5352, %r5353, %r5802, %r5348;
	// end inline asm
	ld.const.u32 	%r5357, [matrix+3644];
	// begin inline asm
	dp4a.u32.u32 %r5356, %r5357, %r5806, %r5352;
	// end inline asm
	ld.const.u32 	%r5361, [matrix+3648];
	// begin inline asm
	dp4a.u32.u32 %r5360, %r5361, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5365, [matrix+3652];
	// begin inline asm
	dp4a.u32.u32 %r5364, %r5365, %r5750, %r5360;
	// end inline asm
	ld.const.u32 	%r5369, [matrix+3656];
	// begin inline asm
	dp4a.u32.u32 %r5368, %r5369, %r5754, %r5364;
	// end inline asm
	ld.const.u32 	%r5373, [matrix+3660];
	// begin inline asm
	dp4a.u32.u32 %r5372, %r5373, %r5758, %r5368;
	// end inline asm
	ld.const.u32 	%r5377, [matrix+3664];
	// begin inline asm
	dp4a.u32.u32 %r5376, %r5377, %r5762, %r5372;
	// end inline asm
	ld.const.u32 	%r5381, [matrix+3668];
	// begin inline asm
	dp4a.u32.u32 %r5380, %r5381, %r5766, %r5376;
	// end inline asm
	ld.const.u32 	%r5385, [matrix+3672];
	// begin inline asm
	dp4a.u32.u32 %r5384, %r5385, %r5770, %r5380;
	// end inline asm
	ld.const.u32 	%r5389, [matrix+3676];
	// begin inline asm
	dp4a.u32.u32 %r5388, %r5389, %r5774, %r5384;
	// end inline asm
	ld.const.u32 	%r5393, [matrix+3680];
	// begin inline asm
	dp4a.u32.u32 %r5392, %r5393, %r5778, %r5388;
	// end inline asm
	ld.const.u32 	%r5397, [matrix+3684];
	// begin inline asm
	dp4a.u32.u32 %r5396, %r5397, %r5782, %r5392;
	// end inline asm
	ld.const.u32 	%r5401, [matrix+3688];
	// begin inline asm
	dp4a.u32.u32 %r5400, %r5401, %r5786, %r5396;
	// end inline asm
	ld.const.u32 	%r5405, [matrix+3692];
	// begin inline asm
	dp4a.u32.u32 %r5404, %r5405, %r5790, %r5400;
	// end inline asm
	ld.const.u32 	%r5409, [matrix+3696];
	// begin inline asm
	dp4a.u32.u32 %r5408, %r5409, %r5794, %r5404;
	// end inline asm
	ld.const.u32 	%r5413, [matrix+3700];
	// begin inline asm
	dp4a.u32.u32 %r5412, %r5413, %r5798, %r5408;
	// end inline asm
	ld.const.u32 	%r5417, [matrix+3704];
	// begin inline asm
	dp4a.u32.u32 %r5416, %r5417, %r5802, %r5412;
	// end inline asm
	ld.const.u32 	%r5421, [matrix+3708];
	// begin inline asm
	dp4a.u32.u32 %r5420, %r5421, %r5806, %r5416;
	// end inline asm
	shr.u32 	%r6085, %r5356, 6;
	and.b32  	%r6086, %r6085, 240;
	shr.u32 	%r6087, %r5420, 10;
	or.b32  	%r6088, %r6087, %r6086;
	cvt.u64.u32 	%rd238, %r6088;
	xor.b64  	%rd239, %rd204, %rd238;
	ld.const.u32 	%r5425, [matrix+3712];
	// begin inline asm
	dp4a.u32.u32 %r5424, %r5425, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5429, [matrix+3716];
	// begin inline asm
	dp4a.u32.u32 %r5428, %r5429, %r5750, %r5424;
	// end inline asm
	ld.const.u32 	%r5433, [matrix+3720];
	// begin inline asm
	dp4a.u32.u32 %r5432, %r5433, %r5754, %r5428;
	// end inline asm
	ld.const.u32 	%r5437, [matrix+3724];
	// begin inline asm
	dp4a.u32.u32 %r5436, %r5437, %r5758, %r5432;
	// end inline asm
	ld.const.u32 	%r5441, [matrix+3728];
	// begin inline asm
	dp4a.u32.u32 %r5440, %r5441, %r5762, %r5436;
	// end inline asm
	ld.const.u32 	%r5445, [matrix+3732];
	// begin inline asm
	dp4a.u32.u32 %r5444, %r5445, %r5766, %r5440;
	// end inline asm
	ld.const.u32 	%r5449, [matrix+3736];
	// begin inline asm
	dp4a.u32.u32 %r5448, %r5449, %r5770, %r5444;
	// end inline asm
	ld.const.u32 	%r5453, [matrix+3740];
	// begin inline asm
	dp4a.u32.u32 %r5452, %r5453, %r5774, %r5448;
	// end inline asm
	ld.const.u32 	%r5457, [matrix+3744];
	// begin inline asm
	dp4a.u32.u32 %r5456, %r5457, %r5778, %r5452;
	// end inline asm
	ld.const.u32 	%r5461, [matrix+3748];
	// begin inline asm
	dp4a.u32.u32 %r5460, %r5461, %r5782, %r5456;
	// end inline asm
	ld.const.u32 	%r5465, [matrix+3752];
	// begin inline asm
	dp4a.u32.u32 %r5464, %r5465, %r5786, %r5460;
	// end inline asm
	ld.const.u32 	%r5469, [matrix+3756];
	// begin inline asm
	dp4a.u32.u32 %r5468, %r5469, %r5790, %r5464;
	// end inline asm
	ld.const.u32 	%r5473, [matrix+3760];
	// begin inline asm
	dp4a.u32.u32 %r5472, %r5473, %r5794, %r5468;
	// end inline asm
	ld.const.u32 	%r5477, [matrix+3764];
	// begin inline asm
	dp4a.u32.u32 %r5476, %r5477, %r5798, %r5472;
	// end inline asm
	ld.const.u32 	%r5481, [matrix+3768];
	// begin inline asm
	dp4a.u32.u32 %r5480, %r5481, %r5802, %r5476;
	// end inline asm
	ld.const.u32 	%r5485, [matrix+3772];
	// begin inline asm
	dp4a.u32.u32 %r5484, %r5485, %r5806, %r5480;
	// end inline asm
	ld.const.u32 	%r5489, [matrix+3776];
	// begin inline asm
	dp4a.u32.u32 %r5488, %r5489, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5493, [matrix+3780];
	// begin inline asm
	dp4a.u32.u32 %r5492, %r5493, %r5750, %r5488;
	// end inline asm
	ld.const.u32 	%r5497, [matrix+3784];
	// begin inline asm
	dp4a.u32.u32 %r5496, %r5497, %r5754, %r5492;
	// end inline asm
	ld.const.u32 	%r5501, [matrix+3788];
	// begin inline asm
	dp4a.u32.u32 %r5500, %r5501, %r5758, %r5496;
	// end inline asm
	ld.const.u32 	%r5505, [matrix+3792];
	// begin inline asm
	dp4a.u32.u32 %r5504, %r5505, %r5762, %r5500;
	// end inline asm
	ld.const.u32 	%r5509, [matrix+3796];
	// begin inline asm
	dp4a.u32.u32 %r5508, %r5509, %r5766, %r5504;
	// end inline asm
	ld.const.u32 	%r5513, [matrix+3800];
	// begin inline asm
	dp4a.u32.u32 %r5512, %r5513, %r5770, %r5508;
	// end inline asm
	ld.const.u32 	%r5517, [matrix+3804];
	// begin inline asm
	dp4a.u32.u32 %r5516, %r5517, %r5774, %r5512;
	// end inline asm
	ld.const.u32 	%r5521, [matrix+3808];
	// begin inline asm
	dp4a.u32.u32 %r5520, %r5521, %r5778, %r5516;
	// end inline asm
	ld.const.u32 	%r5525, [matrix+3812];
	// begin inline asm
	dp4a.u32.u32 %r5524, %r5525, %r5782, %r5520;
	// end inline asm
	ld.const.u32 	%r5529, [matrix+3816];
	// begin inline asm
	dp4a.u32.u32 %r5528, %r5529, %r5786, %r5524;
	// end inline asm
	ld.const.u32 	%r5533, [matrix+3820];
	// begin inline asm
	dp4a.u32.u32 %r5532, %r5533, %r5790, %r5528;
	// end inline asm
	ld.const.u32 	%r5537, [matrix+3824];
	// begin inline asm
	dp4a.u32.u32 %r5536, %r5537, %r5794, %r5532;
	// end inline asm
	ld.const.u32 	%r5541, [matrix+3828];
	// begin inline asm
	dp4a.u32.u32 %r5540, %r5541, %r5798, %r5536;
	// end inline asm
	ld.const.u32 	%r5545, [matrix+3832];
	// begin inline asm
	dp4a.u32.u32 %r5544, %r5545, %r5802, %r5540;
	// end inline asm
	ld.const.u32 	%r5549, [matrix+3836];
	// begin inline asm
	dp4a.u32.u32 %r5548, %r5549, %r5806, %r5544;
	// end inline asm
	shr.u32 	%r6089, %r5484, 6;
	and.b32  	%r6090, %r6089, 240;
	shr.u32 	%r6091, %r5548, 10;
	or.b32  	%r6092, %r6091, %r6090;
	cvt.u64.u32 	%rd240, %r6092;
	xor.b64  	%rd241, %rd206, %rd240;
	ld.const.u32 	%r5553, [matrix+3840];
	// begin inline asm
	dp4a.u32.u32 %r5552, %r5553, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5557, [matrix+3844];
	// begin inline asm
	dp4a.u32.u32 %r5556, %r5557, %r5750, %r5552;
	// end inline asm
	ld.const.u32 	%r5561, [matrix+3848];
	// begin inline asm
	dp4a.u32.u32 %r5560, %r5561, %r5754, %r5556;
	// end inline asm
	ld.const.u32 	%r5565, [matrix+3852];
	// begin inline asm
	dp4a.u32.u32 %r5564, %r5565, %r5758, %r5560;
	// end inline asm
	ld.const.u32 	%r5569, [matrix+3856];
	// begin inline asm
	dp4a.u32.u32 %r5568, %r5569, %r5762, %r5564;
	// end inline asm
	ld.const.u32 	%r5573, [matrix+3860];
	// begin inline asm
	dp4a.u32.u32 %r5572, %r5573, %r5766, %r5568;
	// end inline asm
	ld.const.u32 	%r5577, [matrix+3864];
	// begin inline asm
	dp4a.u32.u32 %r5576, %r5577, %r5770, %r5572;
	// end inline asm
	ld.const.u32 	%r5581, [matrix+3868];
	// begin inline asm
	dp4a.u32.u32 %r5580, %r5581, %r5774, %r5576;
	// end inline asm
	ld.const.u32 	%r5585, [matrix+3872];
	// begin inline asm
	dp4a.u32.u32 %r5584, %r5585, %r5778, %r5580;
	// end inline asm
	ld.const.u32 	%r5589, [matrix+3876];
	// begin inline asm
	dp4a.u32.u32 %r5588, %r5589, %r5782, %r5584;
	// end inline asm
	ld.const.u32 	%r5593, [matrix+3880];
	// begin inline asm
	dp4a.u32.u32 %r5592, %r5593, %r5786, %r5588;
	// end inline asm
	ld.const.u32 	%r5597, [matrix+3884];
	// begin inline asm
	dp4a.u32.u32 %r5596, %r5597, %r5790, %r5592;
	// end inline asm
	ld.const.u32 	%r5601, [matrix+3888];
	// begin inline asm
	dp4a.u32.u32 %r5600, %r5601, %r5794, %r5596;
	// end inline asm
	ld.const.u32 	%r5605, [matrix+3892];
	// begin inline asm
	dp4a.u32.u32 %r5604, %r5605, %r5798, %r5600;
	// end inline asm
	ld.const.u32 	%r5609, [matrix+3896];
	// begin inline asm
	dp4a.u32.u32 %r5608, %r5609, %r5802, %r5604;
	// end inline asm
	ld.const.u32 	%r5613, [matrix+3900];
	// begin inline asm
	dp4a.u32.u32 %r5612, %r5613, %r5806, %r5608;
	// end inline asm
	ld.const.u32 	%r5617, [matrix+3904];
	// begin inline asm
	dp4a.u32.u32 %r5616, %r5617, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5621, [matrix+3908];
	// begin inline asm
	dp4a.u32.u32 %r5620, %r5621, %r5750, %r5616;
	// end inline asm
	ld.const.u32 	%r5625, [matrix+3912];
	// begin inline asm
	dp4a.u32.u32 %r5624, %r5625, %r5754, %r5620;
	// end inline asm
	ld.const.u32 	%r5629, [matrix+3916];
	// begin inline asm
	dp4a.u32.u32 %r5628, %r5629, %r5758, %r5624;
	// end inline asm
	ld.const.u32 	%r5633, [matrix+3920];
	// begin inline asm
	dp4a.u32.u32 %r5632, %r5633, %r5762, %r5628;
	// end inline asm
	ld.const.u32 	%r5637, [matrix+3924];
	// begin inline asm
	dp4a.u32.u32 %r5636, %r5637, %r5766, %r5632;
	// end inline asm
	ld.const.u32 	%r5641, [matrix+3928];
	// begin inline asm
	dp4a.u32.u32 %r5640, %r5641, %r5770, %r5636;
	// end inline asm
	ld.const.u32 	%r5645, [matrix+3932];
	// begin inline asm
	dp4a.u32.u32 %r5644, %r5645, %r5774, %r5640;
	// end inline asm
	ld.const.u32 	%r5649, [matrix+3936];
	// begin inline asm
	dp4a.u32.u32 %r5648, %r5649, %r5778, %r5644;
	// end inline asm
	ld.const.u32 	%r5653, [matrix+3940];
	// begin inline asm
	dp4a.u32.u32 %r5652, %r5653, %r5782, %r5648;
	// end inline asm
	ld.const.u32 	%r5657, [matrix+3944];
	// begin inline asm
	dp4a.u32.u32 %r5656, %r5657, %r5786, %r5652;
	// end inline asm
	ld.const.u32 	%r5661, [matrix+3948];
	// begin inline asm
	dp4a.u32.u32 %r5660, %r5661, %r5790, %r5656;
	// end inline asm
	ld.const.u32 	%r5665, [matrix+3952];
	// begin inline asm
	dp4a.u32.u32 %r5664, %r5665, %r5794, %r5660;
	// end inline asm
	ld.const.u32 	%r5669, [matrix+3956];
	// begin inline asm
	dp4a.u32.u32 %r5668, %r5669, %r5798, %r5664;
	// end inline asm
	ld.const.u32 	%r5673, [matrix+3960];
	// begin inline asm
	dp4a.u32.u32 %r5672, %r5673, %r5802, %r5668;
	// end inline asm
	ld.const.u32 	%r5677, [matrix+3964];
	// begin inline asm
	dp4a.u32.u32 %r5676, %r5677, %r5806, %r5672;
	// end inline asm
	shr.u32 	%r6093, %r5612, 6;
	and.b32  	%r6094, %r6093, 240;
	shr.u32 	%r6095, %r5676, 10;
	or.b32  	%r6096, %r6095, %r6094;
	cvt.u64.u32 	%rd242, %r6096;
	xor.b64  	%rd243, %rd208, %rd242;
	ld.const.u32 	%r5681, [matrix+3968];
	// begin inline asm
	dp4a.u32.u32 %r5680, %r5681, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5685, [matrix+3972];
	// begin inline asm
	dp4a.u32.u32 %r5684, %r5685, %r5750, %r5680;
	// end inline asm
	ld.const.u32 	%r5689, [matrix+3976];
	// begin inline asm
	dp4a.u32.u32 %r5688, %r5689, %r5754, %r5684;
	// end inline asm
	ld.const.u32 	%r5693, [matrix+3980];
	// begin inline asm
	dp4a.u32.u32 %r5692, %r5693, %r5758, %r5688;
	// end inline asm
	ld.const.u32 	%r5697, [matrix+3984];
	// begin inline asm
	dp4a.u32.u32 %r5696, %r5697, %r5762, %r5692;
	// end inline asm
	ld.const.u32 	%r5701, [matrix+3988];
	// begin inline asm
	dp4a.u32.u32 %r5700, %r5701, %r5766, %r5696;
	// end inline asm
	ld.const.u32 	%r5705, [matrix+3992];
	// begin inline asm
	dp4a.u32.u32 %r5704, %r5705, %r5770, %r5700;
	// end inline asm
	ld.const.u32 	%r5709, [matrix+3996];
	// begin inline asm
	dp4a.u32.u32 %r5708, %r5709, %r5774, %r5704;
	// end inline asm
	ld.const.u32 	%r5713, [matrix+4000];
	// begin inline asm
	dp4a.u32.u32 %r5712, %r5713, %r5778, %r5708;
	// end inline asm
	ld.const.u32 	%r5717, [matrix+4004];
	// begin inline asm
	dp4a.u32.u32 %r5716, %r5717, %r5782, %r5712;
	// end inline asm
	ld.const.u32 	%r5721, [matrix+4008];
	// begin inline asm
	dp4a.u32.u32 %r5720, %r5721, %r5786, %r5716;
	// end inline asm
	ld.const.u32 	%r5725, [matrix+4012];
	// begin inline asm
	dp4a.u32.u32 %r5724, %r5725, %r5790, %r5720;
	// end inline asm
	ld.const.u32 	%r5729, [matrix+4016];
	// begin inline asm
	dp4a.u32.u32 %r5728, %r5729, %r5794, %r5724;
	// end inline asm
	ld.const.u32 	%r5733, [matrix+4020];
	// begin inline asm
	dp4a.u32.u32 %r5732, %r5733, %r5798, %r5728;
	// end inline asm
	ld.const.u32 	%r5737, [matrix+4024];
	// begin inline asm
	dp4a.u32.u32 %r5736, %r5737, %r5802, %r5732;
	// end inline asm
	ld.const.u32 	%r5741, [matrix+4028];
	// begin inline asm
	dp4a.u32.u32 %r5740, %r5741, %r5806, %r5736;
	// end inline asm
	ld.const.u32 	%r5745, [matrix+4032];
	// begin inline asm
	dp4a.u32.u32 %r5744, %r5745, %r5746, %r6244;
	// end inline asm
	ld.const.u32 	%r5749, [matrix+4036];
	// begin inline asm
	dp4a.u32.u32 %r5748, %r5749, %r5750, %r5744;
	// end inline asm
	ld.const.u32 	%r5753, [matrix+4040];
	// begin inline asm
	dp4a.u32.u32 %r5752, %r5753, %r5754, %r5748;
	// end inline asm
	ld.const.u32 	%r5757, [matrix+4044];
	// begin inline asm
	dp4a.u32.u32 %r5756, %r5757, %r5758, %r5752;
	// end inline asm
	ld.const.u32 	%r5761, [matrix+4048];
	// begin inline asm
	dp4a.u32.u32 %r5760, %r5761, %r5762, %r5756;
	// end inline asm
	ld.const.u32 	%r5765, [matrix+4052];
	// begin inline asm
	dp4a.u32.u32 %r5764, %r5765, %r5766, %r5760;
	// end inline asm
	ld.const.u32 	%r5769, [matrix+4056];
	// begin inline asm
	dp4a.u32.u32 %r5768, %r5769, %r5770, %r5764;
	// end inline asm
	ld.const.u32 	%r5773, [matrix+4060];
	// begin inline asm
	dp4a.u32.u32 %r5772, %r5773, %r5774, %r5768;
	// end inline asm
	ld.const.u32 	%r5777, [matrix+4064];
	// begin inline asm
	dp4a.u32.u32 %r5776, %r5777, %r5778, %r5772;
	// end inline asm
	ld.const.u32 	%r5781, [matrix+4068];
	// begin inline asm
	dp4a.u32.u32 %r5780, %r5781, %r5782, %r5776;
	// end inline asm
	ld.const.u32 	%r5785, [matrix+4072];
	// begin inline asm
	dp4a.u32.u32 %r5784, %r5785, %r5786, %r5780;
	// end inline asm
	ld.const.u32 	%r5789, [matrix+4076];
	// begin inline asm
	dp4a.u32.u32 %r5788, %r5789, %r5790, %r5784;
	// end inline asm
	ld.const.u32 	%r5793, [matrix+4080];
	// begin inline asm
	dp4a.u32.u32 %r5792, %r5793, %r5794, %r5788;
	// end inline asm
	ld.const.u32 	%r5797, [matrix+4084];
	// begin inline asm
	dp4a.u32.u32 %r5796, %r5797, %r5798, %r5792;
	// end inline asm
	ld.const.u32 	%r5801, [matrix+4088];
	// begin inline asm
	dp4a.u32.u32 %r5800, %r5801, %r5802, %r5796;
	// end inline asm
	ld.const.u32 	%r5805, [matrix+4092];
	// begin inline asm
	dp4a.u32.u32 %r5804, %r5805, %r5806, %r5800;
	// end inline asm
	shr.u32 	%r6097, %r5740, 6;
	and.b32  	%r6098, %r6097, 240;
	shr.u32 	%r6099, %r5804, 10;
	or.b32  	%r6100, %r6099, %r6098;
	cvt.u64.u32 	%rd244, %r6100;
	xor.b64  	%rd245, %rd210, %rd244;
	shl.b32 	%r6101, %r5985, 24;
	cvt.u64.u32 	%rd246, %r6101;
	shl.b32 	%r6102, %r5980, 16;
	and.b32  	%r6103, %r6102, 16711680;
	cvt.u64.u32 	%rd247, %r6103;
	shl.b32 	%r6104, %r5975, 8;
	and.b32  	%r6105, %r6104, 65280;
	cvt.u64.u32 	%rd248, %r6105;
	and.b32  	%r6106, %r5970, 255;
	cvt.u64.u32 	%rd249, %r6106;
	shl.b32 	%r6107, %r6019, 24;
	cvt.u64.u32 	%rd250, %r6107;
	shl.b32 	%r6108, %r6014, 16;
	and.b32  	%r6109, %r6108, 16711680;
	cvt.u64.u32 	%rd251, %r6109;
	shl.b32 	%r6110, %r6009, 8;
	and.b32  	%r6111, %r6110, 65280;
	cvt.u64.u32 	%rd252, %r6111;
	and.b32  	%r6112, %r6004, 255;
	cvt.u64.u32 	%rd253, %r6112;
	shl.b32 	%r6113, %r6053, 24;
	cvt.u64.u32 	%rd254, %r6113;
	shl.b32 	%r6114, %r6048, 16;
	and.b32  	%r6115, %r6114, 16711680;
	cvt.u64.u32 	%rd255, %r6115;
	shl.b32 	%r6116, %r6043, 8;
	and.b32  	%r6117, %r6116, 65280;
	cvt.u64.u32 	%rd256, %r6117;
	and.b32  	%r6118, %r6038, 255;
	cvt.u64.u32 	%rd257, %r6118;
	shr.u32 	%r6119, %r2732, 10;
	or.b32  	%r6120, %r6119, %r5999;
	xor.b32  	%r6121, %r10, %r6120;
	cvt.u64.u32 	%rd258, %r6121;
	shl.b64 	%rd259, %rd258, 56;
	shl.b64 	%rd260, %rd216, 48;
	and.b64  	%rd261, %rd260, 71776119061217280;
	or.b64  	%rd262, %rd259, %rd261;
	shl.b64 	%rd263, %rd214, 40;
	and.b64  	%rd264, %rd263, 280375465082880;
	or.b64  	%rd265, %rd262, %rd264;
	shl.b64 	%rd266, %rd212, 32;
	and.b64  	%rd267, %rd266, 1095216660480;
	or.b64  	%rd268, %rd265, %rd267;
	or.b64  	%rd269, %rd268, %rd246;
	or.b64  	%rd270, %rd269, %rd247;
	or.b64  	%rd271, %rd270, %rd248;
	or.b64  	%rd272, %rd271, %rd249;
	xor.b64  	%rd73, %rd272, 4239941492252378377;
	shr.u32 	%r6122, %r3756, 10;
	or.b32  	%r6123, %r6122, %r6033;
	xor.b32  	%r6124, %r12, %r6123;
	cvt.u64.u32 	%rd273, %r6124;
	shl.b64 	%rd274, %rd273, 56;
	shl.b64 	%rd275, %rd222, 48;
	and.b64  	%rd276, %rd275, 71776119061217280;
	or.b64  	%rd277, %rd274, %rd276;
	shl.b64 	%rd278, %rd220, 40;
	and.b64  	%rd279, %rd278, 280375465082880;
	or.b64  	%rd280, %rd277, %rd279;
	shl.b64 	%rd281, %rd218, 32;
	and.b64  	%rd282, %rd281, 1095216660480;
	or.b64  	%rd283, %rd280, %rd282;
	or.b64  	%rd284, %rd283, %rd250;
	or.b64  	%rd285, %rd284, %rd251;
	or.b64  	%rd286, %rd285, %rd252;
	or.b64  	%rd287, %rd286, %rd253;
	xor.b64  	%rd484, %rd287, 8746723911537738262;
	shr.u32 	%r6125, %r4780, 10;
	or.b32  	%r6126, %r6125, %r6067;
	xor.b32  	%r6127, %r14, %r6126;
	cvt.u64.u32 	%rd288, %r6127;
	shl.b64 	%rd289, %rd288, 56;
	shl.b64 	%rd290, %rd228, 48;
	and.b64  	%rd291, %rd290, 71776119061217280;
	or.b64  	%rd292, %rd289, %rd291;
	shl.b64 	%rd293, %rd226, 40;
	and.b64  	%rd294, %rd293, 280375465082880;
	or.b64  	%rd295, %rd292, %rd294;
	shl.b64 	%rd296, %rd224, 32;
	and.b64  	%rd297, %rd296, 1095216660480;
	or.b64  	%rd298, %rd295, %rd297;
	or.b64  	%rd299, %rd298, %rd254;
	or.b64  	%rd300, %rd299, %rd255;
	or.b64  	%rd301, %rd300, %rd256;
	or.b64  	%rd302, %rd301, %rd257;
	xor.b64  	%rd479, %rd302, 8796936657246353646;
	shl.b64 	%rd303, %rd245, 56;
	shl.b64 	%rd304, %rd243, 48;
	and.b64  	%rd305, %rd304, 71776119061217280;
	or.b64  	%rd306, %rd303, %rd305;
	shl.b64 	%rd307, %rd241, 40;
	and.b64  	%rd308, %rd307, 280375465082880;
	or.b64  	%rd309, %rd306, %rd308;
	shl.b64 	%rd310, %rd239, 32;
	and.b64  	%rd311, %rd310, 1095216660480;
	or.b64  	%rd312, %rd309, %rd311;
	shl.b64 	%rd313, %rd237, 24;
	and.b64  	%rd314, %rd313, 4278190080;
	or.b64  	%rd315, %rd312, %rd314;
	shl.b64 	%rd316, %rd235, 16;
	and.b64  	%rd317, %rd316, 16711680;
	shl.b64 	%rd318, %rd231, 8;
	and.b64  	%rd319, %rd318, 65280;
	or.b64  	%rd320, %rd315, %rd317;
	or.b64  	%rd321, %rd320, %rd319;
	or.b64  	%rd322, %rd321, %rd233;
	xor.b64  	%rd474, %rd322, 1272090201925444760;
	mov.u64 	%rd488, 8270816933120786537;
	mov.u64 	%rd487, -850687345431043546;
	mov.u64 	%rd486, 8596393687355028144;
	mov.u64 	%rd485, -4073852189716399785;
	mov.u64 	%rd483, -4539347866060507718;
	mov.u64 	%rd482, -3233781605604422593;
	mov.u64 	%rd481, 570094237299545110;
	mov.u64 	%rd480, 5171152063242093102;
	mov.u64 	%rd478, 6782861118970774626;
	mov.u64 	%rd477, 7812475424661425213;
	mov.u64 	%rd476, 9119540418498120711;
	mov.u64 	%rd475, -7873636174015165430;
	mov.u64 	%rd473, -9207053471590684088;
	mov.u64 	%rd472, 3370482334374859748;
	mov.u64 	%rd471, -1544774801229058759;
	mov.u64 	%rd470, 6096431547456407061;
	mov.u64 	%rd469, -1792185402154627366;
	mov.u64 	%rd468, -6864424130110145268;
	mov.u64 	%rd467, 5690099369266491460;
	mov.u64 	%rd466, -5074726839974049192;
	mov.u64 	%rd465, 1592359455985097269;
	mov.u64 	%rd464, RC;

$L__BB0_9:
	xor.b64  	%rd323, %rd488, %rd73;
	xor.b64  	%rd324, %rd323, %rd487;
	xor.b64  	%rd325, %rd324, %rd486;
	xor.b64  	%rd326, %rd325, %rd485;
	xor.b64  	%rd327, %rd483, %rd484;
	xor.b64  	%rd328, %rd327, %rd482;
	xor.b64  	%rd329, %rd328, %rd481;
	xor.b64  	%rd330, %rd329, %rd480;
	xor.b64  	%rd331, %rd478, %rd479;
	xor.b64  	%rd332, %rd331, %rd477;
	xor.b64  	%rd333, %rd332, %rd476;
	xor.b64  	%rd334, %rd333, %rd475;
	xor.b64  	%rd335, %rd473, %rd474;
	xor.b64  	%rd336, %rd335, %rd472;
	xor.b64  	%rd337, %rd336, %rd471;
	xor.b64  	%rd338, %rd337, %rd470;
	xor.b64  	%rd339, %rd468, %rd469;
	xor.b64  	%rd340, %rd339, %rd467;
	xor.b64  	%rd341, %rd340, %rd466;
	xor.b64  	%rd342, %rd341, %rd465;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6128}, %rd330;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6129,%dummy}, %rd330;
	}
	shf.l.wrap.b32 	%r6130, %r6129, %r6128, 1;
	shf.l.wrap.b32 	%r6131, %r6128, %r6129, 1;
	mov.b64 	%rd343, {%r6131, %r6130};
	xor.b64  	%rd344, %rd342, %rd343;
	xor.b64  	%rd345, %rd344, %rd73;
	xor.b64  	%rd346, %rd488, %rd344;
	xor.b64  	%rd347, %rd487, %rd344;
	xor.b64  	%rd348, %rd486, %rd344;
	xor.b64  	%rd349, %rd485, %rd344;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6132}, %rd334;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6133,%dummy}, %rd334;
	}
	shf.l.wrap.b32 	%r6134, %r6133, %r6132, 1;
	shf.l.wrap.b32 	%r6135, %r6132, %r6133, 1;
	mov.b64 	%rd350, {%r6135, %r6134};
	xor.b64  	%rd351, %rd350, %rd326;
	xor.b64  	%rd352, %rd484, %rd351;
	xor.b64  	%rd353, %rd483, %rd351;
	xor.b64  	%rd354, %rd482, %rd351;
	xor.b64  	%rd355, %rd481, %rd351;
	xor.b64  	%rd356, %rd480, %rd351;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6136}, %rd338;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6137,%dummy}, %rd338;
	}
	shf.l.wrap.b32 	%r6138, %r6137, %r6136, 1;
	shf.l.wrap.b32 	%r6139, %r6136, %r6137, 1;
	mov.b64 	%rd357, {%r6139, %r6138};
	xor.b64  	%rd358, %rd357, %rd330;
	xor.b64  	%rd359, %rd479, %rd358;
	xor.b64  	%rd360, %rd478, %rd358;
	xor.b64  	%rd361, %rd477, %rd358;
	xor.b64  	%rd362, %rd476, %rd358;
	xor.b64  	%rd363, %rd475, %rd358;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6140}, %rd342;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6141,%dummy}, %rd342;
	}
	shf.l.wrap.b32 	%r6142, %r6141, %r6140, 1;
	shf.l.wrap.b32 	%r6143, %r6140, %r6141, 1;
	mov.b64 	%rd364, {%r6143, %r6142};
	xor.b64  	%rd365, %rd364, %rd334;
	xor.b64  	%rd366, %rd474, %rd365;
	xor.b64  	%rd367, %rd473, %rd365;
	xor.b64  	%rd368, %rd472, %rd365;
	xor.b64  	%rd369, %rd471, %rd365;
	xor.b64  	%rd370, %rd470, %rd365;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6144}, %rd326;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6145,%dummy}, %rd326;
	}
	shf.l.wrap.b32 	%r6146, %r6145, %r6144, 1;
	shf.l.wrap.b32 	%r6147, %r6144, %r6145, 1;
	mov.b64 	%rd371, {%r6147, %r6146};
	xor.b64  	%rd372, %rd338, %rd371;
	xor.b64  	%rd373, %rd469, %rd372;
	xor.b64  	%rd374, %rd468, %rd372;
	xor.b64  	%rd375, %rd467, %rd372;
	xor.b64  	%rd376, %rd466, %rd372;
	xor.b64  	%rd377, %rd465, %rd372;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6148}, %rd352;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6149,%dummy}, %rd352;
	}
	shf.l.wrap.b32 	%r6150, %r6149, %r6148, 1;
	shf.l.wrap.b32 	%r6151, %r6148, %r6149, 1;
	mov.b64 	%rd378, {%r6151, %r6150};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6152}, %rd347;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6153,%dummy}, %rd347;
	}
	shf.l.wrap.b32 	%r6154, %r6153, %r6152, 3;
	shf.l.wrap.b32 	%r6155, %r6152, %r6153, 3;
	mov.b64 	%rd379, {%r6155, %r6154};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6156}, %rd360;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6157,%dummy}, %rd360;
	}
	shf.l.wrap.b32 	%r6158, %r6157, %r6156, 6;
	shf.l.wrap.b32 	%r6159, %r6156, %r6157, 6;
	mov.b64 	%rd380, {%r6159, %r6158};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6160}, %rd354;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6161,%dummy}, %rd354;
	}
	shf.l.wrap.b32 	%r6162, %r6161, %r6160, 10;
	shf.l.wrap.b32 	%r6163, %r6160, %r6161, 10;
	mov.b64 	%rd381, {%r6163, %r6162};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6164}, %rd362;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6165,%dummy}, %rd362;
	}
	shf.l.wrap.b32 	%r6166, %r6165, %r6164, 15;
	shf.l.wrap.b32 	%r6167, %r6164, %r6165, 15;
	mov.b64 	%rd382, {%r6167, %r6166};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6168}, %rd369;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6169,%dummy}, %rd369;
	}
	shf.l.wrap.b32 	%r6170, %r6169, %r6168, 21;
	shf.l.wrap.b32 	%r6171, %r6168, %r6169, 21;
	mov.b64 	%rd383, {%r6171, %r6170};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6172}, %rd366;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6173,%dummy}, %rd366;
	}
	shf.l.wrap.b32 	%r6174, %r6173, %r6172, 28;
	shf.l.wrap.b32 	%r6175, %r6172, %r6173, 28;
	mov.b64 	%rd384, {%r6175, %r6174};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6176,%dummy}, %rd346;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6177}, %rd346;
	}
	shf.r.wrap.b32 	%r6178, %r6177, %r6176, 28;
	shf.r.wrap.b32 	%r6179, %r6176, %r6177, 28;
	mov.b64 	%rd385, {%r6179, %r6178};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6180,%dummy}, %rd355;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6181}, %rd355;
	}
	shf.r.wrap.b32 	%r6182, %r6181, %r6180, 19;
	shf.r.wrap.b32 	%r6183, %r6180, %r6181, 19;
	mov.b64 	%rd386, {%r6183, %r6182};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6184,%dummy}, %rd367;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6185}, %rd367;
	}
	shf.r.wrap.b32 	%r6186, %r6185, %r6184, 9;
	shf.r.wrap.b32 	%r6187, %r6184, %r6185, 9;
	mov.b64 	%rd387, {%r6187, %r6186};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6188}, %rd356;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6189,%dummy}, %rd356;
	}
	shf.l.wrap.b32 	%r6190, %r6189, %r6188, 2;
	shf.l.wrap.b32 	%r6191, %r6188, %r6189, 2;
	mov.b64 	%rd388, {%r6191, %r6190};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6192}, %rd377;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6193,%dummy}, %rd377;
	}
	shf.l.wrap.b32 	%r6194, %r6193, %r6192, 14;
	shf.l.wrap.b32 	%r6195, %r6192, %r6193, 14;
	mov.b64 	%rd389, {%r6195, %r6194};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6196}, %rd373;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6197,%dummy}, %rd373;
	}
	shf.l.wrap.b32 	%r6198, %r6197, %r6196, 27;
	shf.l.wrap.b32 	%r6199, %r6196, %r6197, 27;
	mov.b64 	%rd390, {%r6199, %r6198};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6200,%dummy}, %rd348;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6201}, %rd348;
	}
	shf.r.wrap.b32 	%r6202, %r6201, %r6200, 23;
	shf.r.wrap.b32 	%r6203, %r6200, %r6201, 23;
	mov.b64 	%rd391, {%r6203, %r6202};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6204,%dummy}, %rd370;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6205}, %rd370;
	}
	shf.r.wrap.b32 	%r6206, %r6205, %r6204, 8;
	shf.r.wrap.b32 	%r6207, %r6204, %r6205, 8;
	mov.b64 	%rd392, {%r6207, %r6206};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6208}, %rd376;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6209,%dummy}, %rd376;
	}
	shf.l.wrap.b32 	%r6210, %r6209, %r6208, 8;
	shf.l.wrap.b32 	%r6211, %r6208, %r6209, 8;
	mov.b64 	%rd393, {%r6211, %r6210};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6212}, %rd368;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6213,%dummy}, %rd368;
	}
	shf.l.wrap.b32 	%r6214, %r6213, %r6212, 25;
	shf.l.wrap.b32 	%r6215, %r6212, %r6213, 25;
	mov.b64 	%rd394, {%r6215, %r6214};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6216,%dummy}, %rd361;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6217}, %rd361;
	}
	shf.r.wrap.b32 	%r6218, %r6217, %r6216, 21;
	shf.r.wrap.b32 	%r6219, %r6216, %r6217, 21;
	mov.b64 	%rd395, {%r6219, %r6218};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6220,%dummy}, %rd359;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6221}, %rd359;
	}
	shf.r.wrap.b32 	%r6222, %r6221, %r6220, 2;
	shf.r.wrap.b32 	%r6223, %r6220, %r6221, 2;
	mov.b64 	%rd396, {%r6223, %r6222};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6224}, %rd349;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6225,%dummy}, %rd349;
	}
	shf.l.wrap.b32 	%r6226, %r6225, %r6224, 18;
	shf.l.wrap.b32 	%r6227, %r6224, %r6225, 18;
	mov.b64 	%rd397, {%r6227, %r6226};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6228,%dummy}, %rd375;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6229}, %rd375;
	}
	shf.r.wrap.b32 	%r6230, %r6229, %r6228, 25;
	shf.r.wrap.b32 	%r6231, %r6228, %r6229, 25;
	mov.b64 	%rd398, {%r6231, %r6230};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6232,%dummy}, %rd363;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6233}, %rd363;
	}
	shf.r.wrap.b32 	%r6234, %r6233, %r6232, 3;
	shf.r.wrap.b32 	%r6235, %r6232, %r6233, 3;
	mov.b64 	%rd399, {%r6235, %r6234};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6236}, %rd374;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6237,%dummy}, %rd374;
	}
	shf.l.wrap.b32 	%r6238, %r6237, %r6236, 20;
	shf.l.wrap.b32 	%r6239, %r6236, %r6237, 20;
	mov.b64 	%rd400, {%r6239, %r6238};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r6240,%dummy}, %rd353;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6241}, %rd353;
	}
	shf.r.wrap.b32 	%r6242, %r6241, %r6240, 20;
	shf.r.wrap.b32 	%r6243, %r6240, %r6241, 20;
	mov.b64 	%rd401, {%r6243, %r6242};
	not.b64 	%rd402, %rd401;
	and.b64  	%rd403, %rd395, %rd402;
	xor.b64  	%rd404, %rd403, %rd345;
	not.b64 	%rd405, %rd395;
	and.b64  	%rd406, %rd383, %rd405;
	xor.b64  	%rd484, %rd406, %rd401;
	not.b64 	%rd407, %rd383;
	and.b64  	%rd408, %rd389, %rd407;
	xor.b64  	%rd479, %rd408, %rd395;
	not.b64 	%rd409, %rd389;
	and.b64  	%rd410, %rd345, %rd409;
	xor.b64  	%rd474, %rd410, %rd383;
	not.b64 	%rd411, %rd345;
	and.b64  	%rd412, %rd401, %rd411;
	xor.b64  	%rd469, %rd389, %rd412;
	not.b64 	%rd413, %rd400;
	and.b64  	%rd414, %rd379, %rd413;
	xor.b64  	%rd488, %rd414, %rd384;
	not.b64 	%rd415, %rd379;
	and.b64  	%rd416, %rd386, %rd415;
	xor.b64  	%rd483, %rd416, %rd400;
	not.b64 	%rd417, %rd386;
	and.b64  	%rd418, %rd399, %rd417;
	xor.b64  	%rd478, %rd418, %rd379;
	not.b64 	%rd419, %rd399;
	and.b64  	%rd420, %rd384, %rd419;
	xor.b64  	%rd473, %rd420, %rd386;
	not.b64 	%rd421, %rd384;
	and.b64  	%rd422, %rd400, %rd421;
	xor.b64  	%rd468, %rd399, %rd422;
	not.b64 	%rd423, %rd380;
	and.b64  	%rd424, %rd394, %rd423;
	xor.b64  	%rd487, %rd424, %rd378;
	not.b64 	%rd425, %rd394;
	and.b64  	%rd426, %rd393, %rd425;
	xor.b64  	%rd482, %rd426, %rd380;
	not.b64 	%rd427, %rd393;
	and.b64  	%rd428, %rd397, %rd427;
	xor.b64  	%rd477, %rd428, %rd394;
	not.b64 	%rd429, %rd397;
	and.b64  	%rd430, %rd378, %rd429;
	xor.b64  	%rd472, %rd430, %rd393;
	not.b64 	%rd431, %rd378;
	and.b64  	%rd432, %rd380, %rd431;
	xor.b64  	%rd467, %rd397, %rd432;
	not.b64 	%rd433, %rd385;
	and.b64  	%rd434, %rd381, %rd433;
	xor.b64  	%rd486, %rd434, %rd390;
	not.b64 	%rd435, %rd381;
	and.b64  	%rd436, %rd382, %rd435;
	xor.b64  	%rd481, %rd436, %rd385;
	not.b64 	%rd437, %rd382;
	and.b64  	%rd438, %rd392, %rd437;
	xor.b64  	%rd476, %rd438, %rd381;
	not.b64 	%rd439, %rd392;
	and.b64  	%rd440, %rd390, %rd439;
	xor.b64  	%rd471, %rd440, %rd382;
	not.b64 	%rd441, %rd390;
	and.b64  	%rd442, %rd385, %rd441;
	xor.b64  	%rd466, %rd392, %rd442;
	not.b64 	%rd443, %rd387;
	and.b64  	%rd444, %rd398, %rd443;
	xor.b64  	%rd485, %rd444, %rd396;
	not.b64 	%rd445, %rd398;
	and.b64  	%rd446, %rd391, %rd445;
	xor.b64  	%rd480, %rd446, %rd387;
	not.b64 	%rd447, %rd391;
	and.b64  	%rd448, %rd388, %rd447;
	xor.b64  	%rd475, %rd448, %rd398;
	not.b64 	%rd449, %rd388;
	and.b64  	%rd450, %rd396, %rd449;
	xor.b64  	%rd470, %rd450, %rd391;
	not.b64 	%rd451, %rd396;
	and.b64  	%rd452, %rd387, %rd451;
	xor.b64  	%rd465, %rd388, %rd452;
	ld.global.nc.u64 	%rd453, [%rd464];
	xor.b64  	%rd73, %rd404, %rd453;
	add.s64 	%rd464, %rd464, 8;
	add.s32 	%r6244, %r6244, 1;
	setp.ne.s32 	%p11, %r6244, 24;
	@%p11 bra 	$L__BB0_9;

	ld.const.u64 	%rd75, [target+24];
	setp.eq.s64 	%p12, %rd474, %rd75;
	@%p12 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_11;

$L__BB0_12:
	ld.const.u64 	%rd76, [target+16];
	setp.eq.s64 	%p13, %rd479, %rd76;
	@%p13 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_13;

$L__BB0_14:
	ld.const.u64 	%rd77, [target+8];
	setp.eq.s64 	%p14, %rd484, %rd77;
	@%p14 bra 	$L__BB0_16;
	bra.uni 	$L__BB0_15;

$L__BB0_16:
	ld.const.u64 	%rd454, [target];
	setp.lt.u64 	%p16, %rd73, %rd454;
	bra.uni 	$L__BB0_17;

$L__BB0_11:
	setp.lt.u64 	%p16, %rd474, %rd75;
	bra.uni 	$L__BB0_17;

$L__BB0_13:
	setp.lt.u64 	%p16, %rd479, %rd76;
	bra.uni 	$L__BB0_17;

$L__BB0_15:
	setp.lt.u64 	%p16, %rd484, %rd77;

$L__BB0_17:
	not.pred 	%p15, %p16;
	@%p15 bra 	$L__BB0_19;

	ld.param.u64 	%rd462, [heavy_hash_param_0];
	ld.param.u64 	%rd461, [heavy_hash_param_1];
	and.b64  	%rd460, %rd463, %rd462;
	or.b64  	%rd459, %rd460, %rd461;
	ld.param.u64 	%rd458, [heavy_hash_param_5];
	cvta.to.global.u64 	%rd457, %rd458;
	mov.u64 	%rd455, 0;
	atom.global.cas.b64 	%rd456, [%rd457], %rd455, %rd459;

$L__BB0_19:
	ret;

}

